---
title: "Movie ratings (for the Bayes in Stan book)"
author: "Andrew Gelman"
date: "12 Jul 2018"
output:
  html_document:
    theme: readable
    code folding: hide
---

```{r setup, include=FALSE, echo=FALSE}
options(htmltools.dir.version = FALSE)
options(digits = 2)

library(knitr)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
knitr::opts_chunk$set(comment = "")

print_file <- function(file) {
  cat(paste(readLines(file), "\n", sep=""), sep="")
}

library("arm")
library("rstan")
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```
Consider the following scenario.  You are considering which of two movies to go see.  Both have average online ratings of 4 out of 5 stars, but one is based on 2 ratings and the other is based on 100.  Which movie should you go see?

We will set this up as a statistics problem, making the assumption that you would prefer to see the movie that is most preferred, on average, by others.  That is, we can imagine every movie having a "true popularity" which would be the average rating, if everyone in the population were to see this movie and rate it on a 0--5 scale. We are thus implicitly assuming that these two movies are aimed at the same target audience, which includes you.

We further assume that each of these movies has been rated by a random sample of people from this general audience:  thus, we can consider the observed average rating of 4.0 in each case to be an estimate of the true popularity of each movie.  At the end of this example, we will discuss how to relax these assumptions.

At first you might think that since both movies have average ratings of 4.0, it's a tossup which one to see.  

But the estimate based on 2 ratings should be much less accurate than the estimate based on 100 ratings. If you really want to see a 4-star movie, the one with 100 ratings should be a safer bet.

We'll now make this point using Bayesian inference, going through the following steps:

- Constructing data fitting the above story and fitting a Bayesian model to get inferences about the true popularity of the two movies.

- Embedding this all in a larger problem with many movies being rated.

- Setting up a model in which different people rate different movies, with systematic differences between who rates which movies.

- Espanding the model to allow for the possibility that more people see, and rate, movies that are more popular.

For each step, we set up a model, simulate fake data from that model, and check that we can recover the underlying parameters to some level of accuracy.

## Model for 2 movies

Define $\theta_j, j=1,2$ to be the true popularities of the two movies, with any randomly selected rating for movie $j$ being drawn from a normal distribution with mean $\theta_j$ and standard deviation $\sigma$.

For simplicity, we're pretending radings are continuous unbounded numbers (rather than integers from 0 through 5) and that the distributions of ratings for the teo movies differ only in their mean, not their variance.  We're not allowing, for example, a polarizing movie that you either love or hate.  The ratings distribution for any movie is assumed to be bell-shaped.  Again, we can later go back and expand our models to allow other possibilities.

In our initial example, we have 102 ratings:  2 for one movie and 100 for the other.  We label the individual ratings as $y_i,i=1,\dots,N=102$ and index the movies being rated as $j[i]$, where $j[1]=j[2]=1$ and $j[3]=j[4]=\dots=j[102]=2$.  We can then write our model as,
$$y_i \sim \mbox{normal}(\theta_{j[i]}, \sigma), i=1,\dots,N.$$
In statistics notation such as used in _Bayesian Data Analysis_, the normal distribution is written in terms of mean and variance, or squared standard deviation:  $y \sim \mbox{normal}(\mu,\sigma^2)$.  But in this book we will follow Stan notation and write $y \sim \mbox{normal}(\mu,\sigma)$.

To perform Bayesian inference we also need a prior distribution on all the parameters, which in this case are $\theta_1$, $\theta_2$, and $\sigma$.

First, the parameters have some mathematical constraints.  The available ratings are 0 through 5, so each $\theta$ must fall in that range as well, and $\sigma$ cannot be more than 2.5, and that is the maximum standard deviation among all distributions defined in the range $[0,5]$ (with that maximum attained with the distribution with all its mass at the extremes, with half the responses at 0 and half at 5).

Beyond this, we will assume that movies typically get ratings of around three stars.  We'll give normal prior distributions with mean 3 and standard deviation 1 to each of the parameters $\theta_1$ and $\theta_2$, which represent the underlying or population-average ratings of the two movies.

Here is the resulting Stan model:
```{r, echo=FALSE}
print_file("ratings_1.stan")
```

Near the bottom of the above program, the lines blocked off with a slash and star are comments, indicating how the vectorization in the Stan language allows us to assign an entire set of probability distributions at once.

We next construct data for our two movies.  For movie 1, we suppose the two ratings are 3 and 5, which give the posited average rating of 4.  For movie 2, we suppose the ratings are 10 2's, 20 3's, 30 4's, and 40 5's, which again average to 4.  This set of hypothetical ratings does _not_ follow the assumed normal distribution, but that's fine; we can still fit the model.

```{r, results=FALSE}
y_1 <- c(3, 5)
y_2 <- rep(c(2,3,4,5), c(10,20,30,40))
y <- c(y_1, y_2)
N <- length(y)
movie <- rep(c(1,2), c(length(y_1), length(y_2)))
movie_data <- list(y=y, N=N, movie=movie)
fit_1 <- stan("ratings_1.stan", data=movie_data)
```
Here is the result:
```{r, echo=FALSE}
print(fit_1)
```
The estimated population average popularity is 3.6 for the first movie and 4.0 for the second.  We hae more information on the second movie, so we can more precisely estimate it as truly meriting 4 stars in the public assessment:  the 50\% posterior interval for $\theta_2$ is $[3.9, 4.1]$.  In contrast, we have a lot more uncertainty about $\theta_1$, which according to our model has a 50\% probability of being in the range $[3.2, 4.0]$ but could be as low as 2.5 or as high as 4.7.

So, which movie to see?  It depends on your goals.  Movie 2 is a safer bet, with a higher estimated quality.  But Movie 1 has a small chance of being outstanding, along with a moderate chance of being mediocre.  You can see Movie 1 if you want to roll the dice.

## Estending the model to J movies

It is easy to expand the Stan program to allow an arbitrary number of movies:
```{r, echo=FALSE}
print_file("ratings_2.stan")
```
As usual, to really understand this model, it helps to be able to simulate fake data.  Let's set $J=40$ movies and let the number of times each movie is rated be a random number, uniformly distributed between 0 and 100.  We start by drawing parameters $\theta_j$ from a normal distribution with mean 3 and standard deviation 0.5 (so that the movies' true popularities are mostly between 2 and 4 on that 0--5 scale), and then sample continuous individual ratings $z_i$ for each movie $j[i]$ from a normal  distribution with mean $\theta_{j[i]}$ and standard deviation 2:
$$
\theta_j \sim \mbox{normal}(3.0, 0.5), \mbox{ for } j=1,\dots,J \\
z_i \sim \mbox{normal}(\theta_{j[i]}, 2.0), \mbox{ for } i=1,\dots,n.
$$
To keep things simple, we will ignore the fact that the ratings are constrained to be between 0 and 5; instead, we will simulate continuous ratings on an unbounded scale.  It is not difficult in Stan to model ordered discrete responses; see Chapter **.  Here, though, we will ignore that feature of the data.

We simulate the fake data in R and then fit the new model in Stan.
```{r, echo=FALSE, results=FALSE}
J <- 40
N_ratings <- sample(0:100, J, replace=TRUE)   # Assign each movie a random number of ratings between 0 and 100
N <- sum(N_ratings)
movie <- rep(1:J, N_ratings)  # Create a vector indexing which movie corresponds to each rating
theta <- rnorm(J, 3.0, 0.5)
y <- rnorm(N, theta[movie], 2.0)
movie_data <- list(y=y, N=N, J=J, movie=movie)
fit_2 <- stan("ratings_2.stan", data=movie_data)
```
Here is the result:
```{r, echo=FALSE}
print(fit_2)
```
We can check the fit by plotting the posterior inferences against the true parameter values:

```{r, echo=FALSE}
theta_post <- extract(fit_2)$theta
quants <- c(0.025, 0.25, 0.5, 0.75, 0.975)
theta_post_quants <- array(NA, c(J,length(quants)), dimnames=list(NULL, quants))
for (j in 1:J){
  theta_post_quants[j,] <- quantile(theta_post[,j], quants)
}
par(mar=c(3,3,2,1), mgp=c(1.7,.5,0), tck=-.02)
par(pty="s")
rng <- range(theta_post_quants, theta)
plot(rng, rng, xlab="Posterior median, 50%, and 95% interval", ylab="True parameter value", bty="l", type="n")
abline(0, 1, col="gray")
points(theta_post_quants[,"0.5"], theta, pch=20)
for (j in 1:J){
  lines(c(theta_post_quants[j,"0.25"], theta_post_quants[j,"0.75"]), rep(theta[j], 2), lwd=2)
  lines(c(theta_post_quants[j,"0.025"], theta_post_quants[j,"0.975"]), rep(theta[j], 2), lwd=0.5)
}
mtext("Comparing parameters theta_j to their posterior inferences", side=3)
```

Roughly half the 50\% intervals and 95\% of the 95\% intervals contain the true parameter value, which is about what we would expect to see, given that we have simulated data from the model we are fitting.

The intervals in the above graph vary in width.  The more data we have for any given movie, the more precise is our estimate of its underlying popularity.  Here is a graph showing the width of the 50\% interval as a function of sample size:

```{r, echo=FALSE}
interval_width <- theta_post_quants[,"0.75"] - theta_post_quants[,"0.25"]
par(mar=c(3,3,2,1), mgp=c(1.7,.5,0), tck=-.02)
plot(c(0, 1.02*max(N_ratings)), c(0, 1.02*max(interval_width)), xlab="Number of ratings", ylab="Width of 50% posterior interval", yaxs="i", yaxs="i", bty="l", type="n")
points(N_ratings, interval_width, pch=20)
mtext("Where you have more data, you have less uncertainty", side=3)
```

The only reason the points do not completely fall along a smooth curve here is that the intervals are computed using simulation.  Here we have 4000 simulation draws.  If we were to run Stan longer and obtain more iterations, then the resulting intervals would be more stable, and the above graph would look smoother.  For all practical purposes, though, we have enough simulations and enough iterations.  For example, if $\theta_1$ has a posterior 50\% interval of $[3.1, 3.5]$, there is no real reason to get a huge number of simulations and to find out that the precise interval is $[3.13, 3.48]$, as this makes no real difference in our understanding of $\theta_3$, nor should it seriously affect any decision we might want to make using these data.

## Item-response model with parameters for raters and for movies

There are many ways to extend the above model.  To start with, we can recognize that different people rate different movies, and each rater uses his or own scale.  Suppose, for example, that some people tend to give high ratings and others tend to give low ratings, and the sorts of people who give high ratings are more likely to watch romantic comedies, while the tougher judges more frequently watch crime movies.  Then a simple comparison of average ratings will be unfair to the crime movies, as this does not take into account systematic differences between raters.

We can model rater effects using what is called an _item-response model_, the simplest form of which looks like this, for a numerical rating $y_i$ of movie $j[i]$ by rater $k[i]$,
$$
y_i \sim \mbox{normal}(a_{j[i]} - b_{k[i]}, \sigma_y),
$$
Here, $a_j$ is a parameter that could be said to represent movie "quality," corresponding to the average rating that movie $j$ would receive, if it were reviewed by average raters.  The parameter $b_k$ represents the "difficulty" of rater $k$:  higher values of $b_k$ correspond to raters who give tougher judgments of equivalent movies.

The previous model is equivalent to this new model with all the $b_k$'s fixed at zero.  When fitting the new model, we constrain the $b_k$'s to come from a distribution whose average is zero.  Some such constraint is necessary because otherwise the model would not be _identified_:  for example, you could add 100 to each of the $a_j$'s and $-100$ to each of the $b_k$'s and not change any of the predictors at all.

Our full model looks like this:
$$
y_i \sim \mbox{normal}(a_{j[i]} - b_{k[i]}, \sigma_y), \mbox{ for } i=1,\dots, N \\
a_j \sim \mbox{normal}(\mu_a, \sigma_a), \mbox{ for } j=1,\dots, J \\
b_k \sim \mbox{normal}(0, \sigma_b), \mbox{ for } k=1,\dots, K.
$$
We also need prior distributions for the as-yet-unmodeled parameters $\mu_a,\sigma_a,\sigma_b,\sigma_y$.

But before setting that up, we re-express the model in a way that will be generally useful:

$$
y_i \sim \mbox{normal}(\mu + \sigma_a*\alpha_{j[i]} - \sigma_b*\beta_{k[i]}, \sigma_y), \mbox{ for } i=1,\dots, N \\
\alpha_j \sim \mbox{normal}(0, 1), \mbox{ for } j=1,\dots, J \\
\beta_k \sim \mbox{normal}(0, 1), \mbox{ for } k=1,\dots, K.
$$
This new version, sometimes called the _non-centered parameterization_, is convenient because it separates the scaled and unscaled parameters; also it can have certain computational advantages, as discussed here:  [point to Mike Betancourt's case study?].  The new models are equivalent, with the movle quality parameters being expressed as, $a_j = \mu + \sigma_{\alpha}*\alpha_j$.

And now we can add prior distributions.  We will start with uniform priors (subject to the constraint that $\sigma_a$, $\sigma_b$, and $\sigma_y$ must all be positive), adding in prior information later if the data are weak enough that this seems necessary.

Here is the Stan program:
```{r, echo=FALSE}
print_file("ratings_3.stan")
```
As usual, we'll check it by simulating fake data, then fitting the model in Stan and checking that the parameters are recovered.

We'll start by simulating data from $J=40$ movkes and $K=100$ raters, with each person rating each movie, and with parameters $\mu=3$ (thus, an average rating of 3 for all movies and all raters), $\sigma_a=0.5$, $\sigma_b=0.5$ (thus, the same amount of variation in raters' difficulties than in the quality of movies) and $\sigma_y=2$, as before.
```{r, echo=FALSE, results=FALSE}
J <- 40
K <- 100
N <- J*K
movie <- rep(1:J, rep(K, J))
rater <- rep(1:K, J)
mu <- 3
sigma_a <- 0.5
sigma_b <- 0.5
sigma_y <- 2
alpha <- rnorm(J, 0, 1)
beta <- rnorm(K, 0, 1)
y <- rnorm(N, mu + sigma_a*alpha[movie] + sigma_b*beta[rater], sigma_y)
data_3 <- list(N=N, J=J, K=K, movie=movie, rater=rater, y=y)
fit_3 <- stan("ratings_3.stan", data=data_3)
```
Here is part of the Stan fit.  We just display the hyperparameters, to save space omitting the parameter vectors $\alpha$ and $\beta$ and the transformed parameter vector $a$.
```{r, echo=FALSE}
print(fit_3, pars=c("mu", "sigma_a", "sigma_b", "sigma_y"))
```
Stan did fine recovering the variance parameters.  The mean level $\mu$ is more difficult to nail down, but the true value of 3.0 is within the range of posterior uncertainty.

Let's check the coverage for the $\alpha$'s and $\beta$'s:
```{r, echo=FALSE}
par(mfrow=c(1,2))
alpha_post <- extract(fit_3)$alpha
beta_post <- extract(fit_3)$beta
quants <- c(0.025, 0.25, 0.5, 0.75, 0.975)
alpha_post_quants <- array(NA, c(J,length(quants)), dimnames=list(NULL, quants))
beta_post_quants <- array(NA, c(K,length(quants)), dimnames=list(NULL, quants))
for (j in 1:J){
  alpha_post_quants[j,] <- quantile(alpha_post[,j], quants)
}
for (k in 1:K){
  beta_post_quants[k,] <- quantile(beta_post[,k], quants)
}
par(mar=c(3,3,2,1), mgp=c(1.7,.5,0), tck=-.02)
par(pty="s")

rng <- range(alpha_post_quants, alpha)
plot(rng, rng, xlab="Posterior median, 50%, and 95% interval", ylab="True parameter value", bty="l", type="n")
abline(0, 1, col="gray")
points(alpha_post_quants[,"0.5"], alpha, pch=20)
for (j in 1:J){
  lines(c(alpha_post_quants[j,"0.25"], alpha_post_quants[j,"0.75"]), rep(alpha[j], 2), lwd=2)
  lines(c(alpha_post_quants[j,"0.025"], alpha_post_quants[j,"0.975"]), rep(alpha[j], 2), lwd=0.5)
}
mtext("Checking the alpha_j's", side=3)
rng <- range(beta_post_quants, beta)
plot(rng, rng, xlab="Posterior median, 50%, and 95% interval", ylab="True parameter value", bty="l", type="n")
abline(0, 1, col="gray")
points(beta_post_quants[,"0.5"], beta, pch=20)
for (k in 1:K){
  lines(c(beta_post_quants[k,"0.25"], beta_post_quants[k,"0.75"]), rep(beta[k], 2), lwd=2)
  lines(c(beta_post_quants[k,"0.025"], beta_post_quants[k,"0.975"]), rep(beta[k], 2), lwd=0.5)
}
mtext("Checking the beta_k's", side=3)
```

Now let's put our model to more of a challenge by giving it unbalanced data.  Let's divide the movies into two groups:  romantic comedies (movies $j=1,\dots,20$) and crime stories (movies $j=21,\dots,40$), and set things up so that the more difficult reviewers (those with positive values of $\alpha_k$) are more likely to review crime stories.

We'll set up the simulation as follows.  Each of the $K$ people might rate each of the $J$ movies. If $\beta_k>0$, then person $k$ will have a 30\% chance of rating each romantic comedy and a 60\% chance of rating each crime movie.  If $\beta_k<0$, then the probabilities are reversed, and person $k$ has a 30\% chance of rating each romantic comedy and a 60\% chance of rating each crime movie.  So, in the data, we'll expect to see tougher reviews on the crime stories.

We simulate this model, using the same ratings as before but just selecting a subset according to the above-defined probabilities.
```{r, echo=FALSE, results=FALSE}
genre <- rep(c("romantic","crime"), c(round(J/2), J - round(J/2)))
prob_of_rated <- ifelse(beta[rater]>0, ifelse(genre[movie]=="romantic", 0.6, 0.3),
                        ifelse(genre[movie]=="romantic", 0.3, 0.6))
rated <- rbinom(N, 1, prob_of_rated)   # Returns TRUE if movie was rated, FALSE if not
data_3a <- list(N=sum(rated), J=J, K=K, movie=movie[rated], rater=rater[rated], y=y[rated])
fit_3a <- stan("ratings_3.stan", data=data_3)
```
We then fit the model in Stan.
```{r, echo=FALSE}
print(fit_3a, pars=c("mu", "sigma_a", "sigma_b", "sigma_y"))
```
And display as before.  In the left graph we use open circles for the romantic comedies (those with $j=1,\dots,10$) and solid circles for the crime movies ($j=11,\dots,20$). In the right graph we use open circles for the nice reviewers (those with $\beta_k<0$) and solid circles for the difficult reviewers ($\beta_k<0$).  Coverage still seems fine, which it should be---again, we ran our simulations under the model that we later fit---but it is still gratifying to see, as a confirmation that we are not making any obvious mistakes.

```{r, echo=FALSE}
add_legend <- function(text, pch, range) {
  legend(0.6*min(range)+0.4*max(range), min(range)+0.12*(max(range)-min(range)), text[1], pch=pch[1], cex=0.8, bty="n")
  legend(0.6*min(range)+0.4*max(range), min(range)+0.06*(max(range)-min(range)), text[2], pch=pch[2], cex=0.8, bty="n")
}
par(mfrow=c(1,2), oma=c(0,0,1,0))
alpha_post <- extract(fit_3a)$alpha
beta_post <- extract(fit_3a)$beta
quants <- c(0.025, 0.25, 0.5, 0.75, 0.975)
alpha_post_quants <- array(NA, c(J,length(quants)), dimnames=list(NULL, quants))
beta_post_quants <- array(NA, c(K,length(quants)), dimnames=list(NULL, quants))
for (j in 1:J){
  alpha_post_quants[j,] <- quantile(alpha_post[,j], quants)
}
for (k in 1:K){
  beta_post_quants[k,] <- quantile(beta_post[,k], quants)
}
par(mar=c(3,3,2,1), mgp=c(1.7,.5,0), tck=-.02)
par(pty="s")
rng <- range(alpha_post_quants, alpha)
plot(rng, rng, xlab="Posterior median, 50%, and 95% interval", ylab="True parameter value", bty="l", type="n")
abline(0, 1, col="gray")
points(alpha_post_quants[genre=="romantic", "0.5"], alpha[genre=="romantic"], pch=1, cex=0.9)
points(alpha_post_quants[genre=="crime", "0.5"], alpha[genre=="crime"], pch=20)
for (j in 1:J){
  lines(c(alpha_post_quants[j,"0.25"], alpha_post_quants[j,"0.75"]), rep(alpha[j], 2), lwd=2)
  lines(c(alpha_post_quants[j,"0.025"], alpha_post_quants[j,"0.975"]), rep(alpha[j], 2), lwd=0.5)
}
add_legend(c("Romantic comedies", "Crime movies"), pch=c(1, 20), range=rng)
mtext("Checking the alpha_j's", side=3)
rng <- range(beta_post_quants, beta)
plot(rng, rng, xlab="Posterior median, 50%, and 95% interval", ylab="True parameter value", bty="l", type="n")
abline(0, 1, col="gray")
points(beta_post_quants[beta<0, "0.5"], beta[beta<0], pch=1, cex=0.9)
points(beta_post_quants[beta>0, "0.5"], beta[beta>0], pch=20)
for (k in 1:K){
  lines(c(beta_post_quants[k,"0.25"], beta_post_quants[k,"0.75"]), rep(beta[k], 2), lwd=2)
  lines(c(beta_post_quants[k,"0.025"], beta_post_quants[k,"0.975"]), rep(beta[k], 2), lwd=0.5)
}
add_legend(c("Nice raters", "Difficult raters"), pch=c(1, 20), range=rng)
mtext("Checking the beta_k's", side=3)
mtext("Checking fits for model when difficult reviewers were more likely to rate certain genres", side=3, outer=TRUE)
```

We next do some analysis to show how naive averaging of ratings will give misleading estimates of movie quality, and the model-based estimates correct for this bias.  We first compute the average observed rating, $\bar{y}_j$ for each movie $j$.
```{r, echo=FALSE}
ybar<- rep(NA, J)
for (j in 1:J){
  if (sum(movie==j & rated) == 0){
    ybar[j] <- 3    # Assign an average rating of 3 to any movie that has zero ratings
  }
  else {
    ybar[j] <- mean(y[movie==j & rated])
  }
}
```
Recall that in our model this maps to the transformed parameter $a_j=\mu + \sigma_a*\alpha_j$, the expected average rating that we would see if everyone in the population rated every movie.
We compute the posterior median of $a_j$ and also recall its true value from the process used to simulate the data.
```{r, echo=FALSE}
a <- mu + sigma_a * alpha
a_post <- extract(fit_3a)$a
a_post_median <- apply(a_post, 2, median)
```

We then plot true movie quality (the parameter , first versus raw average rating, then versus the model-based estimate:
```{r, echo=FALSE}
par(mfrow=c(1,2))
par(mar=c(3,3,2,1), mgp=c(1.7,.5,0), tck=-.02)
par(pty="s")

rng <- range(a_post_median, ybar, a)
plot(rng, rng, xlab="Raw average rating for movie j", ylab="True a_j", bty="l", type="n")
abline(0, 1, col="gray")
points(ybar[genre=="romantic"], a[genre=="romantic"], pch=1, cex=0.9)
points(ybar[genre=="crime"], a[genre=="crime"], pch=20)
mtext("Problems with raw averaging", side=3)
add_legend(c("Romantic comedies", "Crime movies"), pch=c(1, 20), range=rng)
plot(rng, rng, xlab="Posterior median estimate for movie j", ylab="True a_j", bty="l", type="n")
abline(0, 1, col="gray")
points(a_post_median[genre=="romantic"], a[genre=="romantic"], pch=1, cex=0.9)
points(a_post_median[genre=="crime"], a[genre=="crime"], pch=20)
mtext("Model-based estimates do better", side=3)
add_legend(c("Romantic comedies", "Crime movies"), pch=c(1, 20), range=rng)
mtext("Problems with raw averages when difficult reviewers were more likely to rate certain genres", side=3, outer=TRUE)
```

The raw averages for the romantic comedies are mostly too high:  The open circles on the first plot are mostly to the right of the diagonal line, implying that those averages $\bar{y}_j$ are higher than the true values of the movie quality parameters $a_j$.  Meanwhile the raw averages for the crime movies are mostly too low, with the solid dots on the first plot mostly to the left of the diagonal line.  This all makes sense, as we have constructed our simulation so that the romantic comedies are more likely to be rated by nicer reviewers, and the crime movies are more likely to rated by tougher reviewers.

The model adjusts for these biases, though, and so the model-based estimates, shown in the second plot above, do not have these systematic problems.

## Other potential extentions to the model

Here are some of the many ways in which the model could be generalized in order to make it more realistic:

* More popular movies should get more ratings.  So we might want to extend the model to allow the probability of a person rating a movie to depend on the movie's popularity, which here is coded by the parameter $\alpha_j$.

* Different people have different preferences for different genres.  The model could capture this by allowing each person to have a vector of difficulty parameters, one for each genre.

* Movie ratings are discrete; we could replace the normal distribution for $y_i$ by an ordered logistic model which would give probabilities of each of the discrete responses from 0 through 5.


