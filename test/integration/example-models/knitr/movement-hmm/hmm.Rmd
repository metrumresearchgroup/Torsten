---
title: "Hidden Markov Models in Stan"
author: "Bob Carpenter"
date: "`r format(Sys.Date(), format='%d %B %Y')`"
output:
  tufte::tufte_html:
    toc: true
    toc_depth: 1
    fig_caption: yes
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
    fig_caption: yes
link-citations: yes
---

```{r setup, include=FALSE, echo=FALSE}
options(htmltools.dir.version = FALSE)
options(digits = 2)
library(tufte)
library(ggplot2)
library(gridExtra)
library(knitr)
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
knitr::opts_chunk$set(comment = "")

library(reshape)

library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores(logical = FALSE))

library(tufte)

ggtheme_tufte <- function() {
  theme(plot.background =
          element_rect(fill = "#fffff8",
                       colour = "#fffff8",
                       size = 0.5,
                       linetype = "solid"),
        plot.margin=unit(c(1, 1, 0.5, 0.5), "lines"),
        panel.background =
          element_rect(fill = "#fffff8",
                       colour = "#fffff8",
                       size = 0.5,
                       linetype = "solid"),
        panel.grid.major = element_line(colour = "white", size = 1, linetype="dashed"),
          # blank(),
        panel.grid.minor = element_blank(),
        legend.box.background =
          element_rect(fill = "#fffff8",
                       colour = "#fffff8",
                       linetype = "solid"),
        axis.ticks = element_blank(),
        axis.text = element_text(family = "Palatino", size = 16),
        axis.title.x = element_text(family = "Palatino", size = 20,
                                    margin = margin(t = 15, r = 0, b = 0, l = 0)),
        axis.title.y = element_text(family = "Palatino", size = 18,
                                    margin = margin(t = 0, r = 15, b = 0, l = 0)),
        strip.background = element_rect(fill = "#fffff8",
                                        colour = "#fffff8",
                                        linetype = "solid"),
        strip.text = element_text(family = "Palatino", size = 16),
        legend.text = element_text(family = "Palatino", size = 16),
        legend.title = element_text(family = "Palatino", size = 16,
                                    margin = margin(b = 5)),
        legend.background = element_rect(fill = "#fffff8",
                                        colour = "#fffff8",
                                        linetype = "solid"),
        legend.key = element_rect(fill = "#fffff8",
                                        colour = "#fffff8",
                                        linetype = "solid")
  )
}

printf <- function(msg = "%5.3f", ...) {
  cat(sprintf(msg, ...))
}
```


## Abstract

A hidden Markov model (HMM) is a sequential finite mixture model where the sequence of latent (unobserved, or hidden) states determining the mixture component forms a Markov chain.  This case study focuses on efficient inference HMMs using dynamic programming.  Algorithms covered include (1) the forward algorithm for the likelihood of the observed sequence, (2) the forward-backward algorithm for marginal latent state probabilities and sufficient statistics, and (3) the Viterbi algorithm for the latent state sequence of highest posterior probability given the observed sequence.

Two examples are provided with full Stan programs.  The first example
is animal movement, with change in bearing (turn angle) and direction
traveled being observed and latent states corresponding to transiting
(long movement, small change in bearing), foraging (short movement,
large change in bearing), and resting (no movement).  The second
example is natural language sentence tagging.  The states are
partially observed in the training data and correspond to parts of
speech like noun and verb; the observed outputs are word sequences
like "Riley ran".  This is an example of semi-supervised machine
learning.



```{r echo=FALSE, fig.margin=TRUE, fig.cap="Histogram of 10000 simulated values with 60% chance of drawing from a population with mean of -2 and standard deviation of 0.75 and a 40% chance of drawing from a population with mean of 2 and standard deviation of 1.5.  Blue lines highlight the means of the two mixture components."}
N <- 10000
lambda <- 0.4
mu <- c(-2, 2)
sigma <- c(0.75, 1.5)
z <- 1 + rbinom(N, 1, lambda)
y <- rnorm(N, mu[z], sigma[z])
plot_mix_sim <- ggplot(data.frame(y = y), aes(x = y)) +
  geom_histogram(color="gray", bins=32) +
  scale_y_continuous(name = "count", breaks=c()) +
  scale_x_continuous(name = "y", lim = c(-6, 8), breaks=c(-6, -4, -2, 0, 2, 4, 6)) +
  geom_vline(xintercept = mu[1], color="red") +
  geom_vline(xintercept = mu[2], color="blue") +
  ggtheme_tufte()
plot_mix_sim
```



```{r echo=FALSE, fig.margin=TRUE, fig.cap="Plot of the mixture density of two normals corresponding to the previous plot.  The underlying mixture components are colored and scaled, with the dotted black line indicating the mixture density."}
plot_mix_density <-
  ggplot(data.frame(x = c(-6, 8)), aes(x = x)) +
  stat_function(fun = function(x) 0.6 * dnorm(x, mean = -2, sd = 0.75),
                colour="red", alpha = 0.6, size = 0.75) +
  stat_function(fun = function(x) 0.4 * dnorm(x, mean = 2, sd = 1.5),
                colour="blue", alpha = 0.4, size = 0.75) +
  stat_function(fun = function(x) 0.6 * dnorm(x, mean = -2, sd = 0.75) +
                                  0.4 * dnorm(x, mean = 2, sd = 1.5),
                colour="black", linetype="dotted", size = 1) +
  scale_y_continuous(name = "density", breaks=c()) +
  scale_x_continuous(name = "y", breaks=c(-6, -4, -2, 0, 2, 4, 6)) +
  ggtheme_tufte()
plot_mix_density
```

# Finite Mixture Models

The basic idea of a finite mixture model is that there is a mixture of two or more populations from which observations are drawn.  Each population is characterized by its own distribution, though these distributions may share parameters.  Each observation is drawn from a single population.


## Binary mixture model generative process

In a simple mixture model, there is a discrete responsibility
parameter determining from which mixture component an item is drawn.
For example, in a simple two-component normal mixture, $z_n$ is the
mixture component responsible for item $y_n$,
$$
\begin{array}{rcl}
z_n & \sim & \mathsf{Bernoulli}(\lambda)
\\
y_n & \sim & \mathsf{Normal}(\mu_{z_n}, \sigma_{z_n})
\end{array}
$$

The generative process can be explored by simulation.


## Binary mixture model likelihood

In order to compute the likelihood^[The likelihood is the distribution of the observed data given the parameters.  Data-specific parameters must be marginalized out in order to get the distribution of the data itself, as required for the likelihood.] $p(y_n \mid \lambda, \mu, \sigma)$, it is necessary to marginalize out the responsibility parameter $z_n$ from
$$
p(y_n, z_n \mid \lambda, \mu, \sigma)
\ = \
\mathsf{Bernoulli}(z_n \mid \lambda)
\cdot \mathsf{Normal}(y_n \mid \mu_{z_n}, \sigma_{z_n}).
$$
The discrete responsibility parameter $z_n$ governing the population from which $y_n$ is drawn may be marginalized out using the law of total probability,
$$
\begin{array}{rcl}
p(y_n \mid \lambda, \mu, \sigma)
& = &
\sum_{k=0}^1 p(y_n, z_n = k \mid \lambda, \mu, \sigma)
\\[4pt]
& = &
\sum_{k=0}^1
  \mathsf{Bernoulli}(k \mid \lambda)
  \cdot \mathsf{Normal}(y_n \mid \mu_k, \sigma_k)
\\[4pt]
& = &
\begin{array}[t]{l}
\mathsf{Bernoulli}(0 \mid \lambda) \cdot \mathsf{Normal}(y_n \mid \mu_0, \sigma_0)
\\
{ } \ + \
\mathsf{Bernoulli}(1 \mid \lambda) \cdot \mathsf{Normal}(y_n \mid \mu_1, \sigma_1)
\end{array}
\\[4pt]
& = &
(1 - \lambda) \cdot \mathsf{Normal}(y_n \mid \mu_0, \sigma_0)
\ + \  \lambda \cdot \mathsf{Normal}(y_n \mid \mu_1, \sigma_1).
\end{array}
$$


## Finite mixture model

The mixture model naturally extends beyond two components to an arbitrary number of components.  Now the mixing proportions $\theta$ are modeled as a $K$-simplex^[A $K$-*simplex* is a vector with $K$ non-negative entries that sum to one.].  

The generative model for the latent mixture component $z_n$ and observation $y_n$ can be written in full detail using sampling notation as
$$
\begin{array}{rcl}
z_n & \sim & \mathsf{Categorical}(\theta)
\\
y_n & \sim & \mathsf{Normal}(\mu_{z_n}, \sigma_{z_n})
\end{array}
$$

```{r, engine="tikz", fig.ext="pdf", fig.width=2, fig.margin=TRUE, echo=FALSE, external=FALSE, fig.cap="Directed graphical model sketch for a mixture of normal distributions.  $\\theta$ is a simplex for the mixture proportions, $z_n$ is the mixture component for observation $y_n$, and $\\mu_k$ and $\\sigma_k$ are the location and scale of mixture component $k$. The plates indicate $z$ and $y$ are size $N$ and $\\mu$ and $\\sigma$ are size $K$.  Edges indicate dependencies in the generative model."}
%
% Copyright 2010-2011 by Laura Dietz
% Copyright 2012 by Jaakko Luttinen
%
% The MIT License
%
% See LICENSE file for more details.

% Load other libraries
\usetikzlibrary{shapes}
\usetikzlibrary{fit}
\usetikzlibrary{chains}
\usetikzlibrary{arrows}

% Latent node
\tikzstyle{latent} = [circle,fill=white,draw=black,inner sep=1pt,
minimum size=20pt, font=\fontsize{10}{10}\selectfont, node distance=1]
% Observed node
\tikzstyle{obs} = [latent,fill=gray!25]
% Constant node
\tikzstyle{const} = [rectangle, inner sep=0pt, node distance=1]
% Factor node
\tikzstyle{factor} = [rectangle, fill=black,minimum size=5pt, inner
sep=0pt, node distance=0.4]
% Deterministic node
\tikzstyle{det} = [latent, diamond]

% Plate node
\tikzstyle{plate} = [draw, rectangle, rounded corners, fit=#1]
% Invisible wrapper node
\tikzstyle{wrap} = [inner sep=0pt, fit=#1]
% Gate
\tikzstyle{gate} = [draw, rectangle, dashed, fit=#1]

% Caption node
\tikzstyle{caption} = [font=\footnotesize, node distance=0] %
\tikzstyle{plate caption} = [caption, node distance=0, inner sep=0pt,
below left=5pt and 0pt of #1.south east] %
\tikzstyle{factor caption} = [caption] %
\tikzstyle{every label} += [caption] %

%\pgfdeclarelayer{b}
%\pgfdeclarelayer{f}
%\pgfsetlayers{b,main,f}

% \factoredge [options] {inputs} {factors} {outputs}
\newcommand{\factoredge}[4][]{ %
  % Connect all nodes #2 to all nodes #4 via all factors #3.
  \foreach \f in {#3} { %
    \foreach \x in {#2} { %
      \path (\x) edge[-,#1] (\f) ; %
      %\draw[-,#1] (\x) edge[-] (\f) ; %
    } ;
    \foreach \y in {#4} { %
      \path (\f) edge[->, >={triangle 45}, #1] (\y) ; %
      %\draw[->,#1] (\f) -- (\y) ; %
    } ;
  } ;
}

% \edge [options] {inputs} {outputs}
\newcommand{\edge}[3][]{ %
  % Connect all nodes #2 to all nodes #3.
  \foreach \x in {#2} { %
    \foreach \y in {#3} { %
      \path (\x) edge [->, >={triangle 45}, #1] (\y) ;%
      %\draw[->,#1] (\x) -- (\y) ;%
    } ;
  } ;
}

% \factor [options] {name} {caption} {inputs} {outputs}
\newcommand{\factor}[5][]{ %
  % Draw the factor node. Use alias to allow empty names.
  \node[factor, label={[name=#2-caption]#3}, name=#2, #1,
  alias=#2-alias] {} ; %
  % Connect all inputs to outputs via this factor
  \factoredge {#4} {#2-alias} {#5} ; %
}

% \plate [options] {name} {fitlist} {caption}
\newcommand{\plate}[4][]{ %
  \node[wrap=#3] (#2-wrap) {}; %
  \node[plate caption=#2-wrap] (#2-caption) {#4}; %
  \node[plate=(#2-wrap)(#2-caption), #1] (#2) {}; %
}

% \gate [options] {name} {fitlist} {inputs}
\newcommand{\gate}[4][]{ %
  \node[gate=#3, name=#2, #1, alias=#2-alias] {}; %
  \foreach \x in {#4} { %
    \draw [-*,thick] (\x) -- (#2-alias); %
  } ;%
}

% \vgate {name} {fitlist-left} {caption-left} {fitlist-right}
% {caption-right} {inputs}
\newcommand{\vgate}[6]{ %
  % Wrap the left and right parts
  \node[wrap=#2] (#1-left) {}; %
  \node[wrap=#4] (#1-right) {}; %
  % Draw the gate
  \node[gate=(#1-left)(#1-right)] (#1) {}; %
  % Add captions
  \node[caption, below left=of #1.north ] (#1-left-caption)
  {#3}; %
  \node[caption, below right=of #1.north ] (#1-right-caption)
  {#5}; %
  % Draw middle separation
  \draw [-, dashed] (#1.north) -- (#1.south); %
  % Draw inputs
  \foreach \x in {#6} { %
    \draw [-*,thick] (\x) -- (#1); %
  } ;%
}

% \hgate {name} {fitlist-top} {caption-top} {fitlist-bottom}
% {caption-bottom} {inputs}
\newcommand{\hgate}[6]{ %
  % Wrap the left and right parts
  \node[wrap=#2] (#1-top) {}; %
  \node[wrap=#4] (#1-bottom) {}; %
  % Draw the gate
  \node[gate=(#1-top)(#1-bottom)] (#1) {}; %
  % Add captions
  \node[caption, above right=of #1.west ] (#1-top-caption)
  {#3}; %
  \node[caption, below right=of #1.west ] (#1-bottom-caption)
  {#5}; %
  % Draw middle separation
  \draw [-, dashed] (#1.west) -- (#1.east); %
  % Draw inputs
  \foreach \x in {#6} { %
    \draw [-*,thick] (\x) -- (#1); %
  } ;%
}
\begin{tikzpicture}
    \node[latent] (theta) {$\theta$} ; %
    \node[latent, right=of theta] (z) {$z_n$} ; %
    \node[obs, right=of z] (y) {$y_n$} ; %
    \node[latent, above=of z] (mu) {$\mu_k$} ; %
    \node[latent, above=of y] (sigma) {$\sigma_k$} ; %
    \plate[inner sep=0.25cm, xshift=-0.12cm, yshift=0.12cm] {plateN} {(z) (y)} {$N$}; %
    \plate[inner sep=0.25cm, xshift=-0.12cm, yshift=0.12cm] {plateK} {(mu) (sigma)} {$K$}; %
    \edge {theta} {z} ; %
    \edge {z} {y} ; %
    \edge {mu, sigma} {y} ; %
\end{tikzpicture}
```

To complete the model, we need priors on $\theta$ (by default uniform) and on $\mu$ and $\sigma$ (by default improper uniform).


## Flexible mixture modeling

Although the example was of a simple mixture of two normals, any finite number of populations may be mixed.  The mixture model itself may be composed of a matching sequence of distributions, but they need not come from the same family or be simply distributions.  The component distributions are not even required to have the same support.  It's even possible in principle to have one discrete component and one continuous component.  The mixture models considered here will draw the mixture components from the same parametric family for simplicity.


# Markov Chains

Hidden Markov models extend the notion of mixture models to sequences
of observations $y = y_1, y_2, \ldots, y_n, \ldots$ generated from a
latent (unobserved) sequence of mixture components $z = z_1, z_2,
\ldots, z_n, \ldots$.^[A sequence of random variables such as $z$ is known as a
*discrete stochastic process*. If the sequence represents points in time, the
sequence is called a *time series.*]

These sequences are unbounded in principle, but data sets will always
be of finite subsequences $y_{1:N}$ and $z_{1:N}$, which will be
numbered from one for convenience.  This section concentrates on the distribution for $z$.


## Markov chains with finite state spaces

A hidden Markov model is based on a finite number of mixture
components.^[In general, a Markov chain may have continuously states,
as in the typical application to Bayesian inference using Markov chain
Monte Carlo (MCMC).]  For concreteness, suppose there are $K$ possible
states, numbered from one, so that $z_n \in 1{:}K$ for $n \in 1{:}N$.

The *state space* of a Markov chain $z$ is the set of possible values
for the sequence of random variables $z$.  Here, because we have
assumed discrete states $1{:}K$ and a sequence of length $N$, the
state space is
$$
(1{:}K)^N
\ = \
\{ z_1, \ldots, z_N \mid z_n \in 1{:}K \mbox{ for } n \in 1{:}N \}.
$$
The state space $(1{:}K)^N$ is of size $K^N$, so the number of
sequences in the state space grows exponentially in the length $N$ of
the sequence, with base $K$ determined by the number of possible
states.  Considerable effort will be required to mitigate this
combinatorial explosion in computing the required likelihoods.

## The Markov property

Hidden Markov models assume that the latent sequence $z$ of mixture
component responsibilities satisfies the Markov property.  The Markov
property requires element $z_n$ in the sequence to be conditionally
independent given the previous element $z_{n-1}$.  In words, a
Markovian system only has a memory of one time step.  A sequence of
random variables $z = z_1, z_2, \ldots, z_n, \ldots$ is a *Markov
chain* if it satisfies
$$
p\!\left(z_n \mid z_1, \ldots, z_{n-1}\right)
=
p\!\left(z_{n} \mid z_{n-1}\right)
$$
for all $n > 0$.  Just a little reminder that we will only be
observing finite subsequences of this unbounded sequence.

```{r, engine="tikz", fig.ext="pdf", fig.margin=TRUE, echo=FALSE, external=FALSE, fig.cap="Directed graphical model notation for a Markov process $z = z_1, z_2, \\ldots$.  The nodes are shaded to indicate that they are observed.  The arcs indicate that each node is conditionally independent given the previous node (i.e., the Markov property).  When used as the basis for an HMM, the Markov states will be hidden (i.e., not observed)."}
\begin{tikzpicture}[->, auto, node distance=1cm, font=\tiny]
\tikzstyle{state}=[circle,draw=black!70,fill=black!6,
                   inner sep=0pt,minimum size=5mm]
\node[state] (A) {$z_1$};
\node[state] (B) [right of=A] {$z_2$};
\node[state] (C) [right of=B] {$z_3$};
\node[circle] (D) [right of=C] {$\cdots$};
\node[state] (E) [right of=D] {$z_N$};
\path (A) edge [left] node {{}} (B);
\path (B) edge [left] node {{}} (C);
\path (C) edge [left] node {{}} (D);
\path (D) edge [left] node {{}} (E);
\end{tikzpicture}
```


## Stochastic transition matrixes

A discrete Markov chain with $K$ possible states can be parameterized
with a $K \times K$ stochastic matrix^[A square $K \times K$ matrix $\theta$ is a
*stochastic matrix* if each row vector $\theta_k = \theta_{k,1},
\ldots, \theta_{k,K}$ forms a unit $K\!$-simplex.  A $K\!$-vector is a
*unit simplex* if its elements are non-negative and sum to one.]
$\theta$, with
$$
z_n \sim \mathsf{Categorical}\!\left( \theta_{z_{n-1}} \right).
$$
This sampling notation is meant to indicate that $z_n$ is
conditionally independent given $z_{n-1}$ and indicate its
distribution, so that
$$
p\!\left(z_n \mid z_{n - 1}, \theta\right)
= \mathsf{Categorical}\!\left( z_n \mid \theta_{z_{n-1}} \right).
$$



## Markov chain initial state distribution

To get a chain started, there needs to be an initial distribution over
the first state, which may be parameterized with a unit $K$-simplex
$\psi$,
$$
z_1 \sim \mathsf{Categorical}\!\left( \psi \right).
$$

## Markov chain stationary distributions

Given a stochastic $K \times K$ matrix $\theta$ with no zero entries,^[This condition, originally due to Markov, may be relaxed slightly to aperiodic and ergodic stochastic matrixes.]
its *stationary distribution* is parameterized by the unique unit $K$-simplex
row vector $\psi$ such that
$$
\psi = \psi \, \theta
$$
It is important that $\psi$ is a row vector so that the dimensions align.
Breaking this down by component, the probability of being in state $j$
is
$$
\begin{array}{rcl}
\psi_j & = & \psi \, \theta_{\cdot,\, j}
\\
& = & \sum_{i=1}^K \psi_i \cdot \theta_{i,\, j}.
\end{array}
$$

In words, the probability of being in state $j$ is equal to the sum
over states $i$ of the probability of being in state $i$ ($\psi_i$)
and transitioning to state $j$ ($\theta_{i,\, j}$).  Each state
$z_i$ in the sequence has the stationary distribution as its marginal
distribution, so that for all $n \in 1{:}N$,
$$
p\!\left( z_n \right) = \ = \ \mathsf{Categorical}(\psi).
$$

## Calculating the stationary distribution

The stationary distribution may be computed by repeated multiplication of the stochastic matrix,^[Natural *powers of matrices* are defined in the base case by taking $\theta^0$ to be the identity matrix and inductively defining $\theta^{n + 1} = \theta^n \ \theta$.]
$$
\psi
\ = \
\lim_{n \rightarrow \infty}
\begin{bmatrix}
1/K & \cdots & 1/K
\end{bmatrix}
\,
\theta^n
$$
Because only the limit is needed, it is much more efficient to grow the sequence multiplicatively^[The multiplicative method is $\mathcal{O}(\log n)$ rather than the $\mathcal{O}(n)$ of the natural additive definition.] by setting
$$
\theta^{2 n} = \theta^n \theta^n.
$$
For example, consider the stochastic matrix
$$
\theta
\ = \
\begin{bmatrix}
0.9 & 0.1 \\
0.4 & 0.6
\end{bmatrix}.
$$
Multiplying it by itself gives
$$
\theta^2
\ = \
\begin{bmatrix}
0.85 & 0.15 \\
0.60 & 0.40
\end{bmatrix}.
$$
Continuing the sequence,
$$
\theta^4
\ = \
\begin{bmatrix}
0.81 & 0.19 \\
0.75 & 0.25
\end{bmatrix}
\ \ \ \ \ \
\theta^8
\ = \
\begin{bmatrix}
0.80 & 0.20 \\
0.80 & 0.20
\end{bmatrix}.
$$
The limit is
$$
\lim_{n \rightarrow \infty} \theta^n
\ = \
\begin{bmatrix}
0.8 & 0.2 \\
0.8 & 0.2
\end{bmatrix}
$$

After convergence of the sequence $\theta^n$, each row will be the same up to the required or available arithmetic precision, transitioning according to the stationary distribution.

To verify $\psi = \begin{bmatrix}0.8 & 0.2\end{bmatrix}$ is the stationary distribution for the stochastic matrix $\theta$,
$$
\psi \, \theta
\ = \
\begin{bmatrix}
0.8 & 0.2
\end{bmatrix}
\
\begin{bmatrix}
0.9 & 0.1 \\
0.4 & 0.6
\end{bmatrix}
=
\begin{bmatrix}
0.8 & 0.2
\end{bmatrix}.
$$



## Stationary distribution as initial distribution

For observations of a sequence taken from a longer sequence, it is
conventional to take the initial distribution to be the stationary
distribution.

## Independent draws from Markov chains

Each draw from a Markov chain (after convergence) is independently distributed.  But typically they are not independent---each state is only conditionally independent given the previous state.  In the special case where each row of the stochastic transition matrix is the same, then the stationary distribution is given by the rows, and
$$
\begin{array}{rcl}
p(z_{n + 1} \mid z_n)
& = & p(z_{n+1})
\\
& = & \mathsf{Categorical}(z_{n+1} \mid \psi).
\end{array}
$$


## Dirichlet prior for Markov model transitions

A convenient prior for the Markov transition matrix $\theta$ is an exchangeable Dirichlet distribution.^[The Dirichlet density for $K$-simplex $\lambda$ given parameter $K$-vector $\alpha$ with $\alpha_k > 0$, is $$\mathsf{Dirichlet}(\lambda \mid \alpha) \propto \prod_{k=1}^K \lambda_k^{\alpha_k - 1}.$$]

scalar $\alpha > 0$ and $K$-simplex $\theta$, let
$$
\mathsf{DirichletSym}(\psi \mid \alpha)
\ = \
\mathsf{Dirichlet}(\psi \mid [\underbrace{\alpha \ \alpha \ \cdots \ \alpha}_{K \ \mathrm{ times}}]^{\top})
$$
Writing out the density up to a proportion for a scalar parameter,
$$
\mathsf{DirichletSym}(\psi \mid \alpha)
\propto
\prod_{k=1}^K \psi_k^{\alpha - 1}.
$$

*Exchangeability*: It is easy to see the exchangeability in that the density is the same for any permutation of $\psi$.

*Uniformity*:  One convenient feature of the Dirichlet prior is that it reduces to a uniform distribution if $\alpha = 1$, because the $\alpha - 1$ exponent renders the density constant.  A simplex parameter receives a uniform prior by default in Stan.^[In general, priors are by default uniform on parameter values satisfying the declared constraints.]

*Conjugacy*: The Dirichlet is the conjugate prior for the multinomial distribution.  This means that if the prior is Dirichlet and the likelihood is multinomial, the posterior will be Dirichlet.  Conjugate priors act like fractional (or even negative) data counts for the distributions for which they are conjugate.  For the Dirichlet, the prior acts as $\alpha - 1$ prior counts for each of the $K$ possible states.  This renders the parameters of the posterior by addition.  The multinomial is defined for a one-dimensional size $K$ array of counts $y$ totaling $N = \sum_{k=1}^K y_k$, and $K$-simplex parameter $\theta$, by
$$
\mathsf{Multinomial}(y \mid N, \theta)
\ \propto \
\prod_{k=1}^K \theta_k^{y_k}.
$$
Suppose the sampling distribution is given by $y \sim \mathsf{Multinomial}(N, \theta)$ and the prior by $\theta \sim \mathsf{Dirichlet}(\alpha)$ for $K$-vector parameter $\alpha$.  Then the posterior is proportional to the product of the sampling distribution and the prior,
$$
\begin{array}{rcl}
p(\theta \mid y)
& \propto &
\mathsf{Dirichlet}(\theta \mid \alpha)
\cdot \mathsf{Multinomial}(y \mid N, \theta)
\\[2pt]
& \propto &
\prod_{k=1}^K \theta^{\alpha_k \, - \, 1}
\prod_{k=1}^K \theta^{y_k}
\\[2pt]
& = &
\prod_{k=1}^K \theta^{y_k \, + \, \alpha_k \, - \, 1}
\\[2pt]
& \propto &
\mathsf{Dirichlet}(\theta \mid y + \alpha).
\end{array}
$$
This conjugacy may be exploited in Stan by coding the posterior using the final Dirichlet rather than as a product of a Dirichlet and multinomial.

*Non-exchangeable priors*

An exchangeable, weakly informative prior intended to allow some probability of each state transitioning to any other state might be $\mathsf{Dirichlet}(5\,\mathrm{I}_K)$ or even $\mathsf{Dirichlet}(10\,\mathrm{I}_K)$.

The major advantage of a Dirichlet prior is that it's conjugate to the categorical sampling defining the model.  This means that if we observe the state sequence $z$, we can count the number of transitions and immediately calculate the posterior analytically.


## Higher-order Markov models

Markov models have only a single sep of memory.  Each latent state is conditionally independent given the previous state.  But what if a process has a longer-term memory?  If the process is discrete, it is easy to expand the state space to a product space and effectively *reduce*^[Reduction is a powerful technique used in many facets of theoretical computer science to establish orderings and equivalence classes among algorithms.] a higher-order Markov model to a first-order model.

The second-order example illustrates how the reduction goes in general.  For example, consider a state sequence $z = z_1, \ldots, z_N$ with second-order history, 
$$
p(z_n \mid z_{n - 1}, z_{n - 2}, \ldots, z_1) 
\ = \ p(z_n \mid  z_{n - 1}, z_{n - 2}),
$$
so that a state $z_n$ is conditionally independent of other states given the previous two states.

If there are $K$ possible values for $z_n$, define a new state space with $K^2$ states using pairs $(k, k') \in K \times K$.  Then the non-zero transition probabilities are
$$
\theta_{(k,k'),\, (k',k'')} = p(z_n = k'' \mid z_{n-1} = k', z_{n - 2} = k).
$$
The $k'$ index is duplicated here across the two states to maintain the history. The transition goes from a state with history $(k, k')$ to a state with history $(k', k'')$---the state $k$ is dropped from the history, $k'$ goes back one position, and $k'' is added.  The initial stationary distribution is calculated in the same way, with the states now being pairs.

For state pairs where the inner states don't match, the transition probability is zero.  The stationary distribution may be computed in the same way.^[The existence proof is beyond the scope of this case study---it requires establishing more general recurrence conditions under which Markov chains have stationary distributions.]


# Hidden Markov Models

## Observed data

The fundamental unit of observed data for an HMM likelihood is a
sequence $y = y_1, \ldots, y_N$ where each $y_n \in \mathcal{Y}$ for
some domain of observation $\mathcal{Y}$.  These $y_n$ may be binary,
categorical, counts, concentrations, locations, distances, etc., or
they may be multivariate quantities consisting of more than one
quantity.

## HMM Generative process

The generative process for a hidden Markov model is to

1. generate a transition matrix $\theta$ from its prior $p(\theta)$,
2. generate parameters $\phi = \phi_1, \ldots, \phi_K$ for the $K$
  mixture components from their joint prior $p(\phi)$,
3. generate the first latent state $z_1$ from the stationary
  distribution of $\theta$ (or another initial distribution)
4. for each subsequent state $z_n$, generate it from the
  categorical distribution based on the previous state according to
  $\theta_{z_{n-1}}$
5. for each element $y_n$ of the observed state sequence, generate it
  from the distribution parameterized by $\phi_{z_n}$

## HMM joint probability function

The joint probability function implied by the generative process may
be factored as
$$
\begin{array}{rcl}
p(y, z, \theta, \phi)
& = &
p(y, z \mid \theta, \phi) \cdot p(\theta, \phi)
\\[4pt]
& = &
p(y \mid z, \phi) \cdot p(z \mid \theta) \cdot p(\theta) \cdot p(\phi)
\\[4pt]
& = &
\begin{array}[t]{l}
\prod_{n=1}^N p(y_n \mid \phi_{z_n})
\\[2pt]
{ } \cdot p(z_1 \mid \theta) \cdot \prod_{n=2}^N p(z_n \mid \theta_{z_{n-1}})
\\[2pt]
{ } \cdot p(\theta) \cdot p(\phi)
\end{array}
\end{array}
$$

The exact form of the $z_n$ terms was shown in the previous section on
Markov models.  The forms of the other terms will be application
dependent.

## HMM likelihood

The term $p(\theta, \phi)$ is the prior, but the likelihood is a
little more difficult to discern because of the complication of the
latent state sequence $z$.  The terms $\theta$ and $\phi$ are
parameters and $y$ is the observed data, but where does that leave
$z$?  It's latent, unobserved data in one sense and a parameter in
another sense.  Because teh number of $z$ parameters grows with the
data size $y$, the likelihood function does not include the $z$
parameters, but rather marginalizes them out, yielding the likelihood
$$
p(y \mid \theta, \phi)
\ = \
\sum_{z \, \in \, (1{:}K)^N} p(y, z \mid \theta, \phi).
$$
The point of the first algorithm introduced, the forward algorithm is
to compute the likelihood efficiently.


## Observed latent states and semi-supervised estimation

In some situations, the latent states $z_1, \ldots, z_N$ are
observed.  An example is when there is human generated or other
training data.

# The Forward Algorithm

The forward algorithm is a *dynamic programming algorithm*^[Dynamic programming typically involves caching or "memoizing" intermediate values for use in computing subsequent values; dynamic programming algorithms typically compute exponentially sized summations in polynomial time.] for computing the seemingly intractable summation in the definition of the likelihood in time $\mathcal{O}(N \cdot K^2)$ where $N$ is the length of the sequence and $K$ the number of latent states.

## Forward algorithm target values

The forward algorithm operates by inductively calculating an $N \times
K$ array of forward values
$$
\alpha_{n, k} = p(y_1, \ldots, y_n, z_n = k \mid \theta, \phi).
$$

The likelihood^[The likelihood considers $p(y \mid \theta, \phi)$ as a function of $\theta$ and $\phi$ for fixed data $y$; the sampling distribution considers $p(y \mid \theta, \phi)$ as a probability function over observations $y$ given fixed parameters $\theta$ and $\phi$.] is then calculated in the final step as
$$
\begin{array}{rcl}
p(y \mid \theta, \phi)
& = & \sum_{k = 1}^K p(y_1, \ldots, y_N, z_N = k \mid \theta, \phi)
\\[4pt]
& = & \sum_{k = 1}^K \alpha_{N, \, k}.
\end{array}
$$


## Base case of forward algorithm

The algorithm proceeds inductively, from $n = 1$ to $n = N$.  For $k
\in 1{:}K$,
$$
\begin{array}{rcl}
\alpha_{1, \, k}
& = & p(y_1, z_1 = k \mid \phi, \theta)
\\
& = & p(y_1 \mid \phi_k) \cdot p(z_1 = k \mid \theta)
\\
& = & p(y_1 \mid \phi_k) \cdot \mathrm{stationary}(\theta)_k
\end{array}
$$
The emission probability function $p(y_1 \mid \phi_k)$ will be
application dependent.  The initial state probability is taken to be
the stationary distribution of $\theta$, as discussed in the section
on Markov models.^[This assumption could be replaced by providing an
initial distribution $\psi$ and replacing
$\mathrm{stationary}(\theta)_k$ with $\psi_k$.]

## Inductive case of forward algorithm

For $n > 0$, $\alpha_{n + 1}$ is defined inductively in terms of
$\alpha_n$ by
$$
\begin{array}{rcl}
\alpha_{n + 1, \, k}
& = & p(y_1, \ldots, y_{n + 1}, z_{n + 1} = k)
\\[4pt]
& = &
p(y_{n + 1} \mid \phi_k)
\cdot \sum_{k' = 1}^K p(y_1, \ldots, y_n, z_n = k' \mid \phi, \theta)
                      \cdot p(z_{n + 1} = k \mid z_n = k', \theta)
\\[4pt]
& = &
p(y_{n + 1} \mid \phi_k)
\cdot \sum_{k' = 1}^K \alpha_{n - 1,\, k'} \cdot \theta_{k', k}
\end{array}
$$

## Likelihood calculation with the forward algorithm

As noted above, the total likelihood $p(y \mid \theta, \phi)$ may be
calculated directly from $\alpha_N$, the final forward values
calculated.
$$
\begin{array}
p(y \mid \theta, \phi)
& = & \sum_{k = 1} p(y, z_N = k \mid \theta, \phi)
\\[4pt]
& = & \sum_{k = 1} \alpha_{N, k}.
\end{array}
$$


## Forward algorithm on the log scale

The forward algorithm on the probability scale is prone to overflow by
multiplying many probability function values.  To mitigate this
problem, the forward algorithm is traditionally calculated on the log
scale.  The base case becomes
$$
\log \alpha_{1, k}
\ = \
\log p(y_1 \mid \phi_k) + \log \mathrm{stationary}(\theta)_k.
$$
The inductive case is then
$$
\begin{array}{rcl}
\log \alpha_{n+1, k}
& = &
\log p(y_{n + 1} \mid \phi_k)
+ \log \sum_{k' = 1}^K
  \exp\left(
    \log \alpha_{n, \, k'} + \log \theta_{k', k}
  \right)
\\[4pt]
& = & \log p(y_{n + 1} \mid \phi_k)
+ \mathrm{log\_sum\_exp}_{k=1}^K \log \alpha_{n, k} + \log \theta_{k',k}
\end{array}
$$
where we use the arithmetically stable compound log sum of exponents operation,^[The log-sum-exp operation both prevents overflow and maintains high precision by calculating as $\mathrm{log\_sum\_exp}_{k=1}^K u_k$ as $$\max(u) + \log \sum_{k=1}^K \exp\left( u_k - \max(u) \right).$$  Only values less than zero are exponentiated and leading digit accuracy is preserved with $\max(u)$.]
$$
\mathrm{log\_sum\_exp}_{k = 1}^K u_k
\ = \
\log \sum_{k=1}^K \exp(u_k).
$$

The final log likelihood is then just
$$
\begin{array}{rcl}
\log p(y \mid \theta, \phi)
& = &
\log \sum_{k=1}^K \exp\left( \log \alpha_{N, k} \right).
\\[4pt]
& = & \mathrm{log\_sum\_exp}_{k = 1}^K  \log \alpha_{N, k}.
\end{array}
$$

## Forward algorithm and complexity

The forward algorithm is very simple

1.  For $k \in 1{:}K$, calculate $\log \alpha_{1, k}$
2.  for $n \in 2{:}N$, for $k \in 1{:}K$, calculate $\log \alpha_{n,\, k}$ based on $\log \alpha_{n-1}$

Step (1) involves $K$ evaluations of $\log \alpha_{1, k}$, each of which has constant cost.  Step (2) involves $(N - 1) \cdot K$ evaluations of an $\alpha_{n, k}$ term, each of which involves $K$ steps (one for each possible previous state).  Thus the overall algorithm is $\mathcal{O}(K + N \cdot K^2) = \mathcal{O}(N \cdot K^2)$.  This is linear in the size $N$ of the input with a constant factor that's quadratic in the number of states.


# The Forward-Backward Algorithm

The forward-backward algorithm adds a backward dynamic programming algorithm to the forward algorithm to copute sufficient statistics for efficient inference in hidden Markov models. Specifically, the forward-backward algorithm computes marginal emission probabilities $p(z_n = k \mid y, \theta, \phi)$ and marginal transition probabilities $p(z_{n - 1} = k', z_n = k' \mid y, \theta, \phi)$.  These may be collected into sufficient statistics for estimating $\phi$ and $\theta$, and is thus the key to efficient inference with hidden Markov models.

## Backward algorithm target values

The backward algorithm targets the conditional probability of completing the sequence $y_1, \ldots, y_n$ with $y_{n+1}, \ldots, y_N$ given that the state $z_n = k$,
$$
\beta_{n,\, k} = p(y_{n+1}, \ldots, y_N \mid z_n = k, \theta, \phi).
$$

## Backward algorithm base case

Because the algorithm works from the end of the sequence back to the front, the base case is for the last position,
$$
\begin{array}{rcl}
\beta_{N,\, k}
& = &
p(\epsilon \mid z_N = k, \theta, \phi)
\\
& = & 1.
\end{array}
$$
The $\epsilon$ here indicates the empty sequence (the sequence of size zero), which is why the value is one---there is only one possible empty sequence, so the distribution is discrete with one outcome, and hence has probability one.

## Backward algorithm inductive case

The backwards algorithm is so named because its induction works from $N$ down to $1$, i.e., backwards compared to the forward algorithm.  The inductive case defining the components of $\beta_{n-1}$ in terms of $\beta_n$ is
$$
\begin{array}{rcl}
\beta_{n - 1, \, k}
& = & p(y_n, \ldots, y_N \mid z_n = k, \theta, \phi)
\\[4pt]
& = &
\sum_{k' = 1}^K 
\begin{array}[t]{l}
p(z_n = k' \mid z_{n - 1} = k, \theta)
\\
{ } \cdot p(y_n \mid z_n = k', \phi)
\\
{ } \cdot p(y_{n + 1}, \ldots, y_N \mid z_n = k', \theta, \phi)
\end{array}
\\[4pt]
& = &
\sum_{k' = 1}^K  \theta_{k,\, k'}
                \cdot p(y_n \mid \phi_{k'})
		\cdot \beta_{n,\, k'}
\end{array}
$$

## Forward-backward target values

The product of the forward and backward values at a given position are the basis of the marginal calculations we need.  A new forward-backward term $\gamma$ is defined as the elementwise products of the $\alpha$ and $\beta$ arrays.  Considering an element, that reduces to
$$
\begin{array}{rcl}
\gamma_{n, k}
& = & \alpha_{n, k} \cdot \beta_{n, k}
\\
& = &
p(y_1, \ldots, y_n, z_n = k \mid \theta, \phi)
\cdot p(y_{n + 1}, y_N \mid z_n = k \mid \theta, \phi)
\\
& = &
p(y_1, \ldots, y_N, z_n = k \mid \theta, \phi)
\\
& = &
p(y, z_n = k \mid \theta, \phi)
\end{array}
$$

## Forward-backward on the log scale

In order to provide stable arithmtic, the backward algorithm is computed on the log scale, as is the combination.  The base case is straightforward,
$$
\log \beta_{N, k} \ = \ \log 1 \ = \ 0.
$$
For the induction, the summation on the original scale becomes a log sum of exponentials on the log scale,
$$
\log \beta{n - 1,\, k}
= \mathrm{log\_sum\_exp}_{k' = 1}^K
    \log \theta_{k, k'}
    + \log p(y_n \mid \phi_{k'})
    + \log \beta_{n, k'}.
$$
The combined forward-backward target is also computed on the log scale,
$$
\log \gamma_{n, k} = \log \alpha_{n, k} + \log \beta_{n, k}.
$$

## Marginal responsibilities with forward-backward

The marginal we care about is $p(z_n = k \mid y, \theta, \phi)$, the probability the element $y_n$ of $y = y_1, \ldots, y_N$ is generated from mixture component $k$.  This conditional probability is calculated from the joint probability.
$$
\begin{array}{rcl}
p(z_n = k \mid y, \theta, \phi)
& = &
\frac{\displaystyle p(z_n = k, y \mid \theta, \phi)}
     {\displaystyle p(y \mid \theta, \phi)}
\\[6pt]
& = & \frac{\displaystyle \gamma_{n,\, k}}
           {\sum_{k'=1}^K \displaystyle \alpha_{N,\, k'}}.
\end{array}
$$
The numerator is what is calculated by the forward-backward algorithm, and the denominator is the likelihood, calculated by the sum of the final $\alpha_N$ values in the forward algorithm.

On the log scale,
$$
\begin{array}{rcl}
\log p(z_n = k \mid y, \theta, \phi)
& = &
\log p(z_n = k, y \mid \theta, \phi)
- \log p(y \mid \theta, \phi)
\\
& = &
\log \gamma_{n,\, k}
- \mathrm{log\_sum\_exp}_{k' = 1}^K \log \alpha_{N,\, k'}.
\end{array}
$$
Here, the term on the left is the log forward-backward target, whereas the term on the right is the log likelihood as computed with the forward algorithm.

## Marginal transitions with forward-backward

The marginal transitions are calculated much like the marginal states using the forward and backward targets. The difference is that that there's a transition mediating the forward and backward values.
$$
\begin{array}{rcl}
p(z_{n - 1} = k, z_n = k', y \mid \theta, \phi)
& = &
\begin{array}[t]{l}
p(y_1, \ldots, y_{n-1}, z_{n-1} = k \mid \theta, \phi)
\\
{ } \cdot p(z_n = k' \mid z_n = k, \theta)
\\
{ } \cdot p(y_n, \ldots, y_N \mid z_n = k' \mid \theta, \phi)
\end{array}
\\[4pt]
& = &
\alpha_{n-1,\, k} \cdot \theta_{k,\, k'} \cdot \beta{n,\, k'}.
\end{array}
$$

To get the conditional transition probabilities given $y$, we divide by the marginal for $y$, as before,
$$
\begin{array}{rcl}
p(z_{n - 1} = k, z_n = k' \mid y, \theta, \phi)
& = &
\frac{\displaystyle p(z_{n - 1} = k, z_n = k' \mid y, \theta, \phi)}
     {\displaystyle p(y \mid \theta, \phi)}
\\[4pt]
& = &
\frac{\displaystyle \alpha_{n-1,\, k}
                    \cdot \theta_{k,\, k'}
		    \cdot \beta{n,\, k'}}
     {\sum_{k'' = 1}^K \displaystyle \alpha_{N,\, k''}}
\end{array}
$$





## Collecting sufficient statistics

Given $p(z_{n-1} = k, z_n = k' \mid y, \phi, \theta)$, we can collect the
expectation for $z_{1,\, k}$ conditioned on the data as
$$
I_k = p(z_1 = k \mid y, \theta, \phi).
$$

The number of expected transitions from state $k$ to state $k'$ given $y$ as
$$
T_{k,\, k'}
\ = \
\sum_{n = 2}^N \, p(z_{n-1} = k, z_n = k \mid y, \theta, \phi).
$$
Along with $p(z_1 = k \mid y, \theta)$, this provides a sufficient statistic for estimating the stochastic transition matrix $\theta$.

Similarly, we can collect the sufficient statistics for emissions.
$$
E_{n,\, k} = p(z_n = k \mid y, \theta, \phi).
$$
If there are duplicate $y_n$ values, which is unlikely in most applications, the emission statistics may be summed.

Together, the statistics $T_{k,\, k'}$ and $E_{n, k}$ may be used to define the log density.
$$
\log p(y, z \mid \theta, \phi)
\ = \
\begin{array}[t]{l}
\sum_{k = 1}^K I_k \cdot \log \psi_k
\\[2pt]
{ } + \sum_{k=1}^K \sum_{k'=1}^K T_{k,\, k'} \cdot \log \theta_{k,\, k'}
\\[2pt]
{ } + \sum_{n=1}^N \sum_{k=1}^K E_{n,\, k} \cdot \log p(y_n \mid \phi_k).
\end{array}
$$
The variable $\psi$ here is initial distribution, assumed to be equal to the stationary distribution of the transition matrix, $\psi = \mathrm{stationary}(\theta)$.

# Viterbi Decoder

The Viterbi decoder is an algorithm for finding the highest probability state sequence $z$ given an observed sequence $y$, i.e.,^[In general, $\mathrm{argmax}_u \ f(u)$ is the $u$ that maximizes $f(u)$.  For example, $$\mathrm{argmax}_y\ -\frac{1}{2} \cdot (y-3)^2 = 3.$$  If the range of the maximization is restricted, the notation is $\mathrm{argmax}_{u \, \in \, \mathcal{U}} \ f(u)$.]
$$
z^* = \mathrm{argmax}_z \ p(z \mid y, \theta, \phi).
$$
By Bayes's rule, $p(z \mid y, \theta, \phi) \propto p(y, z \mid \theta, \phi)$, so
$$
z^* = \mathrm{argmax}_z \ p(y, z \mid \theta, \phi).
$$

## Best density and back pointers

The Viterbi algorithm builds up two data structures as it goes, both in an $N \times K$ structure, where $N$ is the number of elements in the observed sequence $y$ and $K$ is the number of latent states.  The first structure, $\delta_{n,k}$, holds the best-path density up to observation $n$ with latent state $z_n = k$.  

The second structure, $\xi_{n, k} \in 0{:}K$, holds an integer index to a previous state (the value may only be 0 for $n = 1$).  The point of this back-pointer structure is that the best path can be read off of the best final state by following the pointers backward.  

## Base cases

These two structures are defined together, starting with the base cases.  For the initial log density, we take
$$
\delta_{1, k} = p(y_1 \mid z_1 = k, \phi) \cdot p(z_1 = k \mid \theta).
$$
This is the same initial condition as for the forward algorithm, $p(y_1, z_1 = k \mid \theta, \phi)$.  

The initial backpointer can hold a dummy value as the initial state is the last when traversing backward.  To distinguish the final entry when going backward, the first backpointers are arbitrarily set to zero,
$$
\xi_{1, k} = 0.
$$

## Inductive cases

The inductive case for the best density is again very much like the forward inductive step, only taking a maximum rather than a sum,
$$
\delta_{n + 1,\, k} 
\ = \
\max_{k' \, \in \, 1:K}
\delta_{n,\, k'} \cdot p(y_{n + 1} \mid z_{n + 1} = k, \phi) \cdot p(z_{n + 1} = k \mid z_n = k', \theta).
$$

The backpointer is defined to be the $k'$ that led to the highest density.  This can be achieved by replacing the max operator used to define $\delta$ with an argmax operator,
$$
\DeclareMathOperator*{\argmax}{argmax}
\xi_{n + 1,\, k}
\ = \
\argmax_{k' \, \in \, 1:K} \
\delta_{n,\, k'} \cdot p(y_{n + 1} \mid z_{n + 1} = k, \phi) \cdot p(z_{n + 1} = k \mid z_n = k', \theta).
$$

## Extracting the best state sequence

Given the output densities $\delta$ and backpointers $\xi$ for an input $y_1, \ldots, y_N$, the optimal state sequence $z_1, \ldots, z_N$ is best path is constructed backwards, setting
$$
z_N = \argmax_k \, \delta_{N,\, k}.
$$
The rest of the state sequence is then defined inductively, setting for $n \in 1{:}N - 1$,
$$
z_{n - 1} = \xi_{n, z_n}.
$$

## Max-product and sum-product

For a given observed state sequence $y$, the forward algorithm computes the likelihood, which is the probability function summed over all possible latent states,
$$
\sum_{z \in (1:K)^n} \ p(y, z \mid \theta, \phi).
$$
The Viterbi decoder computes the state sequence for which $y$ is most likely,
$$
\argmax_{z \, \in \, (1:K)^n} \ p(y, z \mid \theta, \phi).
$$
The Viterbi decoder replaces the sum in the forward algorithm with a maximization.

The base cases are identical for the two algorithms.  The only difference between the inductive
step is that the forward algorithm uses a sum and the Viterbi algorithm a maximization.  The Viterbi decoder is an instance of a *max-product* algorithm, whereas the forward algorithm is a *sum-product* algorithm.  The max-product algorithm and sum-product algorithm generalize the Viterbi decoder and forward algorithm to directed acyclic graphical dependency structures such as trees.