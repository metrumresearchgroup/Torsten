<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.53">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Regression Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../stan-users-guide/time-series.html" rel="next">
<link href="../stan-users-guide/index.html" rel="prev">
<link href="../img/logo_tm.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 200,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../theming/quarto_styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../img/logo_tm.png" alt="Stan logo" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Overview</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../stan-users-guide/index.html" aria-current="page"> 
<span class="menu-text">Stan Users Guide</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../reference-manual/index.html"> 
<span class="menu-text">Reference Manual</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../functions-reference/index.html"> 
<span class="menu-text">Functions Reference</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-interfaces" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Interfaces</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-interfaces">    
        <li>
    <a class="dropdown-item" href="../cmdstan-guide/index.html">
 <span class="dropdown-text">CmdStan</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/cmdstanpy">
 <span class="dropdown-text">CmdStanPy</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/cmdstanr">
 <span class="dropdown-text">CmdStanR</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/users/interfaces/pystan.html">
 <span class="dropdown-text">PyStan</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/rstan">
 <span class="dropdown-text">RStan</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="http://stanjulia.github.io/Stan.jl/stable/INTRO/">
 <span class="dropdown-text">Stan.jl</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-other-packages" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Other Packages</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-other-packages">    
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/bayesplot/">
 <span class="dropdown-text">bayesplot</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://roualdes.github.io/bridgestan/latest/">
 <span class="dropdown-text">BridgeStan</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://paul-buerkner.github.io/brms/">
 <span class="dropdown-text">brms</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/loo/">
 <span class="dropdown-text">loo</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/posterior">
 <span class="dropdown-text">posterior</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/projpred">
 <span class="dropdown-text">projpred</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/rstanarm">
 <span class="dropdown-text">rstanarm</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/rstantools">
 <span class="dropdown-text">rstantools</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/shinystan">
 <span class="dropdown-text">shinystan</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/stan-dev" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
    <a href="https://discourse.mc-stan.org" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-chat-text-fill"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../stan-users-guide/regression.html">Example Models</a></li><li class="breadcrumb-item"><a href="../stan-users-guide/regression.html">Regression Models</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Stan User’s Guide</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Version 2.35</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Example Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/regression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Regression Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/time-series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Time-Series Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/missing-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Missing Data and Partially Known Parameters</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/truncation-censoring.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Truncated or Censored Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/finite-mixtures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Finite Mixtures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/measurement-error.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Measurement Error and Meta-Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/latent-discrete.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Latent Discrete Parameters</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/sparse-ragged.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sparse and Ragged Data Structures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Clustering Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/gaussian-processes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Gaussian Processes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/hyperspherical-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Directions, Rotations, and Hyperspheres</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/algebraic-equations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Solving Algebraic Equations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/odes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ordinary Differential Equations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/one-dimensional-integrals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Computing One Dimensional Integrals</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/complex-numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Complex Numbers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/dae.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Differential-Algebraic Equations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/survival.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Survival Models</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Programming Techniques</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/floating-point.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Floating Point Arithmetic</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/matrices-arrays.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Matrices, Vectors, Arrays, and Tuples</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/multi-indexing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Multiple Indexing and Range Indexing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/user-functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">User-Defined Functions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/custom-probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Custom Probability Functions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/proportionality-constants.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Proportionality Constants</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/problematic-posteriors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Problematic Posteriors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/reparameterization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reparameterization and Change of Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/efficiency-tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficiency Tuning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/parallelization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Parallelization</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Posterior Inference &amp; Model Checking</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/posterior-prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Posterior Predictive Sampling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/simulation-based-calibration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Simulation-Based Calibration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/posterior-predictive-checks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Posterior and Prior Predictive Checks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/cross-validation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Held-Out Evaluation and Cross-Validation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/poststratification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Poststratification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/decision-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Decision Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/bootstrap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Bootstrap and Bagging</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/using-stanc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Using the Stan Compiler</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/style-guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Stan Program Style Guide</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/for-bugs-users.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transitioning from BUGS</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#regression-models" id="toc-regression-models" class="nav-link active" data-scroll-target="#regression-models">Regression Models</a>
  <ul class="collapse">
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression">Linear regression</a>
  <ul class="collapse">
  <li><a href="#vectorization.section" id="toc-vectorization.section" class="nav-link" data-scroll-target="#vectorization.section">Matrix notation and vectorization</a></li>
  </ul></li>
  <li><a href="#QR-reparameterization.section" id="toc-QR-reparameterization.section" class="nav-link" data-scroll-target="#QR-reparameterization.section">The QR reparameterization</a></li>
  <li><a href="#regression-priors.section" id="toc-regression-priors.section" class="nav-link" data-scroll-target="#regression-priors.section">Priors for coefficients and scales</a></li>
  <li><a href="#robust-noise-models" id="toc-robust-noise-models" class="nav-link" data-scroll-target="#robust-noise-models">Robust noise models</a></li>
  <li><a href="#logistic-probit-regression.section" id="toc-logistic-probit-regression.section" class="nav-link" data-scroll-target="#logistic-probit-regression.section">Logistic and probit regression</a></li>
  <li><a href="#multi-logit.section" id="toc-multi-logit.section" class="nav-link" data-scroll-target="#multi-logit.section">Multi-logit regression</a>
  <ul class="collapse">
  <li><a href="#identifiability" id="toc-identifiability" class="nav-link" data-scroll-target="#identifiability">Identifiability</a></li>
  </ul></li>
  <li><a href="#parameterizing-centered-vectors" id="toc-parameterizing-centered-vectors" class="nav-link" data-scroll-target="#parameterizing-centered-vectors">Parameterizing centered vectors</a>
  <ul class="collapse">
  <li><a href="#k-1-degrees-of-freedom" id="toc-k-1-degrees-of-freedom" class="nav-link" data-scroll-target="#k-1-degrees-of-freedom"><span class="math inline">\(K-1\)</span> degrees of freedom</a></li>
  <li><a href="#qr-decomposition" id="toc-qr-decomposition" class="nav-link" data-scroll-target="#qr-decomposition">QR decomposition</a></li>
  <li><a href="#translated-and-scaled-simplex" id="toc-translated-and-scaled-simplex" class="nav-link" data-scroll-target="#translated-and-scaled-simplex">Translated and scaled simplex</a></li>
  <li><a href="#soft-centering" id="toc-soft-centering" class="nav-link" data-scroll-target="#soft-centering">Soft centering</a></li>
  </ul></li>
  <li><a href="#ordered-logistic.section" id="toc-ordered-logistic.section" class="nav-link" data-scroll-target="#ordered-logistic.section">Ordered logistic and probit regression</a>
  <ul class="collapse">
  <li><a href="#ordered-logistic-regression" id="toc-ordered-logistic-regression" class="nav-link" data-scroll-target="#ordered-logistic-regression">Ordered logistic regression</a></li>
  </ul></li>
  <li><a href="#hierarchical-regression" id="toc-hierarchical-regression" class="nav-link" data-scroll-target="#hierarchical-regression">Hierarchical regression</a></li>
  <li><a href="#hierarchical-priors.section" id="toc-hierarchical-priors.section" class="nav-link" data-scroll-target="#hierarchical-priors.section">Hierarchical priors</a>
  <ul class="collapse">
  <li><a href="#boundary-avoiding-priors-for-mle-in-hierarchical-models" id="toc-boundary-avoiding-priors-for-mle-in-hierarchical-models" class="nav-link" data-scroll-target="#boundary-avoiding-priors-for-mle-in-hierarchical-models">Boundary-avoiding priors for MLE in hierarchical models</a></li>
  </ul></li>
  <li><a href="#item-response-models.section" id="toc-item-response-models.section" class="nav-link" data-scroll-target="#item-response-models.section">Item-response theory models</a>
  <ul class="collapse">
  <li><a href="#data-declaration-with-missingness" id="toc-data-declaration-with-missingness" class="nav-link" data-scroll-target="#data-declaration-with-missingness">Data declaration with missingness</a></li>
  <li><a href="#pl-rasch-model" id="toc-pl-rasch-model" class="nav-link" data-scroll-target="#pl-rasch-model">1PL (Rasch) model</a></li>
  <li><a href="#multilevel-2pl-model" id="toc-multilevel-2pl-model" class="nav-link" data-scroll-target="#multilevel-2pl-model">Multilevel 2PL model</a></li>
  </ul></li>
  <li><a href="#priors-for-identification.section" id="toc-priors-for-identification.section" class="nav-link" data-scroll-target="#priors-for-identification.section">Priors for identifiability</a>
  <ul class="collapse">
  <li><a href="#location-and-scale-invariance" id="toc-location-and-scale-invariance" class="nav-link" data-scroll-target="#location-and-scale-invariance">Location and scale invariance</a></li>
  <li><a href="#collinearity" id="toc-collinearity" class="nav-link" data-scroll-target="#collinearity">Collinearity</a></li>
  <li><a href="#separability" id="toc-separability" class="nav-link" data-scroll-target="#separability">Separability</a></li>
  </ul></li>
  <li><a href="#multivariate-hierarchical-priors.section" id="toc-multivariate-hierarchical-priors.section" class="nav-link" data-scroll-target="#multivariate-hierarchical-priors.section">Multivariate priors for hierarchical models</a>
  <ul class="collapse">
  <li><a href="#multivariate-regression-example" id="toc-multivariate-regression-example" class="nav-link" data-scroll-target="#multivariate-regression-example">Multivariate regression example</a></li>
  </ul></li>
  <li><a href="#prediction-forecasting-and-backcasting" id="toc-prediction-forecasting-and-backcasting" class="nav-link" data-scroll-target="#prediction-forecasting-and-backcasting">Prediction, forecasting, and backcasting</a>
  <ul class="collapse">
  <li><a href="#programming-predictions" id="toc-programming-predictions" class="nav-link" data-scroll-target="#programming-predictions">Programming predictions</a></li>
  <li><a href="#predictions-as-generated-quantities" id="toc-predictions-as-generated-quantities" class="nav-link" data-scroll-target="#predictions-as-generated-quantities">Predictions as generated quantities</a></li>
  </ul></li>
  <li><a href="#multivariate-outcomes" id="toc-multivariate-outcomes" class="nav-link" data-scroll-target="#multivariate-outcomes">Multivariate outcomes</a>
  <ul class="collapse">
  <li><a href="#seemingly-unrelated-regressions" id="toc-seemingly-unrelated-regressions" class="nav-link" data-scroll-target="#seemingly-unrelated-regressions">Seemingly unrelated regressions</a></li>
  <li><a href="#multivariate-probit-regression" id="toc-multivariate-probit-regression" class="nav-link" data-scroll-target="#multivariate-probit-regression">Multivariate probit regression</a></li>
  </ul></li>
  <li><a href="#applications-of-pseudorandom-number-generation" id="toc-applications-of-pseudorandom-number-generation" class="nav-link" data-scroll-target="#applications-of-pseudorandom-number-generation">Applications of pseudorandom number generation</a>
  <ul class="collapse">
  <li><a href="#prediction" id="toc-prediction" class="nav-link" data-scroll-target="#prediction">Prediction</a></li>
  <li><a href="#posterior-predictive-checks" id="toc-posterior-predictive-checks" class="nav-link" data-scroll-target="#posterior-predictive-checks">Posterior predictive checks</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/stan-dev/docs/edit/master/src/stan-users-guide/regression.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/stan-dev/docs/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">




<section id="regression-models" class="level1">
<h1>Regression Models</h1>
<p>Stan supports regression models from simple linear regressions to multilevel generalized linear models.</p>
<section id="linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression">Linear regression</h2>
<p>The simplest linear regression model is the following, with a single predictor and a slope and intercept coefficient, and normally distributed noise. This model can be written using standard regression notation as <span class="math display">\[
y_n = \alpha + \beta x_n + \epsilon_n
\quad\text{where}\quad
\epsilon_n \sim \operatorname{normal}(0,\sigma).
\]</span></p>
<p>This is equivalent to the following sampling involving the residual, <span class="math display">\[
y_n - (\alpha + \beta X_n) \sim \operatorname{normal}(0,\sigma),
\]</span> and reducing still further, to <span class="math display">\[
y_n \sim \operatorname{normal}(\alpha + \beta X_n, \, \sigma).
\]</span></p>
<p>This latter form of the model is coded in Stan as follows.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] x;</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] y;</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> alpha;</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> beta;</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  y ~ normal(alpha + beta * x, sigma);</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>There are <code>N</code> observations and for each observation, <span class="math inline">\(n \in N\)</span>, we have predictor <code>x[n]</code> and outcome <code>y[n]</code>. The intercept and slope parameters are <code>alpha</code> and <code>beta</code>. The model assumes a normally distributed noise term with scale <code>sigma</code>. This model has improper priors for the two regression coefficients.</p>
<section id="vectorization.section" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="vectorization.section">Matrix notation and vectorization</h3>
<p>The distribution statement in the previous model is vectorized, with</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>y ~ normal(alpha + beta * x, sigma);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>providing the same model as the unvectorized version,</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  y[n] ~ normal(alpha + beta * x[n], sigma);</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In addition to being more concise, the vectorized form is much faster.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>In general, Stan allows the arguments to distributions such as <code>normal</code> to be vectors. If any of the other arguments are vectors or arrays, they have to be the same size. If any of the other arguments is a scalar, it is reused for each vector entry.</p>
<p>The other reason this works is that Stan’s arithmetic operators are overloaded to perform matrix arithmetic on matrices. In this case, because <code>x</code> is of type <code>vector</code> and <code>beta</code> of type <code>real</code>, the expression <code>beta * x</code> is of type <code>vector</code>. Because Stan supports vectorization, a regression model with more than one predictor can be written directly using matrix notation.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;   <span class="co">// number of data items</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; K;   <span class="co">// number of predictors</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N, K] x;   <span class="co">// predictor matrix</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] y;      <span class="co">// outcome vector</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> alpha;           <span class="co">// intercept</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[K] beta;       <span class="co">// coefficients for predictors</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;  <span class="co">// error scale</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>  y ~ normal(x * beta + alpha, sigma);  <span class="co">// data model</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The constraint <code>lower=0</code> in the declaration of <code>sigma</code> constrains the value to be greater than or equal to 0. With no prior in the model block, the effect is an improper prior on non-negative real numbers. Although a more informative prior may be added, improper priors are acceptable as long as they lead to proper posteriors.</p>
<p>In the model above, <code>x</code> is an <span class="math inline">\(N \times K\)</span> matrix of predictors and <code>beta</code> a <span class="math inline">\(K\)</span>-vector of coefficients, so <code>x * beta</code> is an <span class="math inline">\(N\)</span>-vector of predictions, one for each of the <span class="math inline">\(N\)</span> data items. These predictions line up with the outcomes in the <span class="math inline">\(N\)</span>-vector <code>y</code>, so the entire model may be written using matrix arithmetic as shown. It would be possible to include a column of ones in the data matrix <code>x</code> to remove the <code>alpha</code> parameter.</p>
<p>The distribution statement in the model above is just a more efficient, vector-based approach to coding the model with a loop, as in the following statistically equivalent model.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    y[n] ~ normal(x[n] * beta, sigma);</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>With Stan’s matrix indexing scheme, <code>x[n]</code> picks out row <code>n</code> of the matrix <code>x</code>; because <code>beta</code> is a column vector, the product <code>x[n] * beta</code> is a scalar of type <code>real</code>.</p>
<section id="intercepts-as-inputs" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="intercepts-as-inputs">Intercepts as inputs</h4>
<p>In the model formulation</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>y ~ normal(x * beta, sigma);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>there is no longer an intercept coefficient <code>alpha</code>. Instead, we have assumed that the first column of the input matrix <code>x</code> is a column of 1 values. This way, <code>beta[1]</code> plays the role of the intercept. If the intercept gets a different prior than the slope terms, then it would be clearer to break it out. It is also slightly more efficient in its explicit form with the intercept variable singled out because there’s one fewer multiplications; it should not make that much of a difference to speed, though, so the choice should be based on clarity.</p>
</section>
</section>
</section>
<section id="QR-reparameterization.section" class="level2">
<h2 class="anchored" data-anchor-id="QR-reparameterization.section">The QR reparameterization</h2>
<p>In the previous example, the linear predictor can be written as <span class="math inline">\(\eta
= x \beta\)</span>, where <span class="math inline">\(\eta\)</span> is a <span class="math inline">\(N\)</span>-vector of predictions, <span class="math inline">\(x\)</span> is a <span class="math inline">\(N
\times K\)</span> matrix, and <span class="math inline">\(\beta\)</span> is a <span class="math inline">\(K\)</span>-vector of coefficients. Presuming <span class="math inline">\(N \geq K\)</span>, we can exploit the fact that any design matrix <span class="math inline">\(x\)</span> can be decomposed using the thin QR decomposition into an orthogonal matrix <span class="math inline">\(Q\)</span> and an upper-triangular matrix <span class="math inline">\(R\)</span>, i.e.&nbsp;<span class="math inline">\(x = Q
R\)</span>.</p>
<p>The functions <code>qr_thin_Q</code> and <code>qr_thin_R</code> implement the thin QR decomposition, which is to be preferred to the fat QR decomposition that would be obtained by using <code>qr_Q</code> and <code>qr_R</code>, as the latter would more easily run out of memory (see the Stan Functions Reference for more information on the <code>qr_thin_Q</code> and <code>qr_thin_R</code> functions). In practice, it is best to write <span class="math inline">\(x = Q^\ast
R^\ast\)</span> where <span class="math inline">\(Q^\ast = Q * \sqrt{n - 1}\)</span> and <span class="math inline">\(R^\ast =
\frac{1}{\sqrt{n - 1}} R\)</span>. Thus, we can equivalently write <span class="math inline">\(\eta = x
\beta = Q R \beta = Q^\ast R^\ast \beta\)</span>. If we let <span class="math inline">\(\theta = R^\ast
\beta\)</span>, then we have <span class="math inline">\(\eta = Q^\ast \theta\)</span> and <span class="math inline">\(\beta = R^{\ast^{-1}}
\theta\)</span>. In that case, the previous Stan program becomes</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;   <span class="co">// number of data items</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; K;   <span class="co">// number of predictors</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N, K] x;   <span class="co">// predictor matrix</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] y;      <span class="co">// outcome vector</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N, K] Q_ast;</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[K, K] R_ast;</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[K, K] R_ast_inverse;</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>  <span class="co">// thin and scale the QR decomposition</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>  Q_ast = qr_thin_Q(x) * sqrt(N - <span class="dv">1</span>);</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>  R_ast = qr_thin_R(x) / sqrt(N - <span class="dv">1</span>);</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>  R_ast_inverse = inverse(R_ast);</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> alpha;           <span class="co">// intercept</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[K] theta;      <span class="co">// coefficients on Q_ast</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;  <span class="co">// error scale</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>  y ~ normal(Q_ast * theta + alpha, sigma);  <span class="co">// data model</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[K] beta;</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>  beta = R_ast_inverse * theta; <span class="co">// coefficients on x</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Since this Stan program generates equivalent predictions for <span class="math inline">\(y\)</span> and the same posterior distribution for <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma\)</span> as the previous Stan program, many wonder why the version with this QR reparameterization performs so much better in practice, often both in terms of wall time and in terms of effective sample size. The reasoning is threefold:</p>
<ol type="1">
<li><p>The columns of <span class="math inline">\(Q^\ast\)</span> are orthogonal whereas the columns of <span class="math inline">\(x\)</span> generally are not. Thus, it is easier for a Markov Chain to move around in <span class="math inline">\(\theta\)</span>-space than in <span class="math inline">\(\beta\)</span>-space.</p></li>
<li><p>The columns of <span class="math inline">\(Q^\ast\)</span> have the same scale whereas the columns of <span class="math inline">\(x\)</span> generally do not. Thus, a Hamiltonian Monte Carlo algorithm can move around the parameter space with a smaller number of larger steps</p></li>
<li><p>Since the covariance matrix for the columns of <span class="math inline">\(Q^\ast\)</span> is an identity matrix, <span class="math inline">\(\theta\)</span> typically has a reasonable scale if the units of <span class="math inline">\(y\)</span> are also reasonable. This also helps HMC move efficiently without compromising numerical accuracy.</p></li>
</ol>
<p>Consequently, this QR reparameterization is recommended for linear and generalized linear models in Stan whenever <span class="math inline">\(K &gt; 1\)</span> and you do not have an informative prior on the location of <span class="math inline">\(\beta\)</span>. It can also be worthwhile to subtract the mean from each column of <span class="math inline">\(x\)</span> before obtaining the QR decomposition, which does not affect the posterior distribution of <span class="math inline">\(\theta\)</span> or <span class="math inline">\(\beta\)</span> but does affect <span class="math inline">\(\alpha\)</span> and allows you to interpret <span class="math inline">\(\alpha\)</span> as the expectation of <span class="math inline">\(y\)</span> in a linear model.</p>
</section>
<section id="regression-priors.section" class="level2">
<h2 class="anchored" data-anchor-id="regression-priors.section">Priors for coefficients and scales</h2>
<p>See our <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">general discussion of priors</a> for tips on priors for parameters in regression models.</p>
<p>Later sections discuss <a href="#hierarchical-priors.section">univariate hierarchical priors</a> and <a href="#multivariate-hierarchical-priors.section">multivariate hierarchical priors</a>, as well as <a href="#priors-for-identification.section">priors used to identify models</a>.</p>
<p>However, as described in <a href="#QR-reparameterization.section">QR-reparameterization section</a>, if you do not have an informative prior on the <em>location</em> of the regression coefficients, then you are better off reparameterizing your model so that the regression coefficients are a generated quantity. In that case, it usually does not matter much what prior is used on on the reparameterized regression coefficients and almost any weakly informative prior that scales with the outcome will do.</p>
</section>
<section id="robust-noise-models" class="level2">
<h2 class="anchored" data-anchor-id="robust-noise-models">Robust noise models</h2>
<p>The standard approach to linear regression is to model the noise term <span class="math inline">\(\epsilon\)</span> as having a normal distribution. From Stan’s perspective, there is nothing special about normally distributed noise. For instance, robust regression can be accommodated by giving the noise term a Student-<span class="math inline">\(t\)</span> distribution. To code this in Stan, the distribution distribution is changed to the following.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; nu;</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">// ...</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>  y ~ student_t(nu, alpha + beta * x, sigma);</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The degrees of freedom constant <code>nu</code> is specified as data.</p>
</section>
<section id="logistic-probit-regression.section" class="level2">
<h2 class="anchored" data-anchor-id="logistic-probit-regression.section">Logistic and probit regression</h2>
<p>For binary outcomes, either of the closely related logistic or probit regression models may be used. These generalized linear models vary only in the link function they use to map linear predictions in <span class="math inline">\((-\infty,\infty)\)</span> to probability values in <span class="math inline">\((0,1)\)</span>. Their respective link functions, the logistic function and the standard normal cumulative distribution function, are both sigmoid functions (i.e., they are both <em>S</em>-shaped).</p>
<p>A logistic regression model with one predictor and an intercept is coded as follows.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] x;</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>, <span class="kw">upper</span>=<span class="dv">1</span>&gt; y;</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> alpha;</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> beta;</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>  y ~ bernoulli_logit(alpha + beta * x);</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The noise parameter is built into the Bernoulli formulation here rather than specified directly.</p>
<p>Logistic regression is a kind of generalized linear model with binary outcomes and the log odds (logit) link function, defined by <span class="math display">\[
\operatorname{logit}(v) = \log \left( \frac{v}{1-v} \right).
\]</span></p>
<p>The inverse of the link function appears in the model: <span class="math display">\[
\operatorname{logit}^{-1}(u) = \texttt{inv}\mathtt{\_}\texttt{logit}(u) = \frac{1}{1 + \exp(-u)}.
\]</span></p>
<p>The model formulation above uses the logit-parameterized version of the Bernoulli distribution, which is defined by <span class="math display">\[
\texttt{bernoulli}\mathtt{\_}\texttt{logit}\left(y \mid \alpha \right)
=
\texttt{bernoulli}\left(y \mid \operatorname{logit}^{-1}(\alpha)\right).
\]</span></p>
<p>The formulation is also vectorized in the sense that <code>alpha</code> and <code>beta</code> are scalars and <code>x</code> is a vector, so that <code>alpha   + beta * x</code> is a vector. The vectorized formulation is equivalent to the less efficient version</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  y[n] ~ bernoulli_logit(alpha + beta * x[n]);</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Expanding out the Bernoulli logit, the model is equivalent to the more explicit, but less efficient and less arithmetically stable</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  y[n] ~ bernoulli(inv_logit(alpha + beta * x[n]));</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Other link functions may be used in the same way. For example, probit regression uses the cumulative normal distribution function, which is typically written as</p>
<p><span class="math display">\[
\Phi(x) = \int_{-\infty}^x \textsf{normal}\left(y \mid 0,1 \right) \,\textrm{d}y.
\]</span></p>
<p>The cumulative standard normal distribution function <span class="math inline">\(\Phi\)</span> is implemented in Stan as the function <code>Phi</code>. The probit regression model may be coded in Stan by replacing the logistic model’s distribution statement with the following.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>y[n] ~ bernoulli(Phi(alpha + beta * x[n]));</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>A fast approximation to the cumulative standard normal distribution function <span class="math inline">\(\Phi\)</span> is implemented in Stan as the function <code>Phi_approx</code>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> The approximate probit regression model may be coded with the following.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>y[n] ~ bernoulli(Phi_approx(alpha + beta * x[n]));</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="multi-logit.section" class="level2">
<h2 class="anchored" data-anchor-id="multi-logit.section">Multi-logit regression</h2>
<p>Multiple outcome forms of logistic regression can be coded directly in Stan. For instance, suppose there are <span class="math inline">\(K\)</span> possible outcomes for each output variable <span class="math inline">\(y_n\)</span>. Also suppose that there is a <span class="math inline">\(D\)</span>-dimensional vector <span class="math inline">\(x_n\)</span> of predictors for <span class="math inline">\(y_n\)</span>. The multi-logit model with <span class="math inline">\(\textsf{normal}(0,5)\)</span> priors on the coefficients is coded as follows.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> K;</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> N;</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> D;</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">int</span> y;</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N, D] x;</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[D, K] beta;</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N, K] x_beta = x * beta;</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>  to_vector(beta) ~ normal(<span class="dv">0</span>, <span class="dv">5</span>);</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    y[n] ~ categorical_logit(x_beta[n]');</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>where <code>x_beta[n]'</code> is the transpose of <code>x_beta[n]</code>. The prior on <code>beta</code> is coded in vectorized form. As of Stan 2.18, the categorical-logit distribution is not vectorized for parameter arguments, so the loop is required. The matrix multiplication is pulled out to define a local variable for all of the predictors for efficiency. Like the Bernoulli-logit, the categorical-logit distribution applies softmax internally to convert an arbitrary vector to a simplex, <span class="math display">\[
\texttt{categorical}\mathtt{\_}\texttt{logit}\left(y \mid \alpha\right)
=
\texttt{categorical}\left(y \mid \texttt{softmax}(\alpha)\right),
\]</span> where <span class="math display">\[
\texttt{softmax}(u) = \exp(u) / \operatorname{sum}\left(\exp(u)\right).
\]</span></p>
<p>The categorical distribution with log-odds (logit) scaled parameters used above is equivalent to writing</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>y[n] ~ categorical(softmax(x[n] * beta));</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="constraints-on-data-declarations" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="constraints-on-data-declarations">Constraints on data declarations</h4>
<p>The data block in the above model is defined without constraints on sizes <code>K</code>, <code>N</code>, and <code>D</code> or on the outcome array <code>y</code>. Constraints on data declarations provide error checking at the point data are read (or transformed data are defined), which is before sampling begins. Constraints on data declarations also make the model author’s intentions more explicit, which can help with readability. The above model’s declarations could be tightened to</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">2</span>&gt; K;</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; D;</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="dt">array</span>[N] <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>, <span class="kw">upper</span>=K&gt; y;</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>These constraints arise because the number of categories, <code>K</code>, must be at least two in order for a categorical model to be useful. The number of data items, <code>N</code>, can be zero, but not negative; unlike R, Stan’s for-loops always move forward, so that a loop extent of <code>1:N</code> when <code>N</code> is equal to zero ensures the loop’s body will not be executed. The number of predictors, <code>D</code>, must be at least one in order for <code>beta * x[n]</code> to produce an appropriate argument for <code>softmax()</code>. The categorical outcomes <code>y[n]</code> must be between <code>1</code> and <code>K</code> in order for the discrete sampling to be well defined.</p>
<p>Constraints on data declarations are optional. Constraints on parameters declared in the <code>parameters</code> block, on the other hand, are <em>not</em> optional—they are required to ensure support for all parameter values satisfying their constraints. Constraints on transformed data, transformed parameters, and generated quantities are also optional.</p>
</section>
<section id="identifiability" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="identifiability">Identifiability</h3>
<p>Because softmax is invariant under adding a constant to each component of its input, the model is typically only identified if there is a suitable prior on the coefficients.</p>
<p>An alternative is to use <span class="math inline">\((K-1)\)</span>-vectors by fixing one of them to be zero. The <a href="../stan-users-guide/missing-data.html#partially-known-parameters.section">partially known parameters section</a> discusses how to mix constants and parameters in a vector. In the multi-logit case, the parameter block would be redefined to use <span class="math inline">\((K - 1)\)</span>-vectors</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[D, K - <span class="dv">1</span>] beta_raw;</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>and then these are transformed to parameters to use in the model. First, a transformed data block is added before the parameters block to define a vector of zero values,</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[D] zeros = rep_vector(<span class="dv">0</span>, D);</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>which can then be appended to <code>beta_raw</code> to produce the coefficient matrix <code>beta</code>,</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed parameters</span> {</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[D, K] beta = append_col(beta_raw, zeros);</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The <code>rep_vector(0, D)</code> call creates a column vector of size <code>D</code> with all entries set to zero. The derived matrix <code>beta</code> is then defined to be the result of appending the vector <code>zeros</code> as a new column at the end of <code>beta_raw</code>; the vector <code>zeros</code> is defined as transformed data so that it doesn’t need to be constructed from scratch each time it is used.</p>
<p>This is not the same model as using <span class="math inline">\(K\)</span>-vectors as parameters, because now the prior only applies to <span class="math inline">\((K-1)\)</span>-vectors. In practice, this will cause the maximum likelihood solutions to be different and also the posteriors to be slightly different when taking priors centered around zero, as is typical for regression coefficients.</p>
</section>
</section>
<section id="parameterizing-centered-vectors" class="level2">
<h2 class="anchored" data-anchor-id="parameterizing-centered-vectors">Parameterizing centered vectors</h2>
<p>It is often convenient to define a parameter vector <span class="math inline">\(\beta\)</span> that is centered in the sense of satisfying the sum-to-zero constraint, <span class="math display">\[
\sum_{k=1}^K \beta_k = 0.
\]</span></p>
<p>Such a parameter vector may be used to identify a multi-logit regression parameter vector (see the <a href="#multi-logit.section">multi-logit section</a> for details), or may be used for ability or difficulty parameters (but not both) in an IRT model (see the <a href="#item-response-models.section">item-response model section</a> for details).</p>
<section id="k-1-degrees-of-freedom" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="k-1-degrees-of-freedom"><span class="math inline">\(K-1\)</span> degrees of freedom</h3>
<p>There is more than one way to enforce a sum-to-zero constraint on a parameter vector, the most efficient of which is to define the <span class="math inline">\(K\)</span>-th element as the negation of the sum of the elements <span class="math inline">\(1\)</span> through <span class="math inline">\(K-1\)</span>.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[K - <span class="dv">1</span>] beta_raw;</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed parameters</span> {</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[K] beta = append_row(beta_raw, -sum(beta_raw));</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Placing a prior on <code>beta_raw</code> in this parameterization leads to a subtly different posterior than that resulting from the same prior on <code>beta</code> in the original parameterization without the sum-to-zero constraint. Most notably, a simple prior on each component of <code>beta_raw</code> produces different results than putting the same prior on each component of an unconstrained <span class="math inline">\(K\)</span>-vector <code>beta</code>. For example, providing a <span class="math inline">\(\textsf{normal}(0,5)\)</span> prior on <code>beta</code> will produce a different posterior mode than placing the same prior on <code>beta_raw</code>.</p>
<section id="marginal-distribution-of-sum-to-zero-components" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="marginal-distribution-of-sum-to-zero-components">Marginal distribution of sum-to-zero components</h4>
<p>On the Stan forums, Aaron Goodman provided the following code to produce a prior with standard normal marginals on the components of <code>beta</code>,</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>  beta ~ normal(<span class="dv">0</span>, inv(sqrt(<span class="dv">1</span> - inv(K))));</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The components are not independent, as they must sum zero. No Jacobian is required because summation and negation are linear operations (and thus have constant Jacobians).</p>
<p>To generate distributions with marginals other than standard normal, the resulting <code>beta</code> may be scaled by some factor <code>sigma</code> and translated to some new location <code>mu</code>.</p>
</section>
</section>
<section id="qr-decomposition" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="qr-decomposition">QR decomposition</h3>
<p>Aaron Goodman, on the Stan forums, also provided this approach, which calculates a QR decomposition in the transformed data block, then uses it to transform to a sum-to-zero parameter <code>x</code>,</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed data</span>{</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[K, K] A = diag_matrix(rep_vector(<span class="dv">1</span>, K));</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[K, K - <span class="dv">1</span>] A_qr;</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span>:K - <span class="dv">1</span>) A[K, i] = -<span class="dv">1</span>;</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>  A[K, K] = <span class="dv">0</span>;</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>  A_qr = qr_Q(A)[ , <span class="dv">1</span>:(K - <span class="dv">1</span>)];</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[K - <span class="dv">1</span>] beta_raw;</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed parameters</span>{</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>   <span class="dt">vector</span>[K] beta =  A_qr * beta_raw;</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>  beta_raw ~ normal(<span class="dv">0</span>, inv(sqrt(<span class="dv">1</span> - inv(K))));</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This produces a marginal standard normal distribution on the values of <code>beta</code>, which will sum to zero by construction of the QR decomposition.</p>
</section>
<section id="translated-and-scaled-simplex" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="translated-and-scaled-simplex">Translated and scaled simplex</h3>
<p>An alternative approach that’s less efficient, but amenable to a symmetric prior, is to offset and scale a simplex.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">simplex</span>[K] beta_raw;</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> beta_scale;</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed parameters</span> {</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[K] beta;</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>  beta = beta_scale * (beta_raw - inv(K));</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Here <code>inv(K)</code> is just a short way to write <code>1.0 / K</code>. Given that <code>beta_raw</code> sums to 1 because it is a simplex, the elementwise subtraction of <code>inv(K)</code> is guaranteed to sum to zero. Because the magnitude of the elements of the simplex is bounded, a scaling factor is required to provide <code>beta</code> with <span class="math inline">\(K\)</span> degrees of freedom necessary to take on every possible value that sums to zero.</p>
<p>With this parameterization, a Dirichlet prior can be placed on <code>beta_raw</code>, perhaps uniform, and another prior put on <code>beta_scale</code>, typically for “shrinkage.”</p>
</section>
<section id="soft-centering" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="soft-centering">Soft centering</h3>
<p>Adding a prior such as <span class="math inline">\(\beta \sim \textsf{normal}(0,\sigma)\)</span> will provide a kind of soft centering of a parameter vector <span class="math inline">\(\beta\)</span> by preferring, all else being equal, that <span class="math inline">\(\sum_{k=1}^K \beta_k = 0\)</span>. This approach is only guaranteed to roughly center if <span class="math inline">\(\beta\)</span> and the elementwise addition <span class="math inline">\(\beta + c\)</span> for a scalar constant <span class="math inline">\(c\)</span> produce the same likelihood (perhaps by another vector <span class="math inline">\(\alpha\)</span> being transformed to <span class="math inline">\(\alpha - c\)</span>, as in the IRT models). This is another way of achieving a symmetric prior.</p>
</section>
</section>
<section id="ordered-logistic.section" class="level2">
<h2 class="anchored" data-anchor-id="ordered-logistic.section">Ordered logistic and probit regression</h2>
<p>Ordered regression for an outcome <span class="math inline">\(y_n \in \{ 1, \dotsc, k \}\)</span> with predictors <span class="math inline">\(x_n \in \mathbb{R}^D\)</span> is determined by a single coefficient vector <span class="math inline">\(\beta \in \mathbb{R}^D\)</span> along with a sequence of cutpoints <span class="math inline">\(c \in
\mathbb{R}^{K-1}\)</span> sorted so that <span class="math inline">\(c_d &lt; c_{d+1}\)</span>. The discrete output is <span class="math inline">\(k\)</span> if the linear predictor <span class="math inline">\(x_n \beta\)</span> falls between <span class="math inline">\(c_{k-1}\)</span> and <span class="math inline">\(c_k\)</span>, assuming <span class="math inline">\(c_0 = -\infty\)</span> and <span class="math inline">\(c_K = \infty\)</span>. The noise term is fixed by the form of regression, with examples for ordered logistic and ordered probit models.</p>
<section id="ordered-logistic-regression" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="ordered-logistic-regression">Ordered logistic regression</h3>
<p>The ordered logistic model can be coded in Stan using the <code>ordered</code> data type for the cutpoints and the built-in <code>ordered_logistic</code> distribution.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">2</span>&gt; K;</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; D;</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>, <span class="kw">upper</span>=K&gt; y;</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">row_vector</span>[D] x;</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[D] beta;</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">ordered</span>[K - <span class="dv">1</span>] c;</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    y[n] ~ ordered_logistic(x[n] * beta, c);</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The vector of cutpoints <code>c</code> is declared as <code>ordered[K - 1]</code>, which guarantees that <code>c[k]</code> is less than <code>c[k + 1]</code>.</p>
<p>If the cutpoints were assigned independent priors, the constraint effectively truncates the joint prior to support over points that satisfy the ordering constraint. Luckily, Stan does not need to compute the effect of the constraint on the normalizing term because the probability is needed only up to a proportion.</p>
<section id="ordered-probit" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="ordered-probit">Ordered probit</h4>
<p>An ordered probit model could be coded in exactly the same way by swapping the cumulative logistic (<code>inv_logit</code>) for the cumulative normal (<code>Phi</code>).</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">2</span>&gt; K;</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; D;</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>, <span class="kw">upper</span>=K&gt; y;</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">row_vector</span>[D] x;</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[D] beta;</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">ordered</span>[K - <span class="dv">1</span>] c;</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[K] theta;</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    <span class="dt">real</span> eta;</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    eta = x[n] * beta;</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    theta[<span class="dv">1</span>] = <span class="dv">1</span> - Phi(eta - c[<span class="dv">1</span>]);</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">2</span>:(K - <span class="dv">1</span>)) {</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>      theta[k] = Phi(eta - c[k - <span class="dv">1</span>]) - Phi(eta - c[k]);</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>    theta[K] = Phi(eta - c[K - <span class="dv">1</span>]);</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>    y[n] ~ categorical(theta);</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The logistic model could also be coded this way by replacing <code>Phi</code> with <code>inv_logit</code>, though the built-in encoding based on the softmax transform is more efficient and more numerically stable. A small efficiency gain could be achieved by computing the values <code>Phi(eta - c[k])</code> once and storing them for re-use.</p>
</section>
</section>
</section>
<section id="hierarchical-regression" class="level2">
<h2 class="anchored" data-anchor-id="hierarchical-regression">Hierarchical regression</h2>
<p>The simplest multilevel model is a hierarchical model in which the data are grouped into <span class="math inline">\(L\)</span> distinct categories (or levels). An extreme approach would be to completely pool all the data and estimate a common vector of regression coefficients <span class="math inline">\(\beta\)</span>. At the other extreme, an approach with no pooling assigns each level <span class="math inline">\(l\)</span> its own coefficient vector <span class="math inline">\(\beta_l\)</span> that is estimated separately from the other levels. A hierarchical model is an intermediate solution where the degree of pooling is determined by the data and a prior on the amount of pooling.</p>
<p>Suppose each binary outcome <span class="math inline">\(y_n \in \{ 0, 1 \}\)</span> has an associated level, <span class="math inline">\(ll_n \in \{ 1, \dotsc, L \}\)</span>. Each outcome will also have an associated predictor vector <span class="math inline">\(x_n \in \mathbb{R}^D\)</span>. Each level <span class="math inline">\(l\)</span> gets its own coefficient vector <span class="math inline">\(\beta_l \in \mathbb{R}^D\)</span>. The hierarchical structure involves drawing the coefficients <span class="math inline">\(\beta_{l,d}
\in \mathbb{R}\)</span> from a prior that is also estimated with the data. This hierarchically estimated prior determines the amount of pooling. If the data in each level are similar, strong pooling will be reflected in low hierarchical variance. If the data in the levels are dissimilar, weaker pooling will be reflected in higher hierarchical variance.</p>
<p>The following model encodes a hierarchical logistic regression model with a hierarchical prior on the regression coefficients.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; D;</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; L;</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>, <span class="kw">upper</span>=<span class="dv">1</span>&gt; y;</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>, <span class="kw">upper</span>=L&gt; ll;</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">row_vector</span>[D] x;</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[D] <span class="dt">real</span> mu;</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[D] <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[L] <span class="dt">vector</span>[D] beta;</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span>:D) {</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>    mu[d] ~ normal(<span class="dv">0</span>, <span class="dv">100</span>);</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span>:L) {</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>      beta[l, d] ~ normal(mu[d], sigma[d]);</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>    y[n] ~ bernoulli(inv_logit(x[n] * beta[ll[n]]));</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The standard deviation parameter <code>sigma</code> gets an implicit uniform prior on <span class="math inline">\((0,\infty)\)</span> because of its declaration with a lower-bound constraint of zero. Stan allows improper priors as long as the posterior is proper. Nevertheless, it is usually helpful to have informative or at least weakly informative priors for all parameters; see the <a href="#regression-priors.section">regression priors section</a> for recommendations on priors for regression coefficients and scales.</p>
<section id="optimizing-the-model" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="optimizing-the-model">Optimizing the model</h4>
<p>Where possible, vectorizing distribution statements leads to faster log probability and derivative evaluations. The speed boost is not because loops are eliminated, but because vectorization allows sharing subcomputations in the log probability and gradient calculations and because it reduces the size of the expression tree required for gradient calculations.</p>
<p>The first optimization vectorizes the for-loop over <code>D</code> as</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>mu ~ normal(<span class="dv">0</span>, <span class="dv">100</span>);</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span>:L) {</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>  beta[l] ~ normal(mu, sigma);</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The declaration of <code>beta</code> as an array of vectors means that the expression <code>beta[l]</code> denotes a vector. Although <code>beta</code> could have been declared as a matrix, an array of vectors (or a two-dimensional array) is more efficient for accessing rows; see the <a href="../stan-users-guide/matrices-arrays.html#indexing-efficiency.section">indexing efficiency section</a> for more information on the efficiency tradeoffs among arrays, vectors, and matrices.</p>
<p>This model can be further sped up and at the same time made more arithmetically stable by replacing the application of inverse-logit inside the Bernoulli distribution with the logit-parameterized Bernoulli,<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>  y[n] ~ bernoulli_logit(x[n] * beta[ll[n]]);</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Unlike in R or BUGS, loops, array access and assignments are fast in Stan because they are translated directly to C++. In most cases, the cost of allocating and assigning to a container is more than made up for by the increased efficiency due to vectorizing the log probability and gradient calculations. Thus the following version is faster than the original formulation as a loop over a distribution statement.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] x_beta_ll;</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    x_beta_ll[n] = x[n] * beta[ll[n]];</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>  y ~ bernoulli_logit(x_beta_ll);</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The brackets introduce a new scope for the local variable <code>x_beta_ll</code>; alternatively, the variable may be declared at the top of the model block.</p>
<p>In some cases, such as the above, the local variable assignment leads to models that are less readable. The recommended practice in such cases is to first develop and debug the more transparent version of the model and only work on optimizations when the simpler formulation has been debugged.</p>
</section>
</section>
<section id="hierarchical-priors.section" class="level2">
<h2 class="anchored" data-anchor-id="hierarchical-priors.section">Hierarchical priors</h2>
<p>Priors on priors, also known as “hyperpriors,” should be treated the same way as priors on lower-level parameters in that as much prior information as is available should be brought to bear. Because hyperpriors often apply to only a handful of lower-level parameters, care must be taken to ensure the posterior is both proper and not overly sensitive either statistically or computationally to wide tails in the priors.</p>
<section id="boundary-avoiding-priors-for-mle-in-hierarchical-models" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="boundary-avoiding-priors-for-mle-in-hierarchical-models">Boundary-avoiding priors for MLE in hierarchical models</h3>
<p>The fundamental problem with maximum likelihood estimation (MLE) in the hierarchical model setting is that as the hierarchical variance drops and the values cluster around the hierarchical mean, the overall density grows without bound. As an illustration, consider a simple hierarchical linear regression (with fixed prior mean) of <span class="math inline">\(y_n \in
\mathbb{R}\)</span> on <span class="math inline">\(x_n \in \mathbb{R}^K\)</span>, formulated as <span class="math display">\[\begin{align*}
y_n     &amp; \sim \textsf{normal}(x_n \beta, \sigma) \\
\beta_k &amp; \sim \textsf{normal}(0,\tau) \\
\tau    &amp; \sim \textsf{Cauchy}(0,2.5) \\
\end{align*}\]</span></p>
<p>In this case, as <span class="math inline">\(\tau \rightarrow 0\)</span> and <span class="math inline">\(\beta_k \rightarrow 0\)</span>, the posterior density <span class="math display">\[ p(\beta,\tau,\sigma|y,x) \propto p(y|x,\beta,\tau,\sigma) \]</span> grows without bound. See the <a href="#funnel.figure">plot of Neal’s funnel density</a>, which has similar behavior.</p>
<p>There is obviously no MLE estimate for <span class="math inline">\(\beta,\tau,\sigma\)</span> in such a case, and therefore the model must be modified if posterior modes are to be used for inference. The approach recommended by <span class="citation" data-cites="ChungEtAl:2013">Chung et al. (<a href="#ref-ChungEtAl:2013" role="doc-biblioref">2013</a>)</span> is to use a gamma distribution as a prior, such as <span class="math display">\[
\sigma \sim \textsf{Gamma}(2, 1/A),
\]</span> for a reasonably large value of <span class="math inline">\(A\)</span>, such as <span class="math inline">\(A = 10\)</span>.</p>
</section>
</section>
<section id="item-response-models.section" class="level2">
<h2 class="anchored" data-anchor-id="item-response-models.section">Item-response theory models</h2>
<p>Item-response theory (IRT) models the situation in which a number of students each answer one or more of a group of test questions. The model is based on parameters for the ability of the students, the difficulty of the questions, and in more articulated models, the discriminativeness of the questions and the probability of guessing correctly; see <span class="citation" data-cites="GelmanHill:2007">Gelman and Hill (<a href="#ref-GelmanHill:2007" role="doc-biblioref">2007</a>, pps. 314–320)</span> for a textbook introduction to hierarchical IRT models and <span class="citation" data-cites="Curtis:2010">Curtis (<a href="#ref-Curtis:2010" role="doc-biblioref">2010</a>)</span> for encodings of a range of IRT models in BUGS.</p>
<section id="data-declaration-with-missingness" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="data-declaration-with-missingness">Data declaration with missingness</h3>
<p>The data provided for an IRT model may be declared as follows to account for the fact that not every student is required to answer every question.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; J;                     <span class="co">// number of students</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; K;                     <span class="co">// number of questions</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N;                     <span class="co">// number of observations</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>, <span class="kw">upper</span>=J&gt; jj;  <span class="co">// student for observation n</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>, <span class="kw">upper</span>=K&gt; kk;  <span class="co">// question for observation n</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>, <span class="kw">upper</span>=<span class="dv">1</span>&gt; y;   <span class="co">// correctness for observation n</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This declares a total of <code>N</code> student-question pairs in the data set, where each <code>n</code> in <code>1:N</code> indexes a binary observation <code>y[n]</code> of the correctness of the answer of student <code>jj[n]</code> on question <code>kk[n]</code>.</p>
<p>The prior hyperparameters will be hard coded in the rest of this section for simplicity, though they could be coded as data in Stan for more flexibility.</p>
</section>
<section id="pl-rasch-model" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="pl-rasch-model">1PL (Rasch) model</h3>
<p>The 1PL item-response model, also known as the Rasch model, has one parameter (1P) for questions and uses the logistic link function (L).</p>
<p>The model parameters are declared as follows.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> delta;            <span class="co">// mean student ability</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[J] <span class="dt">real</span> alpha;   <span class="co">// ability of student j - mean ability</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[K] <span class="dt">real</span> beta;    <span class="co">// difficulty of question k</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The parameter <code>alpha[J]</code> is the ability coefficient for student <code>j</code> and <code>beta[k]</code> is the difficulty coefficient for question <code>k</code>. The non-standard parameterization used here also includes an intercept term <code>delta</code>, which represents the average student’s response to the average question.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>The model itself is as follows.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>  alpha ~ std_normal();         <span class="co">// informative true prior</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>  beta ~ std_normal();          <span class="co">// informative true prior</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>  delta ~ normal(<span class="fl">0.75</span>, <span class="dv">1</span>);      <span class="co">// informative true prior</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    y[n] ~ bernoulli_logit(alpha[jj[n]] - beta[kk[n]] + delta);</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This model uses the logit-parameterized Bernoulli distribution, where <span class="math display">\[
\texttt{bernoulli}\mathtt{\_}\texttt{logit}\left(y \mid \alpha\right)
=
\texttt{bernoulli}\left(y \mid \operatorname{logit}^{-1}(\alpha)\right).
\]</span></p>
<p>The key to understanding it is the term inside the <code>bernoulli_logit</code> distribution, from which it follows that <span class="math display">\[
\Pr[y_n = 1] = \operatorname{logit}^{-1}\left(\alpha_{jj[n]} - \beta_{kk[n]}
+ \delta\right).
\]</span></p>
<p>The model suffers from additive identifiability issues without the priors. For example, adding a term <span class="math inline">\(\xi\)</span> to each <span class="math inline">\(\alpha_j\)</span> and <span class="math inline">\(\beta_k\)</span> results in the same predictions. The use of priors for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> located at 0 identifies the parameters; see <span class="citation" data-cites="GelmanHill:2007">Gelman and Hill (<a href="#ref-GelmanHill:2007" role="doc-biblioref">2007</a>)</span> for a discussion of identifiability issues and alternative approaches to identification.</p>
<p>For testing purposes, the IRT 1PL model distributed with Stan uses informative priors that match the actual data generation process used to simulate the data in R (the simulation code is supplied in the same directory as the models). This is unrealistic for most practical applications, but allows Stan’s inferences to be validated. A simple sensitivity analysis with fatter priors shows that the posterior is fairly sensitive to the prior even with 400 students and 100 questions and only 25% missingness at random. For real applications, the priors should be fit hierarchically along with the other parameters, as described in the next section.</p>
</section>
<section id="multilevel-2pl-model" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="multilevel-2pl-model">Multilevel 2PL model</h3>
<p>The simple 1PL model described in the previous section is generalized in this section with the addition of a discrimination parameter to model how noisy a question is and by adding multilevel priors for the question difficulty and discrimination parameters. The model parameters are declared as follows.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> mu_beta;                <span class="co">// mean question difficulty</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[J] alpha;             <span class="co">// ability for j - mean</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[K] beta;              <span class="co">// difficulty for k</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt;[K] gamma;    <span class="co">// discrimination of k</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma_beta;    <span class="co">// scale of difficulties</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma_gamma;   <span class="co">// scale of log discrimination</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The parameters should be clearer after the model definition.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>  alpha ~ std_normal();</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>  beta ~ normal(<span class="dv">0</span>, sigma_beta);</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>  gamma ~ lognormal(<span class="dv">0</span>, sigma_gamma);</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>  mu_beta ~ cauchy(<span class="dv">0</span>, <span class="dv">5</span>);</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>  sigma_beta ~ cauchy(<span class="dv">0</span>, <span class="dv">5</span>);</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>  sigma_gamma ~ cauchy(<span class="dv">0</span>, <span class="dv">5</span>);</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>  y ~ bernoulli_logit(gamma[kk] .* (alpha[jj] - (beta[kk] + mu_beta)));</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The <code>std_normal</code> function is used here, defined by <span class="math display">\[
\texttt{std}\mathtt{\_}\texttt{normal}(y)
=
\textsf{normal}\left(y \mid 0, 1\right).
\]</span></p>
<p>The distribution statement is also vectorized using elementwise multiplication; it is equivalent to</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>  y[n] ~ bernoulli_logit(gamma[kk[n]]</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>                         * (alpha[jj[n]] - (beta[kk[n]] + mu_beta));</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The 2PL model is similar to the 1PL model, with the additional parameter <code>gamma[k]</code> modeling how discriminative question <code>k</code> is. If <code>gamma[k]</code> is greater than 1, responses are more attenuated with less chance of getting a question right at random. The parameter <code>gamma[k]</code> is constrained to be positive, which prohibits there being questions that are easier for students of lesser ability; such questions are not unheard of, but they tend to be eliminated from most testing situations where an IRT model would be applied.</p>
<p>The model is parameterized here with student abilities <code>alpha</code> being given a standard normal prior. This is to identify both the scale and the location of the parameters, both of which would be unidentified otherwise; see the <a href="../stan-users-guide/problematic-posteriors.html">problematic posteriors chapter</a> for further discussion of identifiability. The difficulty and discrimination parameters <code>beta</code> and <code>gamma</code> then have varying scales given hierarchically in this model. They could also be given weakly informative non-hierarchical priors, such as</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>beta ~ normal(<span class="dv">0</span>, <span class="dv">5</span>);</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>gamma ~ lognormal(<span class="dv">0</span>, <span class="dv">2</span>);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The point is that the <code>alpha</code> determines the scale and location and <code>beta</code> and <code>gamma</code> are allowed to float.</p>
<p>The <code>beta</code> parameter is here given a non-centered parameterization, with parameter <code>mu_beta</code> serving as the mean <code>beta</code> location. An alternative would’ve been to take:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>beta ~ normal(mu_beta, sigma_beta);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>and</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>y[n] ~ bernoulli_logit(gamma[kk[n]] * (alpha[jj[n]] - beta[kk[n]]));</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Non-centered parameterizations tend to be more efficient in hierarchical models; see the <a href="../stan-users-guide/efficiency-tuning.html#reparameterization.section">reparameterization section</a> for more information on non-centered reparameterizations.</p>
<p>The intercept term <code>mu_beta</code> can’t itself be modeled hierarchically, so it is given a weakly informative <span class="math inline">\(\textsf{Cauchy}(0,5)\)</span> prior. Similarly, the scale terms, <code>sigma_beta</code>, and <code>sigma_gamma</code>, are given half-Cauchy priors. As mentioned earlier, the scale and location for <code>alpha</code> are fixed to ensure identifiability. The truncation in the half-Cauchy prior is implicit; explicit truncation is not necessary because the log probability need only be calculated up to a proportion and the scale variables are constrained to <span class="math inline">\((0,\infty)\)</span> by their declarations.</p>
</section>
</section>
<section id="priors-for-identification.section" class="level2">
<h2 class="anchored" data-anchor-id="priors-for-identification.section">Priors for identifiability</h2>
<section id="location-and-scale-invariance" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="location-and-scale-invariance">Location and scale invariance</h3>
<p>One application of (hierarchical) priors is to identify the scale and/or location of a group of parameters. For example, in the IRT models discussed in the previous section, there is both a location and scale non-identifiability. With uniform priors, the posteriors will float in terms of both scale and location. See the <a href="../stan-users-guide/problematic-posteriors.html#collinearity.section">collinearity section</a> for a simple example of the problems this poses for estimation.</p>
<p>The non-identifiability is resolved by providing a standard normal (i.e., <span class="math inline">\(\textsf{normal}(0,1)\)</span>) prior on one group of coefficients, such as the student abilities. With a standard normal prior on the student abilities, the IRT model is identified in that the posterior will produce a group of estimates for student ability parameters that have a sample mean of close to zero and a sample variance of close to one. The difficulty and discrimination parameters for the questions should then be given a diffuse, or ideally a hierarchical prior, which will identify these parameters by scaling and locating relative to the student ability parameters.</p>
</section>
<section id="collinearity" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="collinearity">Collinearity</h3>
<p>Another case in which priors can help provide identifiability is in the case of collinearity in a linear regression. In linear regression, if two predictors are collinear (i.e, one is a linear function of the other), then their coefficients will have a correlation of 1 (or -1) in the posterior. This leads to non-identifiability. By placing normal priors on the coefficients, the maximum likelihood solution of two duplicated predictors (trivially collinear) will be half the value than would be obtained by only including one.</p>
</section>
<section id="separability" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="separability">Separability</h3>
<p>In a logistic regression, if a predictor is positive in cases of 1 outcomes and negative in cases of 0 outcomes, then the maximum likelihood estimate for the coefficient for that predictor diverges to infinity. This divergence can be controlled by providing a prior for the coefficient, which will “shrink” the estimate back toward zero and thus identify the model in the posterior.</p>
<p>Similar problems arise for sampling with improper flat priors. The sampler will try to draw large values. By providing a prior, the posterior will be concentrated around finite values, leading to well-behaved sampling.</p>
</section>
</section>
<section id="multivariate-hierarchical-priors.section" class="level2">
<h2 class="anchored" data-anchor-id="multivariate-hierarchical-priors.section">Multivariate priors for hierarchical models</h2>
<p>In hierarchical regression models (and other situations), several individual-level variables may be assigned hierarchical priors. For example, a model with multiple varying intercepts and slopes within might assign them a multivariate prior.</p>
<p>As an example, the individuals might be people and the outcome income, with predictors such as education level and age, and the groups might be states or other geographic divisions. The effect of education level and age as well as an intercept might be allowed to vary by state. Furthermore, there might be state-level predictors, such as average state income and unemployment level.</p>
<section id="multivariate-regression-example" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="multivariate-regression-example">Multivariate regression example</h3>
<p><span class="citation" data-cites="GelmanHill:2007">Gelman and Hill (<a href="#ref-GelmanHill:2007" role="doc-biblioref">2007, chap. 13</a>, Chapter 17)</span> provide a discussion of a hierarchical model with <span class="math inline">\(N\)</span> individuals organized into <span class="math inline">\(J\)</span> groups. Each individual has a predictor row vector <span class="math inline">\(x_n\)</span> of size <span class="math inline">\(K\)</span>; to unify the notation, they assume that <span class="math inline">\(x_{n,1} = 1\)</span> is a fixed “intercept” predictor. To encode group membership, they assume individual <span class="math inline">\(n\)</span> belongs to group <span class="math inline">\(jj[n] \in \{ 1, \dotsc, J \}\)</span>. Each individual <span class="math inline">\(n\)</span> also has an observed outcome <span class="math inline">\(y_n\)</span> taking on real values.</p>
<section id="data-model" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="data-model">Data model</h4>
<p>The model is a linear regression with slope and intercept coefficients varying by group, so that <span class="math inline">\(\beta_j\)</span> is the coefficient <span class="math inline">\(K\)</span>-vector for group <span class="math inline">\(j\)</span>. The data model for individual <span class="math inline">\(n\)</span> is then just <span class="math display">\[
y_n \sim \textsf{normal}(x_n \, \beta_{jj[n]}, \, \sigma)
\quad\text{for}\quad n \in \{ 1, \dotsc, N \}.
\]</span></p>
</section>
<section id="coefficient-prior" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="coefficient-prior">Coefficient prior</h4>
<p>Gelman and Hill model the coefficient vectors <span class="math inline">\(\beta_j\)</span> as being drawn from a multivariate distribution with mean vector <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span>, <span class="math display">\[
\beta_j \sim \textsf{multivariate normal}(\mu_j, \, \Sigma)
\quad\text{for}\quad j \in \{ 1, \dotsc, J \}.
\]</span></p>
<p>Below, we discuss the full model of Gelman and Hill, which uses group-level predictors to model <span class="math inline">\(\mu\)</span>; for now, we assume <span class="math inline">\(\mu\)</span> is a simple vector parameter.</p>
</section>
<section id="hyperpriors" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="hyperpriors">Hyperpriors</h4>
<p>For hierarchical modeling, the group-level mean vector <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span> must themselves be given priors. The group-level mean vector can be given a reasonable weakly-informative prior for independent coefficients, such as <span class="math display">\[
\mu_j \sim \textsf{normal}(0,5).
\]</span> If more is known about the expected coefficient values <span class="math inline">\(\beta_{j, k}\)</span>, this information can be incorporated into the prior for <span class="math inline">\(\mu_j\)</span>.</p>
<p>For the prior on the covariance matrix, Gelman and Hill suggest using a scaled inverse Wishart. That choice was motivated primarily by convenience as it is conjugate to the multivariate likelihood function and thus simplifies Gibbs sampling</p>
<p>In Stan, there is no restriction to conjugacy for multivariate priors, and we in fact recommend a slightly different approach. Like Gelman and Hill, we decompose our prior into a scale and a matrix, but are able to do so in a more natural way based on the actual variable scales and a correlation matrix. Specifically, we define <span class="math display">\[
\Sigma = \texttt{diag}\mathtt{\_}\texttt{matrix}(\tau) \times \Omega \times \texttt{diag}\mathtt{\_}\texttt{matrix}(\tau),
\]</span> where <span class="math inline">\(\Omega\)</span> is a correlation matrix and <span class="math inline">\(\tau\)</span> is the vector of coefficient scales. This mapping from scale vector <span class="math inline">\(\tau\)</span> and correlation matrix <span class="math inline">\(\Omega\)</span> can be inverted, using <span class="math display">\[
\tau_k = \sqrt{\Sigma_{k,k}}
\quad\textsf{and}\quad
\Omega_{i, j} = \frac{\Sigma_{i, j}}{\tau_i \, \tau_j}.
\]</span></p>
<p>The components of the scale vector <span class="math inline">\(\tau\)</span> can be given any reasonable prior for scales, but we recommend something weakly informative like a half-Cauchy distribution with a small scale, such as <span class="math display">\[
\tau_k \sim \textsf{Cauchy}(0, 2.5)
\quad\text{for}\quad k \in \{ 1, \dotsc, K \}
\quad\text{constrained\ by}\quad \tau_k &gt; 0.
\]</span> As for the prior means, if there is information about the scale of variation of coefficients across groups, it should be incorporated into the prior for <span class="math inline">\(\tau\)</span>. For large numbers of exchangeable coefficients, the components of <span class="math inline">\(\tau\)</span> itself (perhaps excluding the intercept) may themselves be given a hierarchical prior.</p>
<p>Our final recommendation is to give the correlation matrix <span class="math inline">\(\Omega\)</span> an LKJ prior with shape <span class="math inline">\(\eta \geq 1\)</span>,<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p><span class="math display">\[
\Omega \sim \textsf{LKJCorr}(\eta).
\]</span></p>
<p>The LKJ correlation distribution is defined by <span class="math display">\[
\textsf{LKJCorr}\left(\Sigma \mid \eta\right)
\propto
\operatorname{det}\left(\Sigma\right)^{\eta - 1}.
\]</span></p>
<p>The basic behavior of the LKJ correlation distribution is similar to that of a beta distribution. For <span class="math inline">\(\eta = 1\)</span>, the result is a uniform distribution. Despite being the identity over correlation matrices, the marginal distribution over the entries in that matrix (i.e., the correlations) is not uniform between -1 and 1. Rather, it concentrates around zero as the dimensionality increases due to the complex constraints.</p>
<p>For <span class="math inline">\(\eta &gt; 1\)</span>, the density increasingly concentrates mass around the unit matrix, i.e., favoring less correlation. For <span class="math inline">\(\eta &lt; 1\)</span>, it increasingly concentrates mass in the other direction, i.e., favoring more correlation.</p>
<p>The LKJ prior may thus be used to control the expected amount of correlation among the parameters <span class="math inline">\(\beta_j\)</span>. For a discussion of decomposing a covariance prior into a prior on correlation matrices and an independent prior on scales, see <span class="citation" data-cites="barnard-mcculloch-meng:2000">Barnard, McCulloch, and Meng (<a href="#ref-barnard-mcculloch-meng:2000" role="doc-biblioref">2000</a>)</span>.</p>
</section>
<section id="group-level-predictors-for-prior-mean" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="group-level-predictors-for-prior-mean">Group-level predictors for prior mean</h4>
<p>To complete Gelman and Hill’s model, suppose each group <span class="math inline">\(j \in \{ 1, \dotsc, J \}\)</span> is supplied with an <span class="math inline">\(L\)</span>-dimensional row-vector of group-level predictors <span class="math inline">\(u_j\)</span>. The prior mean for the <span class="math inline">\(\beta_j\)</span> can then itself be modeled as a regression, using an <span class="math inline">\(L\)</span>-dimensional coefficient vector <span class="math inline">\(\gamma\)</span>. The prior for the group-level coefficients then becomes <span class="math display">\[
\beta_j \sim \textsf{multivariate normal}(u_j \, \gamma, \Sigma)
\]</span></p>
<p>The group-level coefficients <span class="math inline">\(\gamma\)</span> may themselves be given independent weakly informative priors, such as <span class="math display">\[
\gamma_l \sim \textsf{normal}(0,5).
\]</span> As usual, information about the group-level means should be incorporated into this prior.</p>
</section>
<section id="coding-the-model-in-stan" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="coding-the-model-in-stan">Coding the model in Stan</h4>
<p>The Stan code for the full hierarchical model with multivariate priors on the group-level coefficients and group-level prior means follows its definition.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;              <span class="co">// num individuals</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; K;              <span class="co">// num ind predictors</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; J;              <span class="co">// num groups</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; L;              <span class="co">// num group predictors</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>, <span class="kw">upper</span>=J&gt; jj;  <span class="co">// group for individual</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N, K] x;              <span class="co">// individual predictors</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[J] <span class="dt">row_vector</span>[L] u;    <span class="co">// group predictors</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] y;                 <span class="co">// outcomes</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>  <span class="dt">corr_matrix</span>[K] Omega;        <span class="co">// prior correlation</span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt;[K] tau;      <span class="co">// prior scale</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[L, K] gamma;          <span class="co">// group coeffs</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[J] <span class="dt">vector</span>[K] beta;     <span class="co">// indiv coeffs by group</span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;         <span class="co">// prediction error scale</span></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>  tau ~ cauchy(<span class="dv">0</span>, <span class="fl">2.5</span>);</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>  Omega ~ lkj_corr(<span class="dv">2</span>);</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>  to_vector(gamma) ~ normal(<span class="dv">0</span>, <span class="dv">5</span>);</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>    <span class="dt">array</span>[J] <span class="dt">row_vector</span>[K] u_gamma;</span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span>:J) {</span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>      u_gamma[j] = u[j] * gamma;</span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a>    beta ~ multi_normal(u_gamma, quad_form_diag(Omega, tau));</span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a>    y[n] ~ normal(x[n] * beta[jj[n]], sigma);</span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The hyperprior covariance matrix is defined implicitly through the quadratic form in the code because the correlation matrix <code>Omega</code> and scale vector <code>tau</code> are more natural to inspect in the output; to output <code>Sigma</code>, define it as a transformed parameter. The function <code>quad_form_diag</code> is defined so that <code>quad_form_diag(Sigma, tau)</code> is equivalent to <code>diag_matrix(tau) * Sigma * diag_matrix(tau)</code>, where <code>diag_matrix(tau)</code> returns the matrix with <code>tau</code> on the diagonal and zeroes off diagonal; the version using <code>quad_form_diag</code> should be faster. For details on these and other matrix arithmetic operators and functions, see the function reference manual.</p>
</section>
<section id="optimization-through-vectorization" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="optimization-through-vectorization">Optimization through vectorization</h4>
<p>The code in the Stan program above can be sped up dramatically by replacing the the distribution statement inside the for loop:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>  y[n] ~ normal(x[n] * beta[jj[n]], sigma);</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>with the vectorized distribution statement:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] x_beta_jj;</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    x_beta_jj[n] = x[n] * beta[jj[n]];</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>  y ~ normal(x_beta_jj, sigma);</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The outer brackets create a local scope in which to define the variable <code>x_beta_jj</code>, which is then filled in a loop and used to define a vectorized distribution statement. The reason this is such a big win is that it allows us to take the log of sigma only once and it greatly reduces the size of the resulting expression graph by packing all of the work into a single distribution function.</p>
<p>Although it is tempting to redeclare <code>beta</code> and include a revised model block distribution statement,</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[J, K] beta;</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="co">// ...</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>  y ~ normal(rows_dot_product(x, beta[jj]), sigma);</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>this fails because it breaks the vectorization for <code>beta</code>,<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<div class="sourceCode" id="cb43"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>beta ~ multi_normal(...);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>which requires <code>beta</code> to be an array of vectors. Both vectorizations are important, so the best solution is to just use the loop above, because <code>rows_dot_product</code> cannot do much optimization in and of itself because there are no shared computations.</p>
<p>The code in the Stan program above also builds up an array of vectors for the outcomes and for the multivariate normal, which provides a major speedup by reducing the number of linear systems that need to be solved and differentiated.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[K, K] Sigma_beta;</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>  Sigma_beta = quad_form_diag(Omega, tau);</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span>:J) {</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>    beta[j] ~ multi_normal((u[j] * gamma)', Sigma_beta);</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In this example, the covariance matrix <code>Sigma_beta</code> is defined as a local variable so as not to have to repeat the quadratic form computation <span class="math inline">\(J\)</span> times. This vectorization can be combined with the Cholesky-factor optimization in the next section.</p>
</section>
<section id="optimization-through-cholesky-factorization" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="optimization-through-cholesky-factorization">Optimization through Cholesky factorization</h4>
<p>The multivariate normal density and LKJ prior on correlation matrices both require their matrix parameters to be factored. Vectorizing, as in the previous section, ensures this is only done once for each density. An even better solution, both in terms of efficiency and numerical stability, is to parameterize the model directly in terms of Cholesky factors of correlation matrices using the multivariate version of the non-centered parameterization. For the model in the previous section, the program fragment to replace the full matrix prior with an equivalent Cholesky factorized prior is as follows.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[L, J] u;              <span class="co">// group predictors transposed</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[K, J] z;</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">cholesky_factor_corr</span>[K] L_Omega;</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[K, L] gamma;</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed parameters</span> {</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[K, J] beta;</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>  beta = gamma * u + diag_pre_multiply(tau, L_Omega) * z;</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>  to_vector(z) ~ std_normal();</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>  L_Omega ~ lkj_corr_cholesky(<span class="dv">2</span>);</span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The data variable <code>u</code> was originally an array of vectors, which is efficient for access; here it is redeclared as a matrix in order to use it in matrix arithmetic. Moreover, it is transposed, along with <code>gamma</code> and <code>beta</code>, to minimize the number of transposition operations. The new parameter <code>L_Omega</code> is the Cholesky factor of the original correlation matrix <code>Omega</code>, so that</p>
<pre><code>Omega = L_Omega * L_Omega'</code></pre>
<p>The prior scale vector <code>tau</code> is unchanged, and furthermore, pre-multiplying the Cholesky factor by the scale produces the Cholesky factor of the final covariance matrix,</p>
<pre><code>Sigma_beta
  = quad_form_diag(Omega, tau)
  = diag_pre_multiply(tau, L_Omega) * diag_pre_multiply(tau, L_Omega)'</code></pre>
<p>where the diagonal pre-multiply compound operation is defined by</p>
<pre><code>diag_pre_multiply(a, b) = diag_matrix(a) * b</code></pre>
<p>The new variable <code>z</code> is declared as a matrix, the entries of which are given independent standard normal priors; the <code>to_vector</code> operation turns the matrix into a vector so that it can be used as a vectorized argument to the univariate normal density. This results in every column of <code>z</code> being a <span class="math inline">\(K\)</span>-variate normal random vector with the identity as covariance matrix. Therefore, multiplying <code>z</code> by the Cholesky factor of the covariance matrix and adding the mean <code>(u * gamma)'</code> produces a <code>beta</code> distributed as in the original model, where the variance is, letting <span class="math inline">\(L = \mathrm{diag}(\tau)\,\Omega_L\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{V}[\beta] &amp;= \mathbb{E}\big((L \, z)(L \, z)^\top) \\
&amp;= \mathbb{E}\big((L \, z \, z^\top \, L^\top) \\
&amp;= L \, \mathbb{E}(z \, z^\top) \, L^\top \\
&amp;= L \, L^\top =(\mathrm{diag}(\tau)\,\Omega_L)\,(\mathrm{diag}(\tau)\,\Omega_L)^\top \\
&amp;= \mathrm{diag}(\tau)\,\Omega\,\mathrm{diag}(\tau) \\
&amp;= \Sigma.
\end{aligned}
\]</span> Where we have used the linearity of expectations (line 2 to 3), the definition of <span class="math inline">\(\Omega = \Omega_L \, \Omega_L^\top\)</span>, and the fact that <span class="math inline">\(\mathbb{E}(z \, z^\top) = I\)</span> since <span class="math inline">\(z \sim \mathcal{N}(0, I)\)</span>.</p>
<p>Omitting the remaining data declarations, which are the same as before with the exception of <code>u</code>, the optimized model is as follows.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[K, J] z;</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">cholesky_factor_corr</span>[K] L_Omega;</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>, <span class="kw">upper</span>=pi() / <span class="dv">2</span>&gt;[K] tau_unif;  <span class="co">// prior scale</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[K, L] gamma;                        <span class="co">// group coeffs</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;                       <span class="co">// prediction error scale</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed parameters</span> {</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt;[K] tau = <span class="fl">2.5</span> * tan(tau_unif);</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[K, J] beta = gamma * u + diag_pre_multiply(tau, L_Omega) * z;</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] mu;</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>    mu[n] = x[n, ] * beta[, jj[n]];</span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>  to_vector(z) ~ std_normal();</span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a>  L_Omega ~ lkj_corr_cholesky(<span class="dv">2</span>);</span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a>  to_vector(gamma) ~ normal(<span class="dv">0</span>, <span class="dv">5</span>);</span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a>  y ~ normal(mu, sigma);</span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This model also reparameterizes the prior scale <code>tau</code> to avoid potential problems with the heavy tails of the Cauchy distribution. The statement <code>tau_unif ~ uniform(0, pi() / 2)</code> can be omitted from the model block because Stan increments the log posterior for parameters with uniform priors without it.</p>
</section>
</section>
</section>
<section id="prediction-forecasting-and-backcasting" class="level2">
<h2 class="anchored" data-anchor-id="prediction-forecasting-and-backcasting">Prediction, forecasting, and backcasting</h2>
<p>Stan models can be used for “predicting” the values of arbitrary model unknowns. When predictions are about the future, they’re called “forecasts;” when they are predictions about the past, as in climate reconstruction or cosmology, they are sometimes called “backcasts” (or “aftcasts” or “hindcasts” or “antecasts,” depending on the author’s feelings about the opposite of “fore”).</p>
<section id="programming-predictions" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="programming-predictions">Programming predictions</h3>
<p>As a simple example, the following linear regression provides the same setup for estimating the coefficients <code>beta</code> as in our very first example, using <code>y</code> for the <code>N</code> observations and <code>x</code> for the <code>N</code> predictor vectors. The model parameters and model for observations are exactly the same as before.</p>
<p>To make predictions, we need to be given the number of predictions, <code>N_new</code>, and their predictor matrix, <code>x_new</code>. The predictions themselves are modeled as a parameter <code>y_new</code>. The model statement for the predictions is exactly the same as for the observations, with the new outcome vector <code>y_new</code> and prediction matrix <code>x_new</code>.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; K;</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N, K] x;</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] y;</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N_new;</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N_new, K] x_new;</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[K] beta;</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N_new] y_new;                  <span class="co">// predictions</span></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>  y ~ normal(x * beta, sigma);          <span class="co">// observed model</span></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>  y_new ~ normal(x_new * beta, sigma);  <span class="co">// prediction model</span></span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="predictions-as-generated-quantities" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="predictions-as-generated-quantities">Predictions as generated quantities</h3>
<p>Where possible, the most efficient way to generate predictions is to use the generated quantities block. This provides proper Monte Carlo (not Markov chain Monte Carlo) inference, which can have a much higher effective sample size per iteration.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co">// ...data as above...</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[K] beta;</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>  y ~ normal(x * beta, sigma);</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N_new] y_new;</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N_new) {</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>    y_new[n] = normal_rng(x_new[n] * beta, sigma);</span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now the data are just as before, but the parameter <code>y_new</code> is now declared as a generated quantity, and the prediction model is removed from the model and replaced by a pseudo-random draw from a normal distribution.</p>
<section id="overflow-in-generated-quantities" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="overflow-in-generated-quantities">Overflow in generated quantities</h4>
<p>It is possible for values to overflow or underflow in generated quantities. The problem is that if the result is NaN, then any constraints placed on the variables will be violated. It is possible to check a value assigned by an RNG and reject it if it overflows, but this is both inefficient and leads to biased posterior estimates. Instead, the conditions causing overflow, such as trying to generate a negative binomial random variate with a mean of <span class="math inline">\(2^{31}\)</span>, must be intercepted and dealt with. This is typically done by reparameterizing or reimplementing the random number generator using real values rather than integers, which are upper-bounded by <span class="math inline">\(2^{31} - 1\)</span> in Stan.</p>
</section>
</section>
</section>
<section id="multivariate-outcomes" class="level2">
<h2 class="anchored" data-anchor-id="multivariate-outcomes">Multivariate outcomes</h2>
<p>Most regressions are set up to model univariate observations (be they scalar, boolean, categorical, ordinal, or count). Even multinomial regressions are just repeated categorical regressions. In contrast, this section discusses regression when each observed value is multivariate. To relate multiple outcomes in a regression setting, their error terms are provided with covariance structure.</p>
<p>This section considers two cases, seemingly unrelated regressions for continuous multivariate quantities and multivariate probit regression for boolean multivariate quantities.</p>
<section id="seemingly-unrelated-regressions" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="seemingly-unrelated-regressions">Seemingly unrelated regressions</h3>
<p>The first model considered is the “seemingly unrelated” regressions (SUR) of econometrics where several linear regressions share predictors and use a covariance error structure rather than independent errors <span class="citation" data-cites="Zellner:1962 Greene:2011">(<a href="#ref-Zellner:1962" role="doc-biblioref">Zellner 1962</a>; <a href="#ref-Greene:2011" role="doc-biblioref">Greene 2011</a>)</span>.</p>
<p>The model is easy to write down as a regression, <span class="math display">\[\begin{align*}
y_n        &amp;=  x_n \, \beta + \epsilon_n \\
\epsilon_n &amp;\sim \textsf{multivariate normal}(0, \Sigma)
\end{align*}\]</span></p>
<p>where <span class="math inline">\(x_n\)</span> is a <span class="math inline">\(J\)</span>-row-vector of predictors (<span class="math inline">\(x\)</span> is an <span class="math inline">\((N \times
J)\)</span> matrix), <span class="math inline">\(y_n\)</span> is a <span class="math inline">\(K\)</span>-vector of observations, <span class="math inline">\(\beta\)</span> is a <span class="math inline">\((K
\times J)\)</span> matrix of regression coefficients (vector <span class="math inline">\(\beta_k\)</span> holds coefficients for outcome <span class="math inline">\(k\)</span>), and <span class="math inline">\(\Sigma\)</span> is covariance matrix governing the error. As usual, the intercept can be rolled into <span class="math inline">\(x\)</span> as a column of ones.</p>
<p>The basic Stan code is straightforward (though see below for more optimized code for use with LKJ priors on correlation).</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; K;</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; J;</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">vector</span>[J] x;</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">vector</span>[K] y;</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[K, J] beta;</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">cov_matrix</span>[K] Sigma;</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">vector</span>[K] mu;</span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a>    mu[n] = beta * x[n];</span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a>  y ~ multi_normal(mu, Sigma);</span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For efficiency, the multivariate normal is vectorized by precomputing the array of mean vectors and sharing the same covariance matrix.</p>
<p>Following the advice in the <a href="#multivariate-hierarchical-priors.section">multivariate hierarchical priors section</a>, we will place a weakly informative normal prior on the regression coefficients, an LKJ prior on the correlations and a half-Cauchy prior on standard deviations. The covariance structure is parameterized in terms of Cholesky factors for efficiency and arithmetic stability.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co">// ...</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[K, J] beta;</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">cholesky_factor_corr</span>[K] L_Omega;</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt;[K] L_sigma;</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">vector</span>[K] mu;</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[K, K] L_Sigma;</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>    mu[n] = beta * x[n];</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>  L_Sigma = diag_pre_multiply(L_sigma, L_Omega);</span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>  to_vector(beta) ~ normal(<span class="dv">0</span>, <span class="dv">5</span>);</span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a>  L_Omega ~ lkj_corr_cholesky(<span class="dv">4</span>);</span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a>  L_sigma ~ cauchy(<span class="dv">0</span>, <span class="fl">2.5</span>);</span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a>  y ~ multi_normal_cholesky(mu, L_Sigma);</span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The Cholesky factor of the covariance matrix is then reconstructed as a local variable and used in the model by scaling the Cholesky factor of the correlation matrices. The regression coefficients get a prior all at once by converting the matrix <code>beta</code> to a vector.</p>
<p>If required, the full correlation or covariance matrices may be reconstructed from their Cholesky factors in the generated quantities block.</p>
</section>
<section id="multivariate-probit-regression" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="multivariate-probit-regression">Multivariate probit regression</h3>
<p>The multivariate probit model generates sequences of boolean variables by applying a step function to the output of a seemingly unrelated regression.</p>
<p>The observations <span class="math inline">\(y_n\)</span> are <span class="math inline">\(D\)</span>-vectors of boolean values (coded 0 for false, 1 for true). The values for the observations <span class="math inline">\(y_n\)</span> are based on latent values <span class="math inline">\(z_n\)</span> drawn from a seemingly unrelated regression model (see the previous section), <span class="math display">\[\begin{align*}
z_n        &amp;=     x_n \, \beta + \epsilon_n \\
\epsilon_n &amp;\sim  \textsf{multivariate normal}(0, \Sigma)
\end{align*}\]</span></p>
<p>These are then put through the step function to produce a <span class="math inline">\(K\)</span>-vector <span class="math inline">\(z_n\)</span> of boolean values with elements defined by <span class="math display">\[
y_{n, k} = \operatorname{I}\left(z_{n, k} &gt; 0\right),
\]</span> where <span class="math inline">\(\operatorname{I}()\)</span> is the indicator function taking the value 1 if its argument is true and 0 otherwise.</p>
<p>Unlike in the seemingly unrelated regressions case, here the covariance matrix <span class="math inline">\(\Sigma\)</span> has unit standard deviations (i.e., it is a correlation matrix). As with ordinary probit and logistic regressions, letting the scale vary causes the model (which is defined only by a cutpoint at 0, not a scale) to be unidentified (see <span class="citation" data-cites="Greene:2011">Greene (<a href="#ref-Greene:2011" role="doc-biblioref">2011</a>)</span>).</p>
<p>Multivariate probit regression can be coded in Stan using the trick introduced by <span class="citation" data-cites="AlbertChib:1993">Albert and Chib (<a href="#ref-AlbertChib:1993" role="doc-biblioref">1993</a>)</span>, where the underlying continuous value vectors <span class="math inline">\(y_n\)</span> are coded as truncated parameters. The key to coding the model in Stan is declaring the latent vector <span class="math inline">\(z\)</span> in two parts, based on whether the corresponding value of <span class="math inline">\(y\)</span> is 0 or 1. Otherwise, the model is identical to the seemingly unrelated regression model in the previous section.</p>
<p>First, we introduce a sum function for two-dimensional arrays of integers; this is going to help us calculate how many total 1 values there are in <span class="math inline">\(y\)</span>.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="kw">functions</span> {</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> sum2d(<span class="dt">array</span>[,] <span class="dt">int</span> a) {</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> s = <span class="dv">0</span>;</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span>:size(a)) {</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>      s += sum(a[i]);</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> s;</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The function is trivial, but it’s not a built-in for Stan and it’s easier to understand the rest of the model if it’s pulled into its own function so as not to create a distraction.</p>
<p>The data declaration block is much like for the seemingly unrelated regressions, but the observations <code>y</code> are now integers constrained to be 0 or 1.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; K;</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; D;</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N, D] <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>, <span class="kw">upper</span>=<span class="dv">1</span>&gt; y;</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">vector</span>[K] x;</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>After declaring the data, there is a rather involved transformed data block whose sole purpose is to sort the data array <code>y</code> into positive and negative components, keeping track of indexes so that <code>z</code> can be easily reassembled in the transformed parameters block.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N_pos;</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[sum2d(y)] <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>, <span class="kw">upper</span>=N&gt; n_pos;</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[size(n_pos)] <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>, <span class="kw">upper</span>=D&gt; d_pos;</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N_neg;</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[(N * D) - size(n_pos)] <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>, <span class="kw">upper</span>=N&gt; n_neg;</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[size(n_neg)] <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>, <span class="kw">upper</span>=D&gt; d_neg;</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>  N_pos = size(n_pos);</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>  N_neg = size(n_neg);</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> i;</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> j;</span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>    i = <span class="dv">1</span>;</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>    j = <span class="dv">1</span>;</span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span>:D) {</span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (y[n, d] == <span class="dv">1</span>) {</span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>          n_pos[i] = n;</span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>          d_pos[i] = d;</span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>          i += <span class="dv">1</span>;</span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>        } <span class="cf">else</span> {</span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a>          n_neg[j] = n;</span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a>          d_neg[j] = d;</span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>          j += <span class="dv">1</span>;</span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb56-30"><a href="#cb56-30" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The variables <code>N_pos</code> and <code>N_neg</code> are set to the number of true (1) and number of false (0) observations in <code>y</code>. The loop then fills in the sequence of indexes for the positive and negative values in four arrays.</p>
<p>The parameters are declared as follows.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[D, K] beta;</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">cholesky_factor_corr</span>[D] L_Omega;</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt;[N_pos] z_pos;</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>&lt;<span class="kw">upper</span>=<span class="dv">0</span>&gt;[N_neg] z_neg;</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>These include the regression coefficients <code>beta</code> and the Cholesky factor of the correlation matrix, <code>L_Omega</code>. This time there is no scaling because the covariance matrix has unit scale (i.e., it is a correlation matrix; see above).</p>
<p>The critical part of the parameter declaration is that the latent real value <span class="math inline">\(z\)</span> is broken into positive-constrained and negative-constrained components, whose size was conveniently calculated in the transformed data block. The transformed data block’s real work was to allow the transformed parameter block to reconstruct <span class="math inline">\(z\)</span>.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed parameters</span> {</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">vector</span>[D] z;</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N_pos) {</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>    z[n_pos[n], d_pos[n]] = z_pos[n];</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N_neg) {</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>    z[n_neg[n], d_neg[n]] = z_neg[n];</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>At this point, the model is simple, pretty much recreating the seemingly unrelated regression.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>  L_Omega ~ lkj_corr_cholesky(<span class="dv">4</span>);</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>  to_vector(beta) ~ normal(<span class="dv">0</span>, <span class="dv">5</span>);</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>    <span class="dt">array</span>[N] <span class="dt">vector</span>[D] beta_x;</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>      beta_x[n] = beta * x[n];</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>    z ~ multi_normal_cholesky(beta_x, L_Omega);</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This simple form of model is made possible by the Albert and Chib-style constraints on <code>z</code>.</p>
<p>Finally, the correlation matrix itself can be put back together in the generated quantities block if desired.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">corr_matrix</span>[D] Omega;</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>  Omega = multiply_lower_tri_self_transpose(L_Omega);</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The same could be done for the seemingly unrelated regressions in the previous section.</p>
</section>
</section>
<section id="applications-of-pseudorandom-number-generation" class="level2">
<h2 class="anchored" data-anchor-id="applications-of-pseudorandom-number-generation">Applications of pseudorandom number generation</h2>
<p>The main application of pseudorandom number generator (PRNGs) is for posterior inference, including prediction and posterior predictive checks. They can also be used for pure data simulation, which is like a posterior predictive check with no conditioning. See the function reference manual for a complete description of the syntax and usage of pseudorandom number generators.</p>
<section id="prediction" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="prediction">Prediction</h3>
<p>Consider predicting unobserved outcomes using linear regression. Given predictors <span class="math inline">\(x_1, \dotsc, x_N\)</span> and observed outcomes <span class="math inline">\(y_1, \dotsc, y_N\)</span>, and assuming a standard linear regression with intercept <span class="math inline">\(\alpha\)</span>, slope <span class="math inline">\(\beta\)</span>, and error scale <span class="math inline">\(\sigma\)</span>, along with improper uniform priors, the posterior over the parameters given <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is <span class="math display">\[
p\left(\alpha, \beta, \sigma \mid x, y \right)
\propto
\prod_{n=1}^N
  \textsf{normal}\left(y_n \mid \alpha + \beta x_n, \sigma\right).
\]</span></p>
<p>For this model, the posterior predictive inference for a new outcome <span class="math inline">\(\tilde{y}_m\)</span> given a predictor <span class="math inline">\(\tilde{x}_m\)</span>, conditioned on the observed data <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, is <span class="math display">\[
p\left(\tilde{y}_n \mid \tilde{x}_n, x, y\right)
= \int_{(\alpha,\beta,\sigma)}
  \textsf{normal}\left(\tilde{y}_n \mid \alpha + \beta \tilde{x}_n, \sigma\right)
  \times
  p\left(\alpha, \beta, \sigma \mid x, y\right)
  \,\textrm{d}(\alpha,\beta,\sigma).
\]</span></p>
<p>To code the posterior predictive inference in Stan, a standard linear regression is combined with a random number in the generated quantities block.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] y;</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] x;</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N_tilde;</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N_tilde] x_tilde;</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> alpha;</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> beta;</span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a>  y ~ normal(alpha + beta * x, sigma);</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N_tilde] y_tilde;</span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N_tilde) {</span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a>    y_tilde[n] = normal_rng(alpha + beta * x_tilde[n], sigma);</span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Given observed predictors <span class="math inline">\(x\)</span> and outcomes <span class="math inline">\(y\)</span>, <code>y_tilde</code> will be drawn according to <span class="math inline">\(p\left(\tilde{y} \mid \tilde{x}, y, x\right)\)</span>. This means that, for example, the posterior mean for <code>y_tilde</code> is the estimate of the outcome that minimizes expected square error (conditioned on the data and model).</p>
</section>
<section id="posterior-predictive-checks" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="posterior-predictive-checks">Posterior predictive checks</h3>
<p>A good way to investigate the fit of a model to the data, a critical step in Bayesian data analysis, is to generate simulated data according to the parameters of the model. This is carried out with exactly the same procedure as before, only the observed data predictors <span class="math inline">\(x\)</span> are used in place of new predictors <span class="math inline">\(\tilde{x}\)</span> for unobserved outcomes. If the model fits the data well, the predictions for <span class="math inline">\(\tilde{y}\)</span> based on <span class="math inline">\(x\)</span> should match the observed data <span class="math inline">\(y\)</span>.</p>
<p>To code posterior predictive checks in Stan requires only a slight modification of the prediction code to use <span class="math inline">\(x\)</span> and <span class="math inline">\(N\)</span> in place of <span class="math inline">\(\tilde{x}\)</span> and <span class="math inline">\(\tilde{N}\)</span>,</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] y_tilde;</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>    y_tilde[n] = normal_rng(alpha + beta * x[n], sigma);</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><span class="citation" data-cites="GelmanEtAl:2013">Gelman et al. (<a href="#ref-GelmanEtAl:2013" role="doc-biblioref">2013</a>)</span> recommend choosing several posterior draws <span class="math inline">\(\tilde{y}^{(1)}, \dotsc, \tilde{y}^{(M)}\)</span> and plotting each of them alongside the data <span class="math inline">\(y\)</span> that was actually observed. If the model fits well, the simulated <span class="math inline">\(\tilde{y}\)</span> will look like the actual data <span class="math inline">\(y\)</span>.</p>



</section>
</section>
</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-AlbertChib:1993" class="csl-entry" role="listitem">
Albert, J. H., and S. Chib. 1993. <span>“Bayesian Analysis of Binary and Polychotomous Response Data.”</span> <em>Journal of the American Statistical Association</em> 88: 669–79.
</div>
<div id="ref-barnard-mcculloch-meng:2000" class="csl-entry" role="listitem">
Barnard, John, Robert McCulloch, and Xiao-Li Meng. 2000. <span>“Modeling Covariance Matrices in Terms of Standard Deviations and Correlations, with Application to Shrinkage.”</span> <em>Statistica Sinica</em>, 1281–311.
</div>
<div id="ref-ChungEtAl:2013" class="csl-entry" role="listitem">
Chung, Yeojin, Sophia Rabe-Hesketh, Vincent Dorie, Andrew Gelman, and Jingchen Liu. 2013. <span>“A Nondegenerate Penalized Likelihood Estimator for Variance Parameters in Multilevel Models.”</span> <em>Psychometrika</em> 78 (4): 685–709.
</div>
<div id="ref-Curtis:2010" class="csl-entry" role="listitem">
Curtis, S. McKay. 2010. <span>“<span>BUGS</span> Code for Item Response Theory.”</span> <em>Journal of Statistical Software</em> 36 (1): 1–34.
</div>
<div id="ref-GelmanEtAl:2013" class="csl-entry" role="listitem">
Gelman, Andrew, J. B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. <em>Bayesian Data Analysis</em>. Third Edition. London: Chapman &amp; Hall / CRC Press.
</div>
<div id="ref-GelmanHill:2007" class="csl-entry" role="listitem">
Gelman, Andrew, and Jennifer Hill. 2007. <em>Data Analysis Using Regression and Multilevel-Hierarchical Models</em>. Cambridge, United Kingdom: Cambridge University Press.
</div>
<div id="ref-Greene:2011" class="csl-entry" role="listitem">
Greene, William H. 2011. <em>Econometric Analysis</em>. 7th ed. Prentice-Hall.
</div>
<div id="ref-LewandowskiKurowickaJoe:2009" class="csl-entry" role="listitem">
Lewandowski, Daniel, Dorota Kurowicka, and Harry Joe. 2009. <span>“Generating Random Correlation Matrices Based on Vines and Extended Onion Method.”</span> <em>Journal of Multivariate Analysis</em> 100: 1989–2001.
</div>
<div id="ref-Zellner:1962" class="csl-entry" role="listitem">
Zellner, Arnold. 1962. <span>“An Efficient Method of Estimating Seemingly Unrelated Regression Equations and Tests for Aggregation Bias.”</span> <em>Journal of the American Statistical Association</em> 57: 348–68.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Unlike in Python and R, which are interpreted, Stan is translated to C++ and compiled, so loops and assignment statements are fast. Vectorized code is faster in Stan because (a) the expression tree used to compute derivatives can be simplified, leading to fewer virtual function calls, and (b) computations that would be repeated in the looping version, such as <code>log(sigma)</code> in the above model, will be computed once and reused.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The <code>Phi_approx</code> function is a rescaled version of the inverse logit function, so while the scale is roughly the same <span class="math inline">\(\Phi\)</span>, the tails do not match.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The Bernoulli-logit distribution builds in the log link function, taking <span class="math display">\[\texttt{bernoulli}\mathtt{\_}\texttt{logit}\left(y \mid \alpha\right) = \texttt{bernoulli}\left(y \mid \operatorname{logit}^{-1}(\alpha)\right).\]</span><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><span class="citation" data-cites="GelmanHill:2007">Gelman and Hill (<a href="#ref-GelmanHill:2007" role="doc-biblioref">2007</a>)</span> treat the <span class="math inline">\(\delta\)</span> term equivalently as the location parameter in the distribution of student abilities.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>The prior is named for Lewandowski, Kurowicka, and Joe, as it was derived by inverting the random correlation matrix generation strategy of <span class="citation" data-cites="LewandowskiKurowickaJoe:2009">Lewandowski, Kurowicka, and Joe (<a href="#ref-LewandowskiKurowickaJoe:2009" role="doc-biblioref">2009</a>)</span>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Thanks to Mike Lawrence for pointing this out in the GitHub issue for the manual.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../stan-users-guide/index.html" class="pagination-link" aria-label="Stan User's Guide">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Stan User’s Guide</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../stan-users-guide/time-series.html" class="pagination-link" aria-label="Time-Series Models">
        <span class="nav-page-text">Time-Series Models</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/stan-dev/docs/edit/master/src/stan-users-guide/regression.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/stan-dev/docs/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>