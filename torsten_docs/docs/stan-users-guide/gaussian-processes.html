<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.53">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Gaussian Processes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../stan-users-guide/hyperspherical-models.html" rel="next">
<link href="../stan-users-guide/clustering.html" rel="prev">
<link href="../img/logo_tm.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 200,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../theming/quarto_styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../img/logo_tm.png" alt="Stan logo" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Overview</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../stan-users-guide/index.html" aria-current="page"> 
<span class="menu-text">Stan Users Guide</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../reference-manual/index.html"> 
<span class="menu-text">Reference Manual</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../functions-reference/index.html"> 
<span class="menu-text">Functions Reference</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-interfaces" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Interfaces</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-interfaces">    
        <li>
    <a class="dropdown-item" href="../cmdstan-guide/index.html">
 <span class="dropdown-text">CmdStan</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/cmdstanpy">
 <span class="dropdown-text">CmdStanPy</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/cmdstanr">
 <span class="dropdown-text">CmdStanR</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/users/interfaces/pystan.html">
 <span class="dropdown-text">PyStan</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/rstan">
 <span class="dropdown-text">RStan</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="http://stanjulia.github.io/Stan.jl/stable/INTRO/">
 <span class="dropdown-text">Stan.jl</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-other-packages" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Other Packages</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-other-packages">    
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/bayesplot/">
 <span class="dropdown-text">bayesplot</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://roualdes.github.io/bridgestan/latest/">
 <span class="dropdown-text">BridgeStan</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://paul-buerkner.github.io/brms/">
 <span class="dropdown-text">brms</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/loo/">
 <span class="dropdown-text">loo</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/posterior">
 <span class="dropdown-text">posterior</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/projpred">
 <span class="dropdown-text">projpred</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/rstanarm">
 <span class="dropdown-text">rstanarm</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/rstantools">
 <span class="dropdown-text">rstantools</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://mc-stan.org/shinystan">
 <span class="dropdown-text">shinystan</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/stan-dev" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
    <a href="https://discourse.mc-stan.org" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-chat-text-fill"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../stan-users-guide/regression.html">Example Models</a></li><li class="breadcrumb-item"><a href="../stan-users-guide/gaussian-processes.html">Gaussian Processes</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Stan User’s Guide</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Version 2.35</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Example Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Regression Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/time-series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Time-Series Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/missing-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Missing Data and Partially Known Parameters</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/truncation-censoring.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Truncated or Censored Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/finite-mixtures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Finite Mixtures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/measurement-error.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Measurement Error and Meta-Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/latent-discrete.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Latent Discrete Parameters</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/sparse-ragged.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sparse and Ragged Data Structures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Clustering Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/gaussian-processes.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Gaussian Processes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/hyperspherical-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Directions, Rotations, and Hyperspheres</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/algebraic-equations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Solving Algebraic Equations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/odes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ordinary Differential Equations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/one-dimensional-integrals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Computing One Dimensional Integrals</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/complex-numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Complex Numbers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/dae.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Differential-Algebraic Equations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/survival.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Survival Models</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Programming Techniques</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/floating-point.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Floating Point Arithmetic</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/matrices-arrays.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Matrices, Vectors, Arrays, and Tuples</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/multi-indexing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Multiple Indexing and Range Indexing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/user-functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">User-Defined Functions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/custom-probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Custom Probability Functions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/proportionality-constants.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Proportionality Constants</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/problematic-posteriors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Problematic Posteriors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/reparameterization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reparameterization and Change of Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/efficiency-tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficiency Tuning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/parallelization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Parallelization</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Posterior Inference &amp; Model Checking</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/posterior-prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Posterior Predictive Sampling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/simulation-based-calibration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Simulation-Based Calibration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/posterior-predictive-checks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Posterior and Prior Predictive Checks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/cross-validation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Held-Out Evaluation and Cross-Validation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/poststratification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Poststratification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/decision-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Decision Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/bootstrap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Bootstrap and Bagging</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/using-stanc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Using the Stan Compiler</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/style-guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Stan Program Style Guide</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stan-users-guide/for-bugs-users.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transitioning from BUGS</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#gaussian-processes.chapter" id="toc-gaussian-processes.chapter" class="nav-link active" data-scroll-target="#gaussian-processes.chapter">Gaussian Processes</a>
  <ul class="collapse">
  <li><a href="#gaussian-process-regression" id="toc-gaussian-process-regression" class="nav-link" data-scroll-target="#gaussian-process-regression">Gaussian process regression</a></li>
  <li><a href="#simulating-from-a-gaussian-process" id="toc-simulating-from-a-gaussian-process" class="nav-link" data-scroll-target="#simulating-from-a-gaussian-process">Simulating from a Gaussian process</a>
  <ul class="collapse">
  <li><a href="#multivariate-inputs" id="toc-multivariate-inputs" class="nav-link" data-scroll-target="#multivariate-inputs">Multivariate inputs</a></li>
  <li><a href="#cholesky-factored-and-transformed-implementation" id="toc-cholesky-factored-and-transformed-implementation" class="nav-link" data-scroll-target="#cholesky-factored-and-transformed-implementation">Cholesky factored and transformed implementation</a></li>
  </ul></li>
  <li><a href="#fit-gp.section" id="toc-fit-gp.section" class="nav-link" data-scroll-target="#fit-gp.section">Fitting a Gaussian process</a>
  <ul class="collapse">
  <li><a href="#gp-with-a-normal-outcome" id="toc-gp-with-a-normal-outcome" class="nav-link" data-scroll-target="#gp-with-a-normal-outcome">GP with a normal outcome</a></li>
  <li><a href="#discrete-outcomes-with-gaussian-processes" id="toc-discrete-outcomes-with-gaussian-processes" class="nav-link" data-scroll-target="#discrete-outcomes-with-gaussian-processes">Discrete outcomes with Gaussian processes</a></li>
  <li><a href="#automatic-relevance-determination" id="toc-automatic-relevance-determination" class="nav-link" data-scroll-target="#automatic-relevance-determination">Automatic relevance determination</a></li>
  <li><a href="#priors-gp.section" id="toc-priors-gp.section" class="nav-link" data-scroll-target="#priors-gp.section">Priors for Gaussian process parameters</a></li>
  <li><a href="#predictive-inference-with-a-gaussian-process" id="toc-predictive-inference-with-a-gaussian-process" class="nav-link" data-scroll-target="#predictive-inference-with-a-gaussian-process">Predictive inference with a Gaussian process</a></li>
  <li><a href="#multiple-output-gaussian-processes" id="toc-multiple-output-gaussian-processes" class="nav-link" data-scroll-target="#multiple-output-gaussian-processes">Multiple-output Gaussian processes</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/stan-dev/docs/edit/master/src/stan-users-guide/gaussian-processes.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/stan-dev/docs/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">




<section id="gaussian-processes.chapter" class="level1">
<h1>Gaussian Processes</h1>
<p>Gaussian processes are continuous stochastic processes and thus may be interpreted as providing a probability distribution over functions. A probability distribution over continuous functions may be viewed, roughly, as an uncountably infinite collection of random variables, one for each valid input. The generality of the supported functions makes Gaussian priors popular choices for priors in general multivariate (non-linear) regression problems.</p>
<p>The defining feature of a Gaussian process is that the joint distribution of the function’s value at a finite number of input points is a multivariate normal distribution. This makes it tractable to both fit models from finite amounts of observed data and make predictions for finitely many new data points.</p>
<p>Unlike a simple multivariate normal distribution, which is parameterized by a mean vector and covariance matrix, a Gaussian process is parameterized by a mean function and covariance function. The mean and covariance functions apply to vectors of inputs and return a mean vector and covariance matrix which provide the mean and covariance of the outputs corresponding to those input points in the functions drawn from the process.</p>
<p>Gaussian processes can be encoded in Stan by implementing their mean and covariance functions or by using the specialized covariance functions outlined below, and plugging the result into the Gaussian model.<br>
This form of model is straightforward and may be used for simulation, model fitting, or posterior predictive inference. A more efficient Stan implementation for the GP with a normally distributed outcome marginalizes over the latent Gaussian process, and applies a Cholesky-factor reparameterization of the Gaussian to compute the likelihood and the posterior predictive distribution analytically.</p>
<p>After defining Gaussian processes, this chapter covers the basic implementations for simulation, hyperparameter estimation, and posterior predictive inference for univariate regressions, multivariate regressions, and multivariate logistic regressions. Gaussian processes are general, and by necessity this chapter only touches on some basic models. For more information, see <span class="citation" data-cites="RasmussenWilliams:2006">Rasmussen and Williams (<a href="#ref-RasmussenWilliams:2006" role="doc-biblioref">2006</a>)</span>.</p>
<p>Note that fitting Gaussian processes as described below using exact inference by computing Cholesky of the covariance matrix scales cubicly with the size of data. Due to how Stan autodiff is implemented, Stan is also slower than Gaussian process specialized software. It is likely that Gaussian processes using exact inference by computing Cholesky of the covariance matrix with <span class="math inline">\(N&gt;1000\)</span> are too slow for practical purposes in Stan. There are many approximations to speed-up Gaussian process computation, from which the basis function approaches for 1-3 dimensional <span class="math inline">\(x\)</span> are easiest to implement in Stan (see, e.g., <span class="citation" data-cites="Riutort-Mayol:2023:HSGP">Riutort-Mayol et al. (<a href="#ref-Riutort-Mayol:2023:HSGP" role="doc-biblioref">2023</a>)</span>).</p>
<section id="gaussian-process-regression" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-process-regression">Gaussian process regression</h2>
<p>The data for a multivariate Gaussian process regression consists of a series of <span class="math inline">\(N\)</span> inputs <span class="math inline">\(x_1,\dotsc,x_N \in \mathbb{R}^D\)</span> paired with outputs <span class="math inline">\(y_1,\dotsc,y_N \in \mathbb{R}\)</span>. The defining feature of Gaussian processes is that the probability of a finite number of outputs <span class="math inline">\(y\)</span> conditioned on their inputs <span class="math inline">\(x\)</span> is Gaussian: <span class="math display">\[
y \sim \textsf{multivariate normal}(m(x), K(x \mid \theta)),
\]</span> where <span class="math inline">\(m(x)\)</span> is an <span class="math inline">\(N\)</span>-vector and <span class="math inline">\(K(x \mid \theta)\)</span> is an <span class="math inline">\(N \times N\)</span> covariance matrix. The mean function <span class="math inline">\(m : \mathbb{R}^{N \times D}
\rightarrow \mathbb{R}^{N}\)</span> can be anything, but the covariance function <span class="math inline">\(K : \mathbb{R}^{N \times D} \rightarrow \mathbb{R}^{N \times N}\)</span> must produce a positive-definite matrix for any input <span class="math inline">\(x\)</span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>A popular covariance function, which will be used in the implementations later in this chapter, is an exponentiated quadratic function, <span class="math display">\[
  K(x \mid \alpha, \rho, \sigma)_{i, j}
= \alpha^2
\exp \left(
- \dfrac{1}{2 \rho^2} \sum_{d=1}^D (x_{i,d} - x_{j,d})^2
\right)
+ \delta_{i, j} \sigma^2,
\]</span> where <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\rho\)</span>, and <span class="math inline">\(\sigma\)</span> are hyperparameters defining the covariance function and where <span class="math inline">\(\delta_{i, j}\)</span> is the Kronecker delta function with value 1 if <span class="math inline">\(i = j\)</span> and value 0 otherwise; this test is between the indexes <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, not between values <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>. This kernel is obtained through a convolution of two independent Gaussian processes, <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span>, with kernels <span class="math display">\[
  K_1(x \mid \alpha, \rho)_{i, j}
= \alpha^2
\exp \left(
- \dfrac{1}{2 \rho^2} \sum_{d=1}^D (x_{i,d} - x_{j,d})^2
\right)
\]</span> and <span class="math display">\[
  K_2(x \mid \sigma)_{i, j}
=
\delta_{i, j} \sigma^2,
\]</span></p>
<p>The addition of <span class="math inline">\(\sigma^2\)</span> on the diagonal is important to ensure the positive definiteness of the resulting matrix in the case of two identical inputs <span class="math inline">\(x_i = x_j\)</span>. In statistical terms, <span class="math inline">\(\sigma\)</span> is the scale of the noise term in the regression.</p>
<p>The hyperparameter <span class="math inline">\(\rho\)</span> is the <em>length-scale</em>, and corresponds to the frequency of the functions represented by the Gaussian process prior with respect to the domain. Values of <span class="math inline">\(\rho\)</span> closer to zero lead the GP to represent high-frequency functions, whereas larger values of <span class="math inline">\(\rho\)</span> lead to low-frequency functions. The hyperparameter <span class="math inline">\(\alpha\)</span> is the <em>marginal standard deviation</em>. It controls the magnitude of the range of the function represented by the GP. If you were to take the standard deviation of many draws from the GP <span class="math inline">\(f_1\)</span> prior at a single input <span class="math inline">\(x\)</span> conditional on one value of <span class="math inline">\(\alpha\)</span> one would recover <span class="math inline">\(\alpha\)</span>.</p>
<p>The only term in the squared exponential covariance function involving the inputs <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> is their vector difference, <span class="math inline">\(x_i - x_j\)</span>. This produces a process with stationary covariance in the sense that if an input vector <span class="math inline">\(x\)</span> is translated by a vector <span class="math inline">\(\epsilon\)</span> to <span class="math inline">\(x +
\epsilon\)</span>, the covariance at any pair of outputs is unchanged, because <span class="math inline">\(K(x \mid \theta) = K(x + \epsilon \mid \theta)\)</span>.</p>
<p>The summation involved is just the squared Euclidean distance between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> (i.e., the <span class="math inline">\(L_2\)</span> norm of their difference, <span class="math inline">\(x_i -
x_j\)</span>). This results in support for smooth functions in the process. The amount of variation in the function is controlled by the free hyperparameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\rho\)</span>, and <span class="math inline">\(\sigma\)</span>.</p>
<p>Changing the notion of distance from Euclidean to taxicab distance (i.e., an <span class="math inline">\(L_1\)</span> norm) changes the support to functions which are continuous but not smooth.</p>
</section>
<section id="simulating-from-a-gaussian-process" class="level2">
<h2 class="anchored" data-anchor-id="simulating-from-a-gaussian-process">Simulating from a Gaussian process</h2>
<p>It is simplest to start with a Stan model that does nothing more than simulate draws of functions <span class="math inline">\(f\)</span> from a Gaussian process. In practical terms, the model will draw values <span class="math inline">\(y_n = f(x_n)\)</span> for finitely many input points <span class="math inline">\(x_n\)</span>.</p>
<p>The Stan model defines the mean and covariance functions in a transformed data block and then samples outputs <span class="math inline">\(y\)</span> in the model using a multivariate normal distribution. To make the model concrete, the squared exponential covariance function described in the previous section will be used with hyperparameters set to <span class="math inline">\(\alpha^2 = 1\)</span>, <span class="math inline">\(\rho^2 = 1\)</span>, and <span class="math inline">\(\sigma^2 = 0.1\)</span>, and the mean function <span class="math inline">\(m\)</span> is defined to always return the zero vector, <span class="math inline">\(m(x) = \textbf{0}\)</span>. Consider the following implementation of a Gaussian process simulator.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N;</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">real</span> x;</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N, N] K;</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] mu = rep_vector(<span class="dv">0</span>, N);</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span>:(N - <span class="dv">1</span>)) {</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    K[i, i] = <span class="dv">1</span> + <span class="fl">0.1</span>;</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (j <span class="cf">in</span> (i + <span class="dv">1</span>):N) {</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>      K[i, j] = exp(-<span class="fl">0.5</span> * square(x[i] - x[j]));</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>      K[j, i] = K[i, j];</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  K[N, N] = <span class="dv">1</span> + <span class="fl">0.1</span>;</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] y;</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>  y ~ multi_normal(mu, K);</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The above model can also be written more compactly using the specialized covariance function that implements the exponentiated quadratic kernel.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N;</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">real</span> x;</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N, N] K = cov_exp_quad(x, <span class="fl">1.0</span>, <span class="fl">1.0</span>);</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] mu = rep_vector(<span class="dv">0</span>, N);</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    K[n, n] = K[n, n] + <span class="fl">0.1</span>;</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] y;</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>  y ~ multi_normal(mu, K);</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The input data are just the vector of inputs <code>x</code> and its size <code>N</code>. Such a model can be used with values of <code>x</code> evenly spaced over some interval in order to plot sample draws of functions from a Gaussian process.</p>
<section id="multivariate-inputs" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="multivariate-inputs">Multivariate inputs</h3>
<p>Only the input data needs to change in moving from a univariate model to a multivariate model.</p>
<p>The only lines that change from the univariate model above are as follows.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N;</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; D;</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">vector</span>[D] x;</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The data are now declared as an array of vectors instead of an array of scalars; the dimensionality <code>D</code> is also declared.</p>
<p>In the remainder of the chapter, univariate models will be used for simplicity, but any of the models could be changed to multivariate in the same way as the simple sampling model. The only extra computational overhead from a multivariate model is in the distance calculation.</p>
</section>
<section id="cholesky-factored-and-transformed-implementation" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="cholesky-factored-and-transformed-implementation">Cholesky factored and transformed implementation</h3>
<p>A more efficient implementation of the simulation model can be coded in Stan by relocating, rescaling and rotating an isotropic standard normal variate. Suppose <span class="math inline">\(\eta\)</span> is an an isotropic standard normal variate <span class="math display">\[
\eta \sim \textsf{normal}(\textbf{0}, \textbf{1}),
\]</span> where <span class="math inline">\(\textbf{0}\)</span> is an <span class="math inline">\(N\)</span>-vector of 0 values and <span class="math inline">\(\textbf{1}\)</span> is the <span class="math inline">\(N
\times N\)</span> identity matrix. Let <span class="math inline">\(L\)</span> be the Cholesky decomposition of <span class="math inline">\(K(x \mid \theta)\)</span>, i.e., the lower-triangular matrix <span class="math inline">\(L\)</span> such that <span class="math inline">\(LL^{\top} =
K(x \mid \theta)\)</span>. Then the transformed variable <span class="math inline">\(\mu + L\eta\)</span> has the intended target distribution, <span class="math display">\[
  \mu + L\eta \sim \textsf{multivariate normal}(\mu(x), K(x \mid \theta)).
\]</span></p>
<p>This transform can be applied directly to Gaussian process simulation.</p>
<p>This model has the same data declarations for <code>N</code> and <code>x</code>, and the same transformed data definitions of <code>mu</code> and <code>K</code> as the previous model, with the addition of a transformed data variable for the Cholesky decomposition. The parameters change to the raw parameters sampled from an isotropic standard normal, and the actual samples are defined as generated quantities.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">// ...</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N, N] L;</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  L = cholesky_decompose(K);</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] eta;</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>  eta ~ std_normal();</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] y;</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>  y = mu + L * eta;</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The Cholesky decomposition is only computed once, after the data are loaded and the covariance matrix <code>K</code> computed. The isotropic normal distribution for <code>eta</code> is specified as a vectorized univariate distribution for efficiency; this specifies that each <code>eta[n]</code> has an independent standard normal distribution. The sampled vector <code>y</code> is then defined as a generated quantity using a direct encoding of the transform described above.</p>
</section>
</section>
<section id="fit-gp.section" class="level2">
<h2 class="anchored" data-anchor-id="fit-gp.section">Fitting a Gaussian process</h2>
<section id="gp-with-a-normal-outcome" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="gp-with-a-normal-outcome">GP with a normal outcome</h3>
<p>The full generative model for a GP with a normal outcome, <span class="math inline">\(y \in \mathbb{R}^N\)</span>, with inputs <span class="math inline">\(x \in \mathbb{R}^N\)</span>, for a finite <span class="math inline">\(N\)</span>: <span class="math display">\[\begin{align*}
\rho   &amp;\sim \textsf{InvGamma}(5, 5) \\
\alpha &amp;\sim \textsf{normal}(0, 1) \\
\sigma &amp;\sim \textsf{normal}(0, 1) \\
f      &amp;\sim \textsf{multivariate normal}\left(0, K(x \mid \alpha, \rho)\right) \\
y_i    &amp;\sim \textsf{normal}(f_i, \sigma) \, \forall i \in \{1, \dots, N\}
\end{align*}\]</span> With a normal outcome, it is possible to integrate out the Gaussian process <span class="math inline">\(f\)</span>, yielding the more parsimonious model: <span class="math display">\[\begin{align*}
\rho   &amp;\sim \textsf{InvGamma}(5, 5) \\
\alpha &amp;\sim \textsf{normal}(0, 1) \\
\sigma &amp;\sim \textsf{normal}(0, 1) \\
y      &amp;\sim \textsf{multivariate normal}
         \left(0, K(x \mid \alpha, \rho) + \textbf{I}_N \sigma^2\right) \\
\end{align*}\]</span></p>
<p>It can be more computationally efficient when dealing with a normal outcome to integrate out the Gaussian process, because this yields a lower-dimensional parameter space over which to do inference. We’ll fit both models in Stan. The former model will be referred to as the latent variable GP, while the latter will be called the marginal likelihood GP.</p>
<p>The hyperparameters controlling the covariance function of a Gaussian process can be fit by assigning them priors, like we have in the generative models above, and then computing the posterior distribution of the hyperparameters given observed data. The priors on the parameters should be defined based on prior knowledge of the scale of the output values (<span class="math inline">\(\alpha\)</span>), the scale of the output noise (<span class="math inline">\(\sigma\)</span>), and the scale at which distances are measured among inputs (<span class="math inline">\(\rho\)</span>). See the <a href="#priors-gp.section">Gaussian process priors section</a> for more information about how to specify appropriate priors for the hyperparameters.</p>
<p>The Stan program implementing the marginal likelihood GP is shown below. The program is similar to the Stan programs that implement the simulation GPs above, but because we are doing inference on the hyperparameters, we need to calculate the covariance matrix <code>K</code> in the model block, rather than the transformed data block.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N;</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">real</span> x;</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] y;</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] mu = rep_vector(<span class="dv">0</span>, N);</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; rho;</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; alpha;</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N, N] L_K;</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N, N] K = cov_exp_quad(x, alpha, rho);</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> sq_sigma = square(sigma);</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>  <span class="co">// diagonal elements</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    K[n, n] = K[n, n] + sq_sigma;</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>  L_K = cholesky_decompose(K);</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>  rho ~ inv_gamma(<span class="dv">5</span>, <span class="dv">5</span>);</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>  alpha ~ std_normal();</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>  sigma ~ std_normal();</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>  y ~ multi_normal_cholesky(mu, L_K);</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The data block declares a vector <code>y</code> of observed values <code>y[n]</code> for inputs <code>x[n]</code>. The transformed data block now only defines the mean vector to be zero. The three hyperparameters are defined as parameters constrained to be non-negative. The computation of the covariance matrix <code>K</code> is now in the model block because it involves unknown parameters and thus can’t simply be precomputed as transformed data. The rest of the model consists of the priors for the hyperparameters and the multivariate Cholesky-parameterized normal distribution, only now the value <code>y</code> is known and the covariance matrix <code>K</code> is an unknown dependent on the hyperparameters, allowing us to learn the hyperparameters.</p>
<p>We have used the Cholesky parameterized multivariate normal rather than the standard parameterization because it allows us to the <code>cholesky_decompose</code> function which has been optimized for both small and large matrices. When working with small matrices the differences in computational speed between the two approaches will not be noticeable, but for larger matrices (<span class="math inline">\(N \gtrsim 100\)</span>) the Cholesky decomposition version will be faster.</p>
<p>Hamiltonian Monte Carlo sampling is fast and effective for hyperparameter inference in this model <span class="citation" data-cites="Neal:1997">(<a href="#ref-Neal:1997" role="doc-biblioref">Neal 1997</a>)</span>. If the posterior is well-concentrated for the hyperparameters the Stan implementation will fit hyperparameters in models with a few hundred data points in seconds.</p>
<section id="latent-variable-gp" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="latent-variable-gp">Latent variable GP</h4>
<p>We can also explicitly code the latent variable formulation of a GP in Stan. This will be useful for when the outcome is not normal. We’ll need to add a small positive term, <span class="math inline">\(\delta\)</span> to the diagonal of the covariance matrix in order to ensure that our covariance matrix remains positive definite.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N;</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">real</span> x;</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] y;</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> delta = <span class="fl">1e-9</span>;</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; rho;</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; alpha;</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] eta;</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] f;</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    <span class="dt">matrix</span>[N, N] L_K;</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    <span class="dt">matrix</span>[N, N] K = cov_exp_quad(x, alpha, rho);</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">// diagonal elements</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>      K[n, n] = K[n, n] + delta;</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    L_K = cholesky_decompose(K);</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    f = L_K * eta;</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>  rho ~ inv_gamma(<span class="dv">5</span>, <span class="dv">5</span>);</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>  alpha ~ std_normal();</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>  sigma ~ std_normal();</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>  eta ~ std_normal();</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>  y ~ normal(f, sigma);</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Two differences between the latent variable GP and the marginal likelihood GP are worth noting. The first is that we have augmented our parameter block with a new parameter vector of length <span class="math inline">\(N\)</span> called <code>eta</code>. This is used in the model block to generate a multivariate normal vector called <span class="math inline">\(f\)</span>, corresponding to the latent GP. We put a <span class="math inline">\(\textsf{normal}(0,1)\)</span> prior on <code>eta</code> like we did in the Cholesky-parameterized GP in the simulation section. The second difference is that although we could code the distribution statement for <span class="math inline">\(y\)</span> with one <span class="math inline">\(N\)</span>-dimensional multivariate normal with an identity covariance matrix multiplied by <span class="math inline">\(\sigma^2\)</span>, we instead use vectorized univariate normal distribution, which is equivalent but more efficient to use.</p>
</section>
</section>
<section id="discrete-outcomes-with-gaussian-processes" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="discrete-outcomes-with-gaussian-processes">Discrete outcomes with Gaussian processes</h3>
<p>Gaussian processes can be generalized the same way as standard linear models by introducing a link function. This allows them to be used as discrete data models.</p>
<section id="poisson-gp" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="poisson-gp">Poisson GP</h4>
<p>If we want to model count data, we can remove the <span class="math inline">\(\sigma\)</span> parameter, and use <code>poisson_log</code>, which implements a Poisson distribution with log link function, rather than <code>normal</code>. We can also add an overall mean parameter, <span class="math inline">\(a\)</span>, which will account for the marginal expected value for <span class="math inline">\(y\)</span>. We do this because we cannot center count data like we would for normally distributed data.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; y;</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">// ...</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; rho;</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; alpha;</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> a;</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] eta;</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>  rho ~ inv_gamma(<span class="dv">5</span>, <span class="dv">5</span>);</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>  alpha ~ std_normal();</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>  a ~ std_normal();</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>  eta ~ std_normal();</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>  y ~ poisson_log(a + f);</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="logistic-gaussian-process-regression" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="logistic-gaussian-process-regression">Logistic Gaussian process regression</h4>
<p>For binary classification problems, the observed outputs <span class="math inline">\(z_n \in
\{ 0,1 \}\)</span> are binary. These outputs are modeled using a Gaussian process with (unobserved) outputs <span class="math inline">\(y_n\)</span> through the logistic link, <span class="math display">\[
z_n \sim \textsf{Bernoulli}(\operatorname{logit}^{-1}(y_n)),
\]</span> or in other words, <span class="math display">\[
\Pr[z_n = 1] = \operatorname{logit}^{-1}(y_n).
\]</span></p>
<p>We can extend our latent variable GP Stan program to deal with classification problems. Below <code>a</code> is the bias term, which can help account for imbalanced classes in the training data:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>, <span class="kw">upper</span>=<span class="dv">1</span>&gt; z;</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">// ...</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>  y ~ bernoulli_logit(a + f);</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="automatic-relevance-determination" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="automatic-relevance-determination">Automatic relevance determination</h3>
<p>If we have multivariate inputs <span class="math inline">\(x \in \mathbb{R}^D\)</span>, the squared exponential covariance function can be further generalized by fitting a scale parameter <span class="math inline">\(\rho_d\)</span> for each dimension <span class="math inline">\(d\)</span>, <span class="math display">\[
  k(x \mid \alpha, \vec{\rho}, \sigma)_{i, j} = \alpha^2 \exp
\left(-\dfrac{1}{2}
\sum_{d=1}^D \dfrac{1}{\rho_d^2} (x_{i,d} - x_{j,d})^2
\right)
+ \delta_{i, j}\sigma^2.
\]</span> The estimation of <span class="math inline">\(\rho\)</span> was termed “automatic relevance determination” by <span class="citation" data-cites="Neal:1996">Neal (<a href="#ref-Neal:1996" role="doc-biblioref">1996</a>)</span>, but this is misleading, because the magnitude of the scale of the posterior for each <span class="math inline">\(\rho_d\)</span> is dependent on the scaling of the input data along dimension <span class="math inline">\(d\)</span>. Moreover, the scale of the parameters <span class="math inline">\(\rho_d\)</span> measures non-linearity along the <span class="math inline">\(d\)</span>-th dimension, rather than “relevance” <span class="citation" data-cites="PiironenVehtari:2016">(<a href="#ref-PiironenVehtari:2016" role="doc-biblioref">Piironen and Vehtari 2016</a>)</span>.</p>
<p>A priori, the closer <span class="math inline">\(\rho_d\)</span> is to zero, the more nonlinear the conditional mean in dimension <span class="math inline">\(d\)</span> is. A posteriori, the actual dependencies between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> play a role. With one covariate <span class="math inline">\(x_1\)</span> having a linear effect and another covariate <span class="math inline">\(x_2\)</span> having a nonlinear effect, it is possible that <span class="math inline">\(\rho_1 &gt; \rho_2\)</span> even if the predictive relevance of <span class="math inline">\(x_1\)</span> is higher <span class="citation" data-cites="RasmussenWilliams:2006">(<a href="#ref-RasmussenWilliams:2006" role="doc-biblioref">Rasmussen and Williams 2006, 80</a>)</span>. The collection of <span class="math inline">\(\rho_d\)</span> (or <span class="math inline">\(1/\rho_d\)</span>) parameters can also be modeled hierarchically.</p>
<p>The implementation of automatic relevance determination in Stan is straightforward, though it currently requires the user to directly code the covariance matrix. We’ll write a function to generate the Cholesky of the covariance matrix called <code>L_cov_exp_quad_ARD</code>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">functions</span> {</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span> L_cov_exp_quad_ARD(<span class="dt">vector</span>[] x,</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>                            <span class="dt">real</span> alpha,</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>                            <span class="dt">vector</span> rho,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>                            <span class="dt">real</span> delta) {</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> N = size(x);</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    <span class="dt">matrix</span>[N, N] K;</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="dt">real</span> sq_alpha = square(alpha);</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span>:(N<span class="dv">-1</span>)) {</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>      K[i, i] = sq_alpha + delta;</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> (j <span class="cf">in</span> (i + <span class="dv">1</span>):N) {</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        K[i, j] = sq_alpha</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>                      * exp(-<span class="fl">0.5</span> * dot_self((x[i] - x[j]) ./ rho));</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        K[j, i] = K[i, j];</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    K[N, N] = sq_alpha + delta;</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cholesky_decompose(K);</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N;</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; D;</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">vector</span>[D] x;</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] y;</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> delta = <span class="fl">1e-9</span>;</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt;[D] rho;</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; alpha;</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] eta;</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] f;</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>    <span class="dt">matrix</span>[N, N] L_K = L_cov_exp_quad_ARD(x, alpha, rho, delta);</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>    f = L_K * eta;</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>  rho ~ inv_gamma(<span class="dv">5</span>, <span class="dv">5</span>);</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>  alpha ~ std_normal();</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>  sigma ~ std_normal();</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>  eta ~ std_normal();</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>  y ~ normal(f, sigma);</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="priors-gp.section" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="priors-gp.section">Priors for Gaussian process parameters</h3>
<p>Formulating priors for GP hyperparameters requires the analyst to consider the inherent statistical properties of a GP, the GP’s purpose in the model, and the numerical issues that may arise in Stan when estimating a GP.</p>
<p>Perhaps most importantly, the parameters <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\alpha\)</span> are weakly identified <span class="citation" data-cites="zhang-gp:2004">(<a href="#ref-zhang-gp:2004" role="doc-biblioref">Zhang 2004</a>)</span>. The ratio of the two parameters is well-identified, but in practice we put independent priors on the two hyperparameters because these two quantities are more interpretable than their ratio.</p>
<section id="priors-for-length-scale" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="priors-for-length-scale">Priors for length-scale</h4>
<p>GPs are a flexible class of priors and, as such, can represent a wide spectrum of functions. For length scales below the minimum spacing of the covariates the GP likelihood plateaus. Unless regularized by a prior, this flat likelihood induces considerable posterior mass at small length scales where the observation variance drops to zero and the functions supported by the GP begin to exactly interpolate between the input data. The resulting posterior not only significantly overfits to the input data, it also becomes hard to accurately sample using Euclidean HMC.</p>
<p>We may wish to put further soft constraints on the length-scale, but these are dependent on how the GP is used in our statistical model.</p>
<p>If our model consists of only the GP, i.e.: <span class="math display">\[\begin{align*}
f   &amp;\sim \textsf{multivariate normal}\left(0, K(x \mid \alpha, \rho)\right) \\
y_i &amp;\sim \textsf{normal}(f_i, \sigma) \, \forall i \in \{1, \dots, N\} \\
    &amp; x \in \mathbb{R}^{N \times D}, \quad
      f \in \mathbb{R}^N
\end{align*}\]</span></p>
<p>we likely don’t need constraints beyond penalizing small length-scales. We’d like to allow the GP prior to represent both high-frequency and low-frequency functions, so our prior should put non-negligible mass on both sets of functions. In this case, an inverse gamma, <code>inv_gamma_lpdf</code> in Stan’s language, will work well as it has a sharp left tail that puts negligible mass on infinitesimal length-scales, but a generous right tail, allowing for large length-scales. Inverse gamma priors will avoid infinitesimal length-scales because the density is zero at zero, so the posterior for length-scale will be pushed away from zero. An inverse gamma distribution is one of many zero-avoiding or boundary-avoiding distributions.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>If we’re using the GP as a component in a larger model that includes an overall mean and fixed effects for the same variables we’re using as the domain for the GP, i.e.: <span class="math display">\[\begin{align*}
f   &amp;\sim \textsf{multivariate normal}\big(0, K(x \mid \alpha, \rho)\big) \\
y_i &amp;\sim \textsf{normal}\left(\beta_0 + x_i \beta_{[1:D]} + f_i, \sigma\right) \, \forall i
  \in \{1, \dots, N\} \\
    &amp; x_i^T, \beta_{[1:D]} \in \mathbb{R}^D,\quad
      x \in \mathbb{R}^{N \times D},\quad
      f \in \mathbb{R}^N
\end{align*}\]</span></p>
<p>we’ll likely want to constrain large length-scales as well. A length scale that is larger than the scale of the data yields a GP posterior that is practically linear (with respect to the particular covariate) and increasing the length scale has little impact on the likelihood. This will introduce nonidentifiability in our model, as both the fixed effects and the GP will explain similar variation. In order to limit the amount of overlap between the GP and the linear regression, we should use a prior with a sharper right tail to limit the GP to higher-frequency functions. We can use a generalized inverse Gaussian distribution: <span class="math display">\[\begin{align*}
f(x \mid a, b, p) &amp;= \dfrac{\left(a/b\right)^{p/2}}{2K_p\left(\sqrt{ab}\right)} x^{p - 1}\exp\big(-(ax + b
  / x)/2\big) \\
  &amp; x, a, b \in \mathbb{R}^{+},\quad
    p \in \mathbb{Z}
\end{align*}\]</span></p>
<p>which has an inverse gamma left tail if <span class="math inline">\(p \leq 0\)</span> and an inverse Gaussian right tail. This has not yet been implemented in Stan’s math library, but it is possible to implement as a user defined function:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">functions</span> {</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> generalized_inverse_gaussian_lpdf(<span class="dt">real</span> x, <span class="dt">int</span> p,</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>                                        <span class="dt">real</span> a, <span class="dt">real</span> b) {</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> p * <span class="fl">0.5</span> * log(a / b)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>      - log(<span class="dv">2</span> * modified_bessel_second_kind(p, sqrt(a * b)))</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>      + (p - <span class="dv">1</span>) * log(x)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>      - (a * x + b / x) * <span class="fl">0.5</span>;</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a> }</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>  <span class="co">// ...</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If we have high-frequency covariates in our fixed effects, we may wish to further regularize the GP away from high-frequency functions, which means we’ll need to penalize smaller length-scales. Luckily, we have a useful way of thinking about how length-scale affects the frequency of the functions supported the GP. If we were to repeatedly draw from a zero-mean GP with a length-scale of <span class="math inline">\(\rho\)</span> in a fixed-domain <span class="math inline">\([0,T]\)</span>, we would get a distribution for the number of times each draw of the GP crossed the zero axis. The expectation of this random variable, the number of zero crossings, is <span class="math inline">\(T / \pi
\rho\)</span>. You can see that as <span class="math inline">\(\rho\)</span> decreases, the expectation of the number of upcrossings increases as the GP is representing higher-frequency functions. Thus, this is a good statistic to keep in mind when setting a lower-bound for our prior on length-scale in the presence of high-frequency covariates. However, this statistic is only valid for one-dimensional inputs.</p>
</section>
<section id="priors-for-marginal-standard-deviation" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="priors-for-marginal-standard-deviation">Priors for marginal standard deviation</h4>
<p>The parameter <span class="math inline">\(\alpha\)</span> corresponds to how much of the variation is explained by the regression function and has a similar role to the prior variance for linear model weights. This means the prior can be the same as used in linear models, such as a half-<span class="math inline">\(t\)</span> prior on <span class="math inline">\(\alpha\)</span>.</p>
<p>A half-<span class="math inline">\(t\)</span> or half-Gaussian prior on alpha also has the benefit of putting nontrivial prior mass around zero. This allows the GP support the zero functions and allows the possibility that the GP won’t contribute to the conditional mean of the total output.</p>
</section>
</section>
<section id="predictive-inference-with-a-gaussian-process" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="predictive-inference-with-a-gaussian-process">Predictive inference with a Gaussian process</h3>
<p>Suppose for a given sequence of inputs <span class="math inline">\(x\)</span> that the corresponding outputs <span class="math inline">\(y\)</span> are observed. Given a new sequence of inputs <span class="math inline">\(\tilde{x}\)</span>, the posterior predictive distribution of their labels is computed by sampling outputs <span class="math inline">\(\tilde{y}\)</span> according to <span class="math display">\[
p\left(\tilde{y} \mid \tilde{x},x,y\right)
\ = \
\frac{p\left(\tilde{y}, y \mid \tilde{x},x\right)}
     {p(y \mid x)}
\ \propto \
p\left(\tilde{y}, y \mid \tilde{x},x\right).
\]</span></p>
<p>A direct implementation in Stan defines a model in terms of the joint distribution of the observed <span class="math inline">\(y\)</span> and unobserved <span class="math inline">\(\tilde{y}\)</span>.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N1;</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N1] <span class="dt">real</span> x1;</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N1] y1;</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N2;</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N2] <span class="dt">real</span> x2;</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> delta = <span class="fl">1e-9</span>;</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N = N1 + N2;</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">real</span> x;</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n1 <span class="cf">in</span> <span class="dv">1</span>:N1) {</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    x[n1] = x1[n1];</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n2 <span class="cf">in</span> <span class="dv">1</span>:N2) {</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    x[N1 + n2] = x2[n2];</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; rho;</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; alpha;</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] eta;</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed parameters</span> {</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] f;</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    <span class="dt">matrix</span>[N, N] L_K;</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    <span class="dt">matrix</span>[N, N] K = cov_exp_quad(x, alpha, rho);</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>    <span class="co">// diagonal elements</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>      K[n, n] = K[n, n] + delta;</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>    L_K = cholesky_decompose(K);</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>    f = L_K * eta;</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>  rho ~ inv_gamma(<span class="dv">5</span>, <span class="dv">5</span>);</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>  alpha ~ std_normal();</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>  sigma ~ std_normal();</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>  eta ~ std_normal();</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>  y1 ~ normal(f[<span class="dv">1</span>:N1], sigma);</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N2] y2;</span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n2 <span class="cf">in</span> <span class="dv">1</span>:N2) {</span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>    y2[n2] = normal_rng(f[N1 + n2], sigma);</span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The input vectors <code>x1</code> and <code>x2</code> are declared as data, as is the observed output vector <code>y1</code>. The unknown output vector <code>y2</code>, which corresponds to input vector <code>x2</code>, is declared in the generated quantities block and will be sampled when the model is executed.</p>
<p>A transformed data block is used to combine the input vectors <code>x1</code> and <code>x2</code> into a single vector <code>x</code>.</p>
<p>The model block declares and defines a local variable for the combined output vector <code>f</code>, which consists of the concatenation of the conditional mean for known outputs <code>y1</code> and unknown outputs <code>y2</code>. Thus the combined output vector <code>f</code> is aligned with the combined input vector <code>x</code>. All that is left is to define the univariate normal distribution statement for <code>y</code>.</p>
<p>The generated quantities block defines the quantity <code>y2</code>. We generate <code>y2</code> by randomly generating <code>N2</code> values from univariate normals with each mean corresponding to the appropriate element in <code>f</code>.</p>
<section id="predictive-inference-in-non-gaussian-gps" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="predictive-inference-in-non-gaussian-gps">Predictive inference in non-Gaussian GPs</h4>
<p>We can do predictive inference in non-Gaussian GPs in much the same way as we do with Gaussian GPs.</p>
<p>Consider the following full model for prediction using logistic Gaussian process regression.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N1;</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N1] <span class="dt">real</span> x1;</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N1] <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>, <span class="kw">upper</span>=<span class="dv">1</span>&gt; z1;</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N2;</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N2] <span class="dt">real</span> x2;</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> delta = <span class="fl">1e-9</span>;</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N = N1 + N2;</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">real</span> x;</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n1 <span class="cf">in</span> <span class="dv">1</span>:N1) {</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    x[n1] = x1[n1];</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n2 <span class="cf">in</span> <span class="dv">1</span>:N2) {</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    x[N1 + n2] = x2[n2];</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; rho;</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; alpha;</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> a;</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] eta;</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed parameters</span> {</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N] f;</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>    <span class="dt">matrix</span>[N, N] L_K;</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>    <span class="dt">matrix</span>[N, N] K = cov_exp_quad(x, alpha, rho);</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>    <span class="co">// diagonal elements</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>      K[n, n] = K[n, n] + delta;</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>    L_K = cholesky_decompose(K);</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>    f = L_K * eta;</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>  rho ~ inv_gamma(<span class="dv">5</span>, <span class="dv">5</span>);</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>  alpha ~ std_normal();</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>  a ~ std_normal();</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>  eta ~ std_normal();</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>  z1 ~ bernoulli_logit(a + f[<span class="dv">1</span>:N1]);</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N2] <span class="dt">int</span> z2;</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n2 <span class="cf">in</span> <span class="dv">1</span>:N2) {</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>    z2[n2] = bernoulli_logit_rng(a + f[N1 + n2]);</span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="analytical-form-of-joint-predictive-inference" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="analytical-form-of-joint-predictive-inference">Analytical form of joint predictive inference</h4>
<p>Bayesian predictive inference for Gaussian processes with Gaussian observations can be sped up by deriving the posterior analytically, then directly sampling from it.</p>
<p>Jumping straight to the result, <span class="math display">\[
p\left(\tilde{y} \mid \tilde{x},y,x\right)
=
\textsf{normal}\left(K^{\top}\Sigma^{-1}y,\
                \Omega - K^{\top}\Sigma^{-1}K\right),
\]</span> where <span class="math inline">\(\Sigma = K(x \mid \alpha, \rho, \sigma)\)</span> is the result of applying the covariance function to the inputs <span class="math inline">\(x\)</span> with observed outputs <span class="math inline">\(y\)</span>, <span class="math inline">\(\Omega =
K(\tilde{x} \mid \alpha, \rho)\)</span> is the result of applying the covariance function to the inputs <span class="math inline">\(\tilde{x}\)</span> for which predictions are to be inferred, and <span class="math inline">\(K\)</span> is the matrix of covariances between inputs <span class="math inline">\(x\)</span> and <span class="math inline">\(\tilde{x}\)</span>, which in the case of the exponentiated quadratic covariance function would be <span class="math display">\[
K(x \mid \alpha, \rho)_{i, j} = \eta^2 \exp\left(-\dfrac{1}{2 \rho^2}
\sum_{d=1}^D \left(x_{i,d} - \tilde{x}_{j,d}\right)^2\right).
\]</span></p>
<p>There is no noise term including <span class="math inline">\(\sigma^2\)</span> because the indexes of elements in <span class="math inline">\(x\)</span> and <span class="math inline">\(\tilde{x}\)</span> are never the same.</p>
<p>This Stan code below uses the analytic form of the posterior and provides sampling of the resulting multivariate normal through the Cholesky decomposition. The data declaration is the same as for the latent variable example, but we’ve defined a function called <code>gp_pred_rng</code> which will generate a draw from the posterior predictive mean conditioned on observed data <code>y1</code>. The code uses a Cholesky decomposition in triangular solves in order to cut down on the number of matrix-matrix multiplications when computing the conditional mean and the conditional covariance of <span class="math inline">\(p(\tilde{y})\)</span>.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">functions</span> {</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span> gp_pred_rng(<span class="dt">array</span>[] <span class="dt">real</span> x2,</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>                     <span class="dt">vector</span> y1,</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>                     <span class="dt">array</span>[] <span class="dt">real</span> x1,</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>                     <span class="dt">real</span> alpha,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>                     <span class="dt">real</span> rho,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>                     <span class="dt">real</span> sigma,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>                     <span class="dt">real</span> delta) {</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> N1 = rows(y1);</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> N2 = size(x2);</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    <span class="dt">vector</span>[N2] f2;</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>      <span class="dt">matrix</span>[N1, N1] L_K;</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>      <span class="dt">vector</span>[N1] K_div_y1;</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>      <span class="dt">matrix</span>[N1, N2] k_x1_x2;</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>      <span class="dt">matrix</span>[N1, N2] v_pred;</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>      <span class="dt">vector</span>[N2] f2_mu;</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>      <span class="dt">matrix</span>[N2, N2] cov_f2;</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>      <span class="dt">matrix</span>[N2, N2] diag_delta;</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>      <span class="dt">matrix</span>[N1, N1] K;</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>      K = cov_exp_quad(x1, alpha, rho);</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N1) {</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>        K[n, n] = K[n, n] + square(sigma);</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>      L_K = cholesky_decompose(K);</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>      K_div_y1 = mdivide_left_tri_low(L_K, y1);</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>      K_div_y1 = mdivide_right_tri_low(K_div_y1', L_K)';</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>      k_x1_x2 = cov_exp_quad(x1, x2, alpha, rho);</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>      f2_mu = (k_x1_x2' * K_div_y1);</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>      v_pred = mdivide_left_tri_low(L_K, k_x1_x2);</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>      cov_f2 = cov_exp_quad(x2, alpha, rho) - v_pred' * v_pred;</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>      diag_delta = diag_matrix(rep_vector(delta, N2));</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>      f2 = multi_normal_rng(f2_mu, cov_f2 + diag_delta);</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> f2;</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N1;</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N1] <span class="dt">real</span> x1;</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N1] y1;</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N2;</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N2] <span class="dt">real</span> x2;</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N1] mu = rep_vector(<span class="dv">0</span>, N1);</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> delta = <span class="fl">1e-9</span>;</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; rho;</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; alpha;</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N1, N1] L_K;</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>    <span class="dt">matrix</span>[N1, N1] K = cov_exp_quad(x1, alpha, rho);</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>    <span class="dt">real</span> sq_sigma = square(sigma);</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>    <span class="co">// diagonal elements</span></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (n1 <span class="cf">in</span> <span class="dv">1</span>:N1) {</span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>      K[n1, n1] = K[n1, n1] + sq_sigma;</span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a>    L_K = cholesky_decompose(K);</span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a>  rho ~ inv_gamma(<span class="dv">5</span>, <span class="dv">5</span>);</span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a>  alpha ~ std_normal();</span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a>  sigma ~ std_normal();</span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a>  y1 ~ multi_normal_cholesky(mu, L_K);</span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N2] f2;</span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>[N2] y2;</span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a>  f2 = gp_pred_rng(x2, y1, x1, alpha, rho, sigma, delta);</span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n2 <span class="cf">in</span> <span class="dv">1</span>:N2) {</span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>    y2[n2] = normal_rng(f2[n2], sigma);</span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="multiple-output-gaussian-processes" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="multiple-output-gaussian-processes">Multiple-output Gaussian processes</h3>
<p>Suppose we have observations <span class="math inline">\(y_i \in \mathbb{R}^M\)</span> observed at <span class="math inline">\(x_i \in \mathbb{R}^K\)</span>. One can model the data like so: <span class="math display">\[\begin{align*}
y_i  &amp;\sim \textsf{multivariate normal}\left(f(x_i), \textbf{I}_M \sigma^2\right) \\
f(x) &amp;\sim \textsf{GP}\big(m(x), K(x \mid \theta, \phi)\big) \\
     &amp; K(x \mid \theta) \in \mathbb{R}^{M \times M}, \quad
       f(x), m(x) \in \mathbb{R}^M
\end{align*}\]</span> where the <span class="math inline">\(K(x, x^\prime \mid \theta, \phi)_{[m, m^\prime]}\)</span> entry defines the covariance between <span class="math inline">\(f_m(x)\)</span> and <span class="math inline">\(f_{m^\prime}(x^\prime)(x)\)</span>. This construction of Gaussian processes allows us to learn the covariance between the output dimensions of <span class="math inline">\(f(x)\)</span>. If we parameterize our kernel <span class="math inline">\(K\)</span>: <span class="math display">\[
K(x, x^\prime \mid \theta, \phi)_{[m, m^\prime]} = k\left(x, x^\prime \mid
\theta\right) k\left(m, m^\prime \mid \phi\right)
\]</span> then our finite dimensional generative model for the above is: <span class="math display">\[\begin{align*}
f        &amp;\sim \textsf{matrixnormal}\big(m(x), K(x \mid \alpha, \rho), C(\phi)\big) \\
y_{i, m} &amp;\sim \textsf{normal}(f_{i,m}, \sigma) \\
f        &amp;\in  \mathbb{R}^{N \times M}
\end{align*}\]</span> where <span class="math inline">\(K(x \mid \alpha, \rho)\)</span> is the exponentiated quadratic kernel we’ve used throughout this chapter, and <span class="math inline">\(C(\phi)\)</span> is a positive-definite matrix, parameterized by some vector <span class="math inline">\(\phi\)</span>.</p>
<p>The matrix normal distribution has two covariance matrices: <span class="math inline">\(K(x \mid
\alpha, \rho)\)</span> to encode column covariance, and <span class="math inline">\(C(\phi)\)</span> to define row covariance. The salient features of the matrix normal are that the rows of the matrix <span class="math inline">\(f\)</span> are distributed: <span class="math display">\[
f_{[n,]} \sim \textsf{multivariate normal}\big(m(x)_{[n,]}, K(x \mid \alpha,
\rho)_{[n,n]} C(\phi)\big)
\]</span> and that the columns of the matrix <span class="math inline">\(f\)</span> are distributed: <span class="math display">\[
f_{[,m]} \sim \textsf{multivariate normal}\big(m(x)_{[,m]}, K(x
  \mid \alpha, \rho) C(\phi)_{[m,m]}\big)
\]</span> This also means means that <span class="math inline">\(\mathbb{E}\left[f^T f\right]\)</span> is equal to <span class="math inline">\(\operatorname{trace}\!\big(K(x \mid \alpha, \rho)\big) \times C\)</span>, whereas <span class="math inline">\(\mathbb{E}\left[ff^T\right]\)</span> is <span class="math inline">\(\operatorname{trace}(C) \times K(x \mid \alpha, \rho)\)</span>. We can derive this using properties of expectation and the matrix normal density.</p>
<p>We should set <span class="math inline">\(\alpha\)</span> to <span class="math inline">\(1.0\)</span> because the parameter is not identified unless we constrain <span class="math inline">\(\operatorname{trace}(C) = 1\)</span>. Otherwise, we can multiply <span class="math inline">\(\alpha\)</span> by a scalar <span class="math inline">\(d\)</span> and <span class="math inline">\(C\)</span> by <span class="math inline">\(1/d\)</span> and our likelihood will not change.</p>
<p>We can generate a random variable <span class="math inline">\(f\)</span> from a matrix normal density in <span class="math inline">\(\mathbb{R}^{N \times M}\)</span> using the following algorithm: <span class="math display">\[\begin{align*}
\eta_{i,j} &amp;\sim \textsf{normal}(0, 1) \, \forall i,j \\
f          &amp;= L_{K(x \mid 1.0, \rho)} \, \eta \, L_C(\phi)^T \\
f          &amp;\sim \textsf{matrixnormal}\big(0, K(x \mid 1.0, \rho), C(\phi)\big) \\
\eta       &amp;\in \mathbb{R}^{N \times M} \\
L_C(\phi)  &amp;= \texttt{cholesky}\mathtt{\_}\texttt{decompose}\big(C(\phi)\big) \\
L_{K(x \mid 1.0, \rho)} &amp;= \texttt{cholesky}\mathtt{\_}\texttt{decompose}\big(K(x \mid 1.0, \rho)\big)
\end{align*}\]</span></p>
<p>This can be implemented in Stan using a latent-variable GP formulation. We’ve used <span class="math inline">\(\textsf{LKJCorr}\)</span> for <span class="math inline">\(C(\phi)\)</span>, but any positive-definite matrix will do.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N;</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; D;</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">array</span>[N] <span class="dt">real</span> x;</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N, D] y;</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> delta = <span class="fl">1e-9</span>;</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; rho;</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>  <span class="dt">vector</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt;[D] alpha;</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>  <span class="dt">cholesky_factor_corr</span>[D] L_Omega;</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N, D] eta;</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[N, D] f;</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    <span class="dt">matrix</span>[N, N] K = cov_exp_quad(x, <span class="fl">1.0</span>, rho);</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    <span class="dt">matrix</span>[N, N] L_K;</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">// diagonal elements</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>      K[n, n] = K[n, n] + delta;</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>    L_K = cholesky_decompose(K);</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>    f = L_K * eta</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>        * diag_pre_multiply(alpha, L_Omega)';</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>  rho ~ inv_gamma(<span class="dv">5</span>, <span class="dv">5</span>);</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>  alpha ~ std_normal();</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>  sigma ~ std_normal();</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>  L_Omega ~ lkj_corr_cholesky(<span class="dv">3</span>);</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>  to_vector(eta) ~ std_normal();</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>  to_vector(y) ~ normal(to_vector(f), sigma);</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>  <span class="dt">matrix</span>[D, D] Omega;</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>  Omega = L_Omega * L_Omega';</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>



</section>
</section>
</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Neal:1996" class="csl-entry" role="listitem">
Neal, Radford M. 1996. <em>Bayesian Learning for Neural Networks</em>. Lecture Notes in Statistics 118. New York: Springer.
</div>
<div id="ref-Neal:1997" class="csl-entry" role="listitem">
———. 1997. <span>“Monte <span>C</span>arlo Implementation of <span>G</span>aussian Process Models for <span>B</span>ayesian Regression and Classification.”</span> 9702. University of Toronto, Department of Statistics.
</div>
<div id="ref-PiironenVehtari:2016" class="csl-entry" role="listitem">
Piironen, Juho, and Aki Vehtari. 2016. <span>“Projection Predictive Model Selection for <span>Gaussian</span> Processes.”</span> In <em>Machine Learning for Signal Processing (MLSP), 2016 IEEE 26th International Workshop on</em>. IEEE.
</div>
<div id="ref-RasmussenWilliams:2006" class="csl-entry" role="listitem">
Rasmussen, Carl Edward, and Christopher K. I. Williams. 2006. <em>Gaussian Processes for Machine Learning</em>. MIT Press.
</div>
<div id="ref-Riutort-Mayol:2023:HSGP" class="csl-entry" role="listitem">
Riutort-Mayol, Gabriel, Paul-Christian Bürkner, Michael R Andersen, Arno Solin, and Aki Vehtari. 2023. <span>“Practical <span>Hilbert</span> Space Approximate <span>Bayesian</span> <span>Gaussian</span> Processes for Probabilistic Programming.”</span> <em>Statistics and Computing</em> 33 (1): 17.
</div>
<div id="ref-zhang-gp:2004" class="csl-entry" role="listitem">
Zhang, Hao. 2004. <span>“Inconsistent Estimation and Asymptotically Equal Interpolations in Model-Based Geostatistics.”</span> <em>Journal of the American Statistical Association</em> 99 (465): 250–61.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Gaussian processes can be extended to covariance functions producing positive semi-definite matrices, but Stan does not support inference in the resulting models because the resulting distribution does not have unconstrained support.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>A boundary-avoiding prior is just one where the limit of the density is zero at the boundary, the result of which is estimates that are pushed away from the boundary.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../stan-users-guide/clustering.html" class="pagination-link" aria-label="Clustering Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Clustering Models</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../stan-users-guide/hyperspherical-models.html" class="pagination-link" aria-label="Directions, Rotations, and Hyperspheres">
        <span class="nav-page-text">Directions, Rotations, and Hyperspheres</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/stan-dev/docs/edit/master/src/stan-users-guide/gaussian-processes.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/stan-dev/docs/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>