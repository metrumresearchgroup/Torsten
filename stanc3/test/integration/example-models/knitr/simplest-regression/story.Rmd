---
title: "Simplest regression (for the Bayes in Stan book)"
author: "Andrew Gelman"
date: "27 Jun 2018"
output:
  pdf_document: default
  html_document:
    theme: readable
---

```{r setup, include=FALSE, echo=FALSE}
options(htmltools.dir.version = FALSE)
options(digits = 2)

library(ggplot2)

library(gridExtra)

library(knitr)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
knitr::opts_chunk$set(comment = "")

print_file <- function(file) {
  cat(paste(readLines(file), "\n", sep=""), sep="")
}

extract_one_draw <- function(stanfit, chain = 1, iter = 1) {
  x <- get_inits(stanfit, iter = iter)
  x[[chain]]
}
```
Our "Hello World" example for R and Stan is a linear regression,
$$y_i=a+bx_i +\mbox{error}_i, \mbox{ for } i=1,\dots,N,$$
with errors independent and normally distributed with mean 0 and standard deviation $\sigma$.  We will perform the following steps which are a key part of our workflow:

1. _Simulate fake data._ Set the sample size $N$, predictor vector $x$, and parameters $a,b,\sigma$, and from that information simulate fake data $y$ from the model above.

2. _Fit the model._ Express the model in Stan, pass the data $N,x,y$ into the program, and estimate the parameters.

3. _Evaluate the fit._  Compare the estimated parameters (or, more fully, the posterior distribution of the parameters) to their true values, which in this simulated-data scenario are known.

We begin by setting up R and Stan, setting it to run in parallel and to save compiled Stan programs in the working directory:
```{r}
library("rstan")
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```
Then we write the program to simulate fake data:
```{r}
print_file("fake-data.stan")
```
To run this, we need to specify the sample size $N$, the regression coefficients $a$ and $b$, the residual standard deviation $\sigma$, and the vector of predictors $x$, which we then put in a list:
```{r}
N <- 100
data <- list(N=N, a=2, b=3, sigma=5, x=runif(N, 0, 10))
```
We then run the Stan model and create the fake data:
```{r, warning=FALSE, results=FALSE}
fake_data <- stan("fake-data.stan", chains=1, iter=1, algorithm="Fixed_param", data=data)
```
We only needed one chain and one iteration as we are just simulating one fake dataset (in this case, a vector $y$ of length 100).  Also, just to know:  the first time you run this on your computer, you'll have to wait 15 seconds or so for the Stan model to compile.  After that, it will save the compiled code in your home directory.

We then extract the simulated data vector and append it to our data list:
```{r}
data$y <- extract_one_draw(fake_data)$y
```

Alternatively we could simulate the fake data in R:
```{r}
a <- 2
b <- 3
sigma <- 5
x <- runif(N, 0, 10)
data$y <- rnorm(N, a + b*x, sigma)
```
In this case it is simple enough to fit the fake data in R.  But we wanted to show above how to do it in Stan, as this can be useful for more complicated models.

In any case, now that we have the data list, we can use it to estimate the parameters.

Here is the Stan program to fit the model:
```{r}
print_file("simplest-regression.stan")
```
And now we can run it:
```{r, results=FALSE}
fit <- stan("simplest-regression.stan", data=data)

```
Here is the summary of the fitted model:
```{r}
print(fit)

```
Now we go through the output:

* The first few lines summarize the Stan run, with the name of the file, the number of chains and iterations.  In this case, Stan ran the default 4 chains with 1000 warmup iterations followed by 1000 post-warmup iterations, yielding 4000 post-warmup simulation draws in total.

* The left column of the table has the names of parameters, transformed parameters, and generated quantities produced by model.stan.  In this case, the parameters are a, b, and sigma; the only transformed parameter is lp__ (the log-posterior density or target function created by the Stan model); and there are no generated quantities.

* The next column of the table shows the mean (average) of the 4000 draws for each quantity.

* The next column shows the Monte Carlo standard error, which is an estimate of the uncertainty in the mean.

* The next column shows the standard deviation of the draws for each quantity.  As the number of simulation draws increases, mean should approach the posterior mean, se_mean should go to zero, and sd should approach the posterior standard deviation.  For most purposes we can ignore se_mean.

* The next several columns give quantiles of the simulations.

* The next columns gives the effective sample size and $\widehat{R}$. Typically we want $\widehat{R}$ to be less then 1.1 for each quantity in the table.

In the above output, $\widehat{R}$ is less then 1.1 for all quantities, so the chains seem to have mixed well, and we use the results to summarize the posterior distribution.  We can compare the posterior inferences to the true parameter values (here, $a=2$, $b=3$, and $\sigma=5$).  These true values are roughly within the range of uncertainty of the inferences.

